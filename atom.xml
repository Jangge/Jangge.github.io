<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>凹凸岛</title>
  
  
  <link href="http://example.com/atom.xml" rel="self"/>
  
  <link href="http://example.com/"/>
  <updated>2024-04-22T12:24:41.887Z</updated>
  <id>http://example.com/</id>
  
  <author>
    <name>凹凸</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>数据可视化（EDA）</title>
    <link href="http://example.com/2024/04/22/%E6%95%B0%E6%8D%AE%E5%8F%AF%E8%A7%86%E5%8C%96%EF%BC%88EDA%EF%BC%89/"/>
    <id>http://example.com/2024/04/22/%E6%95%B0%E6%8D%AE%E5%8F%AF%E8%A7%86%E5%8C%96%EF%BC%88EDA%EF%BC%89/</id>
    <published>2024-04-22T12:21:31.000Z</published>
    <updated>2024-04-22T12:24:41.887Z</updated>
    
    <content type="html"><![CDATA[<h2 id="1、对数据集进行EDA是什么？"><a href="#1、对数据集进行EDA是什么？" class="headerlink" title="1、对数据集进行EDA是什么？"></a>1、对数据集进行EDA是什么？</h2><ul><li>EDA是一种用于分析数据集以概括其主要特征的方法，通常使用统计图形和其它数据可视化方法，是理解和准备用于任何数据集分析或机器学习项目的关键步骤。<span id="more"></span></li></ul><h2 id="2、要达成的主要目标是什么？"><a href="#2、要达成的主要目标是什么？" class="headerlink" title="2、要达成的主要目标是什么？"></a>2、要达成的主要目标是什么？</h2><ul><li>了解数据集的整体结构和分布。</li><li>识别数据中的异常值或错误。</li><li>发现数据中的模式和趋势。</li><li>为进一步的分析做好准备。</li></ul><h2 id="3、EDA常用的方法有哪些？"><a href="#3、EDA常用的方法有哪些？" class="headerlink" title="3、EDA常用的方法有哪些？"></a>3、EDA常用的方法有哪些？</h2><ul><li><strong>统计描述</strong>：使用统计量，例如均值、中位数、标准差来描述数据集的基本特征。</li><li><strong>数据可视化</strong>：使用图标和图形来直观的展示数据，如直方图、箱线图、散点图等。</li><li><strong>相关性分析</strong>：考察不同变量之间的关系。</li><li><strong>降维</strong>：将高维数据降维到低维空间以便于分析。</li></ul><h2 id="4、数据分析技巧"><a href="#4、数据分析技巧" class="headerlink" title="4、数据分析技巧"></a>4、数据分析技巧</h2><ul><li><strong>检查数据的基本特征</strong>：<ul><li>检查数据表的前几行：head()</li><li>检查数据类型和缺失值：df.info()</li><li>查看统计信息：df.describe()</li></ul></li><li><strong>处理缺失值、异常值和重复项</strong>：<ul><li>对缺失值使用插补等技术</li><li>适当的识别和处理异常值</li><li>如果有必要直接删除某些样本</li></ul></li><li><strong>数据可视化</strong>：<ul><li>创建可视化图表以深入洞察数据</li><li>对数据特征使用直方图和箱线图</li><li>对类别特征使用条形图</li><li>通过相关矩阵和散点图来理解变量间的关系</li></ul></li><li><strong>特征分析</strong>：<ul><li>探究分类和异常检测中特征于目标变量之间的关系</li><li>通过可视化方法来展示特征在不同类型和类别键的差异和变化</li><li>利用箱线图、提琴图或蜂窝状堆积图来比较特征分布</li></ul></li></ul><h2 id="5、数据EDA的例子"><a href="#5、数据EDA的例子" class="headerlink" title="5、数据EDA的例子"></a>5、数据EDA的例子</h2><ul><li>以UBC数据集为例子</li><li>原始数据集长这样<ul><li>一个csv文件包含图像名字和标签以及该图像大小等信息</li><li>一个文件夹里有对应名字的图像<img src="/pic/EDA/1.png" width = "60%" height = "60%" /></li></ul></li></ul><p><strong>5.1 显示每个类别有多少训练数据</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">train_df[<span class="string">&#x27;label&#x27;</span>].value_counts():</span><br><span class="line">label</span><br><span class="line">HGSC    <span class="number">222</span></span><br><span class="line">EC      <span class="number">124</span></span><br><span class="line">CC       <span class="number">99</span></span><br><span class="line">LGSC     <span class="number">47</span></span><br><span class="line">MC       <span class="number">46</span></span><br><span class="line">Name: count, dtype: int64</span><br><span class="line">(<span class="number">538</span>, <span class="number">5</span>)</span><br></pre></td></tr></table></figure><p>饼形图显示：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 查看所有标签类在数据集中所占的比例</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 先将每个不同标签提取出来</span></span><br><span class="line">HGSC = train_df[train_df[<span class="string">&#x27;label&#x27;</span>]==<span class="string">&quot;HGSC&quot;</span>]</span><br><span class="line">EC = train_df[train_df[<span class="string">&#x27;label&#x27;</span>]==<span class="string">&quot;EC&quot;</span>]</span><br><span class="line">CC = train_df[train_df[<span class="string">&#x27;label&#x27;</span>]==<span class="string">&quot;CC&quot;</span>]</span><br><span class="line">LGSC = train_df[train_df[<span class="string">&#x27;label&#x27;</span>]==<span class="string">&quot;LGSC&quot;</span>]</span><br><span class="line">MC = train_df[train_df[<span class="string">&#x27;label&#x27;</span>]==<span class="string">&quot;MC&quot;</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 设置图像大小</span></span><br><span class="line">plt.figure(figsize=(<span class="number">20</span>, <span class="number">6</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 设置字体大小</span></span><br><span class="line">plt.rcParams[<span class="string">&#x27;font.size&#x27;</span>] = <span class="number">14</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 设置颜色</span></span><br><span class="line">colors = [<span class="string">&#x27;lightgreen&#x27;</span>, <span class="string">&#x27;lightblue&#x27;</span>, <span class="string">&#x27;purple&#x27;</span>, <span class="string">&#x27;blue&#x27;</span>, <span class="string">&#x27;yellow&#x27;</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 开始绘制饼图</span></span><br><span class="line">plt.subplot(<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>) <span class="comment"># 创建一个子图</span></span><br><span class="line">plt.pie([<span class="built_in">len</span>(HGSC), <span class="built_in">len</span>(EC), <span class="built_in">len</span>(CC), <span class="built_in">len</span>(LGSC), <span class="built_in">len</span>(MC)], labels=[<span class="string">&#x27;HGSC&#x27;</span>, <span class="string">&#x27;EC&#x27;</span>, <span class="string">&#x27;CC&#x27;</span>, <span class="string">&#x27;LGSC&#x27;</span>, <span class="string">&#x27;MC&#x27;</span>], autopct=<span class="string">&#x27;%1.1f%%&#x27;</span>, colors=colors)</span><br><span class="line">plt.title(<span class="string">&#x27;Training Set&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 添加整个图像的主标题</span></span><br><span class="line">plt.suptitle(<span class="string">&#x27;Distribution of HGSC, EC, CC, LGSC and MC Image in  the Training data&#x27;</span>, fontsize=<span class="number">20</span>, y=<span class="number">1.05</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 显示这个表</span></span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><img src="/pic/EDA/2.png" width = "60%" height = "60%" /><p><strong>5.2查看缺失值</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">train = pd.read_csv(<span class="string">&#x27;train.csv&#x27;</span>)</span><br><span class="line">pd.isnull(train).values.<span class="built_in">any</span>()</span><br></pre></td></tr></table></figure><p><strong>5.3查看每个特征中有哪些不同的值</strong></p><ul><li>比如性别有男女，年龄有很多种等<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 先提取保存不同的特征及其所不同的类别</span></span><br><span class="line">feature_uniques = []</span><br><span class="line"><span class="keyword">for</span> cat <span class="keyword">in</span> features:</span><br><span class="line">    feature_uniques.append(<span class="built_in">len</span>(train[cat].unique()))</span><br><span class="line">    </span><br><span class="line">uniq_values_in_categories = pd.DataFrame.from_items([(<span class="string">&#x27;name&#x27;</span>, features), (<span class="string">&#x27;unique_values&#x27;</span>, uniques)])</span><br><span class="line"></span><br></pre></td></tr></table></figure></li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">fig, (ax1, ax2) = plt.subplots(<span class="number">1</span>,<span class="number">2</span>)</span><br><span class="line">fig.set_size_inches(<span class="number">16</span>,<span class="number">5</span>)</span><br><span class="line">ax1.hist(uniq_values_in_categories.unique_values, bins=<span class="number">50</span>)</span><br><span class="line">ax1.set_title(<span class="string">&#x27;Amount of categorical features with X distinct values&#x27;</span>)</span><br><span class="line">ax1.set_xlabel(<span class="string">&#x27;Distinct values in a feature&#x27;</span>)</span><br><span class="line">ax1.set_ylabel(<span class="string">&#x27;Features&#x27;</span>)</span><br><span class="line">ax1.annotate(<span class="string">&#x27;A feature with 326 vals&#x27;</span>, xy=(<span class="number">322</span>, <span class="number">2</span>), xytext=(<span class="number">200</span>, <span class="number">38</span>), arrowprops=<span class="built_in">dict</span>(facecolor=<span class="string">&#x27;black&#x27;</span>))</span><br><span class="line"></span><br><span class="line">ax2.set_xlim(<span class="number">2</span>,<span class="number">30</span>)</span><br><span class="line">ax2.set_title(<span class="string">&#x27;Zooming in the [0,30] part of left histogram&#x27;</span>)</span><br><span class="line">ax2.set_xlabel(<span class="string">&#x27;Distinct values in a feature&#x27;</span>)</span><br><span class="line">ax2.set_ylabel(<span class="string">&#x27;Features&#x27;</span>)</span><br><span class="line">ax2.grid(<span class="literal">True</span>)</span><br><span class="line">ax2.hist(uniq_values_in_categories[uniq_values_in_categories.unique_values &lt;= <span class="number">30</span>].unique_values, bins=<span class="number">30</span>)</span><br><span class="line">ax2.annotate(<span class="string">&#x27;Binary features&#x27;</span>, xy=(<span class="number">3</span>, <span class="number">71</span>), xytext=(<span class="number">7</span>, <span class="number">71</span>), arrowprops=<span class="built_in">dict</span>(facecolor=<span class="string">&#x27;black&#x27;</span>))</span><br></pre></td></tr></table></figure><p><strong>特征之间的相关性</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">plt.subplots(figsize=(<span class="number">16</span>,<span class="number">9</span>))</span><br><span class="line">correlation_mat = train[cont_features].corr()</span><br><span class="line">sns.heatmap(correlation_mat, annot=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure><img src="/pic/EDA/3.png" width = "60%" height = "60%" />]]></content>
    
    
    <summary type="html">&lt;h2 id=&quot;1、对数据集进行EDA是什么？&quot;&gt;&lt;a href=&quot;#1、对数据集进行EDA是什么？&quot; class=&quot;headerlink&quot; title=&quot;1、对数据集进行EDA是什么？&quot;&gt;&lt;/a&gt;1、对数据集进行EDA是什么？&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;EDA是一种用于分析数据集以概括其主要特征的方法，通常使用统计图形和其它数据可视化方法，是理解和准备用于任何数据集分析或机器学习项目的关键步骤。</summary>
    
    
    
    
  </entry>
  
  <entry>
    <title>使用Pytorch搭建Transformer</title>
    <link href="http://example.com/2024/04/14/%E4%BD%BF%E7%94%A8Pytorch%E6%90%AD%E5%BB%BATransformer/"/>
    <id>http://example.com/2024/04/14/%E4%BD%BF%E7%94%A8Pytorch%E6%90%AD%E5%BB%BATransformer/</id>
    <published>2024-04-14T13:48:25.000Z</published>
    <updated>2024-04-16T14:07:08.607Z</updated>
    
    <content type="html"><![CDATA[<p>经典的transformer架构如下图所示，分为编码器和解码器。构建模型的过程中，先构建通用的模块，如位置编码、多头自注意力、前馈神经网络层等，然后组建编码器和解码器，最后统一到一个模型中。<br><img src="/pic/transformers/1.png" width = "60%" height = "60%" /></p><span id="more"></span><h1 id="1、定义通用的层"><a href="#1、定义通用的层" class="headerlink" title="1、定义通用的层"></a>1、定义通用的层</h1><h2 id="1-1-位置编码-Positional-Encoding"><a href="#1-1-位置编码-Positional-Encoding" class="headerlink" title="1.1 位置编码 Positional Encoding"></a>1.1 位置编码 Positional Encoding</h2><ul><li>用于为输入的词向量添加位置编码</li><li>思路：<ul><li>先构建一个包含5000长度的向量表</li><li>在构建关于位置信息的矩阵，使用余弦衰减函数作为位置信息编码</li><li>因为我们在构建注意力机制时希望当前位置和其他位置之间的相似度可以随着距离的增大而逐渐减小。余弦衰减函数正好满足这样的需求，因为它越靠近原点的地方越陡峭，在远离原点的地方则越来越平缓。这样就可以让相近的位置有更多的交互机会，而较远的位置则较少参与交互。</li></ul></li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">PositionalEncoding</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, d_model, dropout=<span class="number">0.1</span>, max_len=<span class="number">5000</span></span>): <span class="comment"># dropout原文是0.1</span></span><br><span class="line">        <span class="comment"># max_len是假设一个句子最多包含5000个token</span></span><br><span class="line">        <span class="built_in">super</span>(PositionalEncoding, self).__init__()</span><br><span class="line">        self.dropout = nn.Dropout(p=dropout)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 开始位置编码部分，先生成一个max_len * d_model的矩阵，即5000*512</span></span><br><span class="line">        <span class="comment"># 5000是一个句子中最多的token数， 512是一个token用多长的向量来表示，5000*512这个矩阵用于表示一个句子的信息</span></span><br><span class="line">        pe = torch.zeros(max_len, d_model)</span><br><span class="line">        <span class="comment"># pos用于表示每个位置的索引，从0到5000</span></span><br><span class="line">        pos = torch.arange(<span class="number">0</span>, max_len, dtype=torch.<span class="built_in">float</span>).unsqueeze(<span class="number">1</span>) <span class="comment"># pos.shape=([5000, 1])</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 先把括号内的分时求出来，pos是[5000,1]，分母是256，通过广播机制相乘后是[5000, 256]</span></span><br><span class="line">        <span class="comment"># 计算一个余弦衰减函数， 将输入的位置信息转化为向量模式</span></span><br><span class="line">        div_term = pos / <span class="built_in">pow</span>(<span class="number">10000.0</span>, torch.arange(<span class="number">0</span>, d_model, <span class="number">2</span>).<span class="built_in">float</span>() / d_model) </span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 在取正余弦，周期型的取正弦和余弦有助于捕获更多位置信息</span></span><br><span class="line">        pe[:, <span class="number">0</span>::<span class="number">2</span>] = torch.sin(div_term)</span><br><span class="line">        pe[:, <span class="number">1</span>::<span class="number">2</span>] = torch.cos(div_term)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 一个句子要做一次配pe， 一个batch中会有多个句子，所以增加以为用来和输入的一个batch的数据相加时做广播</span></span><br><span class="line">        pe = pe.unsqueeze(<span class="number">0</span>) <span class="comment"># [5000,512]到[15000,512]</span></span><br><span class="line">        <span class="comment"># 将pe作为固定参数保存到缓冲区，不会被更新</span></span><br><span class="line">        self.register_buffer(<span class="string">&#x27;pe&#x27;</span>, pe)</span><br><span class="line">        </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="comment"># x时输入[batch_size, seq_len, d_model]</span></span><br><span class="line">        <span class="comment"># 5000时预定义的最大seq_len, 也就是把至多这么多位置的位置函数都计算好了，使用的时候直接相加就行了</span></span><br><span class="line">        x = x + self.pe[:, :x.size(<span class="number">1</span>), :]</span><br><span class="line">        <span class="keyword">return</span> self.dropout(x) <span class="comment"># 加入降噪，防止过拟合， 返回[batch_size, seq_len, d_model]</span></span><br></pre></td></tr></table></figure><h2 id="1-2-ScaledDotProductAttention"><a href="#1-2-ScaledDotProductAttention" class="headerlink" title="1.2 ScaledDotProductAttention"></a>1.2 ScaledDotProductAttention</h2><ul><li>用于计算缩放点积注意力， 在MultiHeadAttention中被调用。</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">ScaledDotProductionAttention</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(ScaledDotProductionAttention, self).__init__()</span><br><span class="line">        </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, Q, K, V, attn_mask</span>):</span><br><span class="line">        <span class="comment"># Q: [batch_size, n_heads, len_q, d_k]</span></span><br><span class="line">        <span class="comment"># K: [batch_size, n_heads, len_k, d_k]</span></span><br><span class="line">        <span class="comment"># V: [batch_size, n_heads, len_v(=len_k), d_v]</span></span><br><span class="line">        <span class="comment"># 总共使用两处自注意力， 一处时自注意力，一处时掩蔽自注意力</span></span><br><span class="line">        <span class="comment"># attn_mask: [batch_size, n_heads, seq_len, seq_len]</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 1、计算注意力分数QK^T/sqrt(d_k),</span></span><br><span class="line">        <span class="comment"># np.sqrt(d_k) 是一个平方根函数，它可以将一个非负实数的平方根提取出来。除以 sqrt(d_k) 来降低结果的数值大小，使其落入一个小范围。</span></span><br><span class="line">        <span class="comment"># 为什么要除一个根号d_k？这是因为点积的数量级增长很大，因此将 softmax 函数推向了梯度极小的区域</span></span><br><span class="line">        <span class="comment"># 这样做的目的是使softmax函数能够更加稳定地运行，从而提高模型的性能和鲁棒性。</span></span><br><span class="line">        scores = torch.matmul(Q, K.transpose(-<span class="number">1</span>, -<span class="number">2</span>)) / np.sqrt(d_k) </span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 2、进行mask和softmax</span></span><br><span class="line">        <span class="comment"># mask为True的地方会被设置为-1e9，相当于不考虑</span></span><br><span class="line">        scores.masked_fill_(attn_mask, -<span class="number">1e9</span>)</span><br><span class="line">        attn = nn.Softmax(dim=-<span class="number">1</span>)(scores) <span class="comment"># attn:[batch_szie, n_heads, len_q, len_k]</span></span><br><span class="line">        <span class="comment"># 3、乘V得到最终的加权和</span></span><br><span class="line">        context = torch.matmul(attn, V) <span class="comment"># context:[batch_size, n_heads, len_q, d_v]</span></span><br><span class="line">        <span class="comment"># 返回的context: [batch_size, n_heads, len_q, d_v]本质上还是batch_size个句子，</span></span><br><span class="line">        <span class="comment"># 只不过每个句子中词向量维度512被分成了8个部分，分别由8个头各自看一部分，每个头算的是整个句子(一列)的512/8=64个维度，最后按列拼接起来</span></span><br><span class="line">        </span><br><span class="line">        <span class="keyword">return</span> context <span class="comment"># [batch_size, n_heads, len_q, d_v]</span></span><br></pre></td></tr></table></figure><h2 id="1-3-多头自注意力-MultiHeadAttention"><a href="#1-3-多头自注意力-MultiHeadAttention" class="headerlink" title="1.3 多头自注意力 MultiHeadAttention"></a>1.3 多头自注意力 MultiHeadAttention</h2><ul><li>多头注意力的实现， Transformer的核心。</li><li>原来一个头对应512个特征，现在分为8个头，每个头处理64个特征<img src="/pic/transformers/2.png" width = "100%" height = "100%" /></li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">MultiHeadAttention</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(MultiHeadAttention, self).__init__()</span><br><span class="line">        self.W_Q = nn.Linear(d_model, d_model)</span><br><span class="line">        self.W_K = nn.Linear(d_model, d_model)</span><br><span class="line">        self.W_V = nn.Linear(d_model, d_model)</span><br><span class="line">        self.concat = nn.Linear(d_model, d_model)</span><br><span class="line">        </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, input_Q, input_K, input_V, attn_mask</span>):</span><br><span class="line">        <span class="comment"># input_Q:[batch_size, len_q, d_model] len_q是作为query的句子的长度，比如enc_inputs(2, 5, 512), 句子长度5就是len_q</span></span><br><span class="line">        <span class="comment"># input_K:[batch_size, len_k, d_model]</span></span><br><span class="line">        <span class="comment"># input_V:[batch_size, len_v(=len_k), d_model]</span></span><br><span class="line">        </span><br><span class="line">        residual, batch_size = input_Q, input_Q.size(<span class="number">0</span>)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 1、线性投影：通过原始输入，映射出QKV，然后调整形状，准备多个头，每个头对应64个特征</span></span><br><span class="line">        Q = self.W_Q(input_Q).view(batch_size, -<span class="number">1</span>, n_heads, d_k).transpose(<span class="number">1</span>, <span class="number">2</span>) <span class="comment"># Q:[batch_size, n_heads, len_q, d_k]</span></span><br><span class="line">        K = self.W_K(input_K).view(batch_size, -<span class="number">1</span>, n_heads, d_k).transpose(<span class="number">1</span>, <span class="number">2</span>) <span class="comment"># K:[batch_size, n_heads, len_k, d_k]</span></span><br><span class="line">        V = self.W_V(input_V).view(batch_size, -<span class="number">1</span>, n_heads, d_v).transpose(<span class="number">1</span>, <span class="number">2</span>) <span class="comment"># V:[batch_size, n_heads, len_v, d_k]</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 2、计算注意力</span></span><br><span class="line">        <span class="comment"># 先自我复制n_heads次，为每个头准备一份mask</span></span><br><span class="line">        attn_mask = attn_mask.unsqueeze(<span class="number">1</span>).repeat(<span class="number">1</span>, n_heads, <span class="number">1</span>, <span class="number">1</span>) <span class="comment"># attn_mask:[batch_size, n_heads, seq_len, seq_len]</span></span><br><span class="line">        context = ScaledDotProductionAttention()(Q, K, V, attn_mask) <span class="comment"># context:[batch_size, n_heads, len_q, d_v]</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 3、concat部分</span></span><br><span class="line">        context = torch.cat([context[:,i,:,:] <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(context.size(<span class="number">1</span>))], dim=-<span class="number">1</span>)</span><br><span class="line">        output = self.concat(context) <span class="comment"># [batch_size, len_q, d_model]</span></span><br><span class="line">        <span class="keyword">return</span> nn.LayerNorm(d_model).cuda()(output + residual) <span class="comment"># output:[batch_size, len_q, d_mdoel] 这一步加了残差</span></span><br></pre></td></tr></table></figure><h2 id="1-4-前馈网络-MLP-和层归一化FeedForward-Networks"><a href="#1-4-前馈网络-MLP-和层归一化FeedForward-Networks" class="headerlink" title="1.4 前馈网络(MLP)和层归一化FeedForward Networks"></a>1.4 前馈网络(MLP)和层归一化FeedForward Networks</h2><ul><li>对应Feed Forward和 Add &amp; Norm层归一化</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">PositionwiseFeedForward</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(PositionwiseFeedForward, self).__init__()</span><br><span class="line">        <span class="comment"># 前馈神经网络也就是MLP多层感知机</span></span><br><span class="line">        self.fc = nn.Sequential(</span><br><span class="line">           nn.Linear(d_model, d_ff),</span><br><span class="line">            nn.ReLU(),</span><br><span class="line">            nn.Linear(d_ff, d_model)</span><br><span class="line">        )</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, inputs</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;inputs:[batch_size, seq_len, d_model]&quot;&quot;&quot;</span></span><br><span class="line">        residual = inputs</span><br><span class="line">        output = self.fc(inputs)</span><br><span class="line">        <span class="keyword">return</span> nn.LayerNorm(d_model).cuda()(output + residual) <span class="comment"># 先残差连接，然后层归一化从右往左计算</span></span><br></pre></td></tr></table></figure><h2 id="1-5-Pad-Mask"><a href="#1-5-Pad-Mask" class="headerlink" title="1.5 Pad Mask"></a>1.5 Pad Mask</h2><ul><li>首先我们要清楚，这是一个计算mask的函数，它的返回是一个布尔矩阵，为True的位置是需要被mask掉的，False的位置是不需要动的</li><li>其次这个函数是理解Transformer代码中非常重要的一环，因为我们输入模型的句子有长有短，我们用占位符P统一补足成了最长的那个句子的长度，而这些占位符是没有意义的，不能让他们吸收到query的注意力，因此我们要把这些位置设为True</li><li>这个计算出的mask在何时被使用呢？<ul><li>在query和key的转置相乘得出（len_q,len_k）这个注意力分数矩阵以后，将使用本函数得到的mask来掩盖相乘结果矩阵</li><li>原来的相乘结果矩阵（len_q,len_k）中第 i 行第 j 列的意义是“作为q的序列中第i个词对作为k的序列中第j个词的注意力分数”，而第 i 整行就是q中这个词对k中所有词的注意力，第 j 整列就是q中所有词对k中第j个词的注意力分数，作为padding，q中的所有词都不应该注意它，因此对应列均需设为True</li></ul></li><li>为什么只有k的padding位被mask了，q的padding位为什么没被mask？（即此函数的返回矩阵为什么只有最后几列是True，最后几行不应该也是True么）<ul><li>按理来说是这样的，作为padding不该被别人注意，同时它也不该注意别人，计算出的padding对其他词的注意力也是无意义的，我们这里其实是偷了个懒，但这是因为：q中的padding对k中的词的注意力我们是不会用到的，因为我们不会用一个padding字符去预测下一个词，并且它的向量表示不管怎么更新都不会影响到别的q中别的词的计算，所以我们就放任自流了。但k中的padding不一样，如果不管它将无意义的吸收掉大量q中词汇的注意力，使得模型的学习出现偏差。</li></ul></li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 为enc_input和 dec_input做一个mask，把占位符P的token（也就是0）mask掉</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">get_attn_pad_mask</span>(<span class="params">seq_q, seq_k</span>):</span><br><span class="line">    batch_size, len_q = seq_q.size()</span><br><span class="line">    batch_size, len_k = seq_k.size()</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># seq_k.data.eq(0)返回一个相同大小的布尔张量，seq_k元素等于0的位置为True， 否则为False</span></span><br><span class="line">    <span class="comment"># .unsqueeze(1)扩充维度</span></span><br><span class="line">    pad_attn_mask = seq_k.data.eq(<span class="number">0</span>).unsqueeze(<span class="number">1</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 要为每一个q提供一份k， 所以把第二维度扩展了q次</span></span><br><span class="line">    <span class="comment"># expand并非真正加倍内存，知识重复了引用，对任意引用的修改都会修改原始值</span></span><br><span class="line">    <span class="comment"># 不修改这个mask，用来节省内存</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> pad_attn_mask.expand(batch_size, len_q, len_k) <span class="comment"># return :[batch_size, len_q, len_k]</span></span><br></pre></td></tr></table></figure><h1 id="2、定义Encoder"><a href="#2、定义Encoder" class="headerlink" title="2、定义Encoder"></a>2、定义Encoder</h1><ul><li>先定义一个Encoder Layer层：包含一个MultiHeadAttention和一个FFN。</li><li>叠加几次然后加上一个源序列词向量嵌入nn.Embedding、一个位置编码和6个Encoder Layer位置编码组成完整的Encoder。<img src="/pic/transformers/3.png" width = "40%" height = "40%" /></li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">EncoderLayer</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(EncoderLayer, self).__init__()</span><br><span class="line">        self.enc_self_attn = MultiHeadAttention()</span><br><span class="line">        self.pos_ffn = PositionwiseFeedForward()</span><br><span class="line">        </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, enc_inputs, enc_self_attn_mask</span>):</span><br><span class="line">        <span class="comment"># enc_inputs:[batch_size, scr_len, d_model]</span></span><br><span class="line">        <span class="comment"># enc_self_attn_mask:[batch_size, scr_len, src_len]</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Q,K,V均为enc_inputs</span></span><br><span class="line">        enc_ouputs = self.enc_self_attn(enc_inputs, enc_inputs, enc_inputs, enc_self_attn_mask) <span class="comment"># [batch_size, src_len, d_model]</span></span><br><span class="line">        enc_ouputs = self.pos_ffn(enc_ouputs) <span class="comment"># [batch_size, scr_len, d_model]</span></span><br><span class="line">        </span><br><span class="line">        <span class="keyword">return</span> enc_ouputs <span class="comment"># [batch_size,  src_len, d_mdoel]</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Encoder</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(Encoder, self).__init__()</span><br><span class="line">        <span class="comment"># 使用torch自带的词嵌入，输入是词元数和每个词元使用多少维特征表示</span></span><br><span class="line">        self.src_emb = nn.Embedding(src_vocab_size, d_model)</span><br><span class="line">        </span><br><span class="line">        self.pos_emb = PositionalEncoding(d_model)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 创建6个编码器，作为list</span></span><br><span class="line">        self.layers = nn.ModuleList([EncoderLayer() <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(n_layers)])</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, enc_inputs</span>):</span><br><span class="line">        <span class="comment"># enc_inputs:[batch_size, src_len]</span></span><br><span class="line">        <span class="comment"># 第一步词嵌入会将输入张量形状改变</span></span><br><span class="line">        enc_outputs = self.src_emb(enc_inputs) <span class="comment"># [batch_size, src_len]--&gt;[batch_size, src_len, d_model]</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 加入位置编码，张量形状不变</span></span><br><span class="line">        enc_outputs = self.pos_emb(enc_outputs) <span class="comment"># --&gt;[batch_size, src_len, d_model]</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 准备mask</span></span><br><span class="line">        enc_self_attn_mask = get_attn_pad_mask(enc_inputs, enc_inputs) <span class="comment"># [batch_size, src_len, src_len] 和输入形状一样</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 使用6个编码器</span></span><br><span class="line">        <span class="keyword">for</span> layer <span class="keyword">in</span> self.layers:</span><br><span class="line">            enc_outputs = layer(enc_outputs, enc_self_attn_mask)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> enc_outputs <span class="comment"># [batch_size, src_len, d_model]</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">encoder = Encoder()</span><br><span class="line"><span class="built_in">print</span>(encoder)</span><br></pre></td></tr></table></figure><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">Encoder(</span><br><span class="line">  (src<span class="emphasis">_emb): Embedding(6, 512)</span></span><br><span class="line"><span class="emphasis">  (pos_</span>emb): PositionalEncoding(</span><br><span class="line"><span class="code">    (dropout): Dropout(p=0.1, inplace=False)</span></span><br><span class="line"><span class="code">  )</span></span><br><span class="line"><span class="code">  (layers): ModuleList(</span></span><br><span class="line"><span class="code">    (0-5): 6 x EncoderLayer(</span></span><br><span class="line"><span class="code">      (enc_self_attn): MultiHeadAttention(</span></span><br><span class="line"><span class="code">        (W_Q): Linear(in_features=512, out_features=512, bias=True)</span></span><br><span class="line"><span class="code">        (W_K): Linear(in_features=512, out_features=512, bias=True)</span></span><br><span class="line"><span class="code">        (W_V): Linear(in_features=512, out_features=512, bias=True)</span></span><br><span class="line"><span class="code">        (concat): Linear(in_features=512, out_features=512, bias=True)</span></span><br><span class="line"><span class="code">      )</span></span><br><span class="line"><span class="code">      (pos_ffn): PositionwiseFeedForward(</span></span><br><span class="line"><span class="code">        (fc): Sequential(</span></span><br><span class="line"><span class="code">          (0): Linear(in_features=512, out_features=2048, bias=True)</span></span><br><span class="line"><span class="code">          (1): ReLU()</span></span><br><span class="line"><span class="code">          (2): Linear(in_features=2048, out_features=512, bias=True)</span></span><br><span class="line"><span class="code">        )</span></span><br><span class="line"><span class="code">      )</span></span><br><span class="line"><span class="code">    )</span></span><br><span class="line"><span class="code">  )</span></span><br><span class="line"><span class="code">)</span></span><br></pre></td></tr></table></figure><h1 id="3、定义Decoder"><a href="#3、定义Decoder" class="headerlink" title="3、定义Decoder"></a>3、定义Decoder</h1><h2 id="3-1-定义掩蔽自注意力Subsequence-Mask"><a href="#3-1-定义掩蔽自注意力Subsequence-Mask" class="headerlink" title="3.1 定义掩蔽自注意力Subsequence Mask"></a>3.1 定义掩蔽自注意力Subsequence Mask</h2><ul><li>对应Transformer模型架构中Decoder的第一个掩蔽注意力，防止模型看到未来时刻的输入</li><li>在推理的时候，是看不到还未推理的单词的所以在训练时也需要将未来时刻的词元掩蔽掉<img src="/pic/transformers/4.png" width = "60%" height = "60%" /></li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 用于获取对后续位置的掩码，防止在预测过程中看到未来时刻的输入</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">get_attn_subsequence_mask</span>(<span class="params">seq</span>):</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># seq:[batch_size, tgt_len]</span></span><br><span class="line">    <span class="comment"># batch_size个tgt_len * tgt_len的mask矩阵</span></span><br><span class="line">    attn_shape = [seq.size(<span class="number">0</span>), seq.size(<span class="number">1</span>), seq.size(<span class="number">1</span>)]</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># np.triu是生成一个upper traingular matrix上三角矩阵，K是相对主对角线的偏移量</span></span><br><span class="line">    <span class="comment"># k=1是之不包含住对角线</span></span><br><span class="line">    subsequence_mask = np.triu(np.ones(attn_shape), k=<span class="number">1</span>)</span><br><span class="line">    subsequence_mask = torch.from_numpy(subsequence_mask).byte() <span class="comment"># 因为只有0，1所以使用byte节省内存</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> subsequence_mask <span class="comment"># [batch_size, tgt_len, tgt_len]</span></span><br><span class="line"></span><br></pre></td></tr></table></figure><h2 id="3-2-DecoderLayer"><a href="#3-2-DecoderLayer" class="headerlink" title="3.2 DecoderLayer"></a>3.2 DecoderLayer</h2><ul><li>包含两个MultiHeadAttention和一个FFN<img src="/pic/transformers/5.png" width = "70%" height = "70%" /></li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">DecoderLayer</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(DecoderLayer, self).__init__()</span><br><span class="line">        self.dec_self_attn = MultiHeadAttention()</span><br><span class="line">        self.dec_enc_attn = MultiHeadAttention()</span><br><span class="line">        self.pos_ffn = PositionwiseFeedForward()</span><br><span class="line">        </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, dec_inputs, enc_outputs, dec_self_attn_mask, dec_enc_attn_mask</span>):</span><br><span class="line">        <span class="comment"># dec_inputs: [batch_size, tgt_len, d_model]</span></span><br><span class="line">        <span class="comment"># enc_outputs:[batch_size, src_len, d_model]</span></span><br><span class="line">        <span class="comment"># dec_self_attn_mask:[batch_size, tgt_len, d_model]</span></span><br><span class="line">        <span class="comment"># dec_enc_attn_mask:[batch_size, tgt_len, src_len] 前者是Q后者是K</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 第一个掩码自注意力，输入是上一个输出，所以QKV都是一样的</span></span><br><span class="line">        dec_outputs = self.dec_self_attn(dec_inputs, dec_inputs, dec_inputs, dec_self_attn_mask)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 第二个多头自注意力，并且使用的是Q，去查询编码器输出的K，V也是编码器输出的！</span></span><br><span class="line">        dec_outputs = self.dec_enc_attn(dec_outputs, enc_outputs, enc_outputs, dec_enc_attn_mask)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 对后一个是ffw层</span></span><br><span class="line">        dec_outputs = self.pos_ffn(dec_outputs)</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">return</span> dec_outputs <span class="comment"># [batch_size, tgt_len, d_model]</span></span><br></pre></td></tr></table></figure><h2 id="3-3-组合成Decoder"><a href="#3-3-组合成Decoder" class="headerlink" title="3.3 组合成Decoder"></a>3.3 组合成Decoder</h2><ul><li>包含一个目标序列词向量序列嵌入，一个位置编码和6个Decoder Layer。<img src="/pic/transformers/6.png" width = "50%" height = "50%" /></li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Decoder</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(Decoder, self).__init__()</span><br><span class="line">        self.tgt_emb = nn.Embedding(tgt_vocab_size, d_model)</span><br><span class="line">        self.pos_emb = PositionalEncoding(d_model)</span><br><span class="line">        self.layers = nn.ModuleList([DecoderLayer() <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(n_layers)])</span><br><span class="line">        </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, dec_inputs, enc_inputs, enc_outputs</span>):</span><br><span class="line">        <span class="comment"># dec_inputs:[batch_size, tgt_len] 对应Q</span></span><br><span class="line">        <span class="comment"># enc_inputs:[batch_size, src_len] 对应K V</span></span><br><span class="line">        <span class="comment"># enc_outputs:[batch_size, src_len, d_model] 用来计算padding mask</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 词嵌入</span></span><br><span class="line">        dec_outputs = self.tgt_emb(dec_inputs) </span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 添加位置编码，并转移到cuda</span></span><br><span class="line">        dec_outputs = self.pos_emb(dec_outputs).cuda() </span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 计算占位符需要的掩码</span></span><br><span class="line">        dec_self_attn_pad_mask = get_attn_pad_mask(dec_inputs, dec_inputs).cuda()  </span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 计算掩码自注意力需要的掩码</span></span><br><span class="line">        dec_self_attn_subsequence_mask = get_attn_subsequence_mask(dec_inputs).cuda()</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 将两个掩码合并在一起，大于0的位置是需要mask的, 这是第一个掩码自注意力需要的mask</span></span><br><span class="line">        dec_self_attn_mask = torch.gt((dec_self_attn_pad_mask + dec_self_attn_subsequence_mask), <span class="number">0</span>).cuda()</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 这是第二个多头自注意力需要的mask</span></span><br><span class="line">        dec_enc_attn_mask = get_attn_pad_mask(dec_inputs, enc_inputs)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 循环6个层</span></span><br><span class="line">        <span class="keyword">for</span> layer <span class="keyword">in</span> self.layers:</span><br><span class="line">            dec_outputs = layer(dec_outputs, enc_outputs, dec_self_attn_mask, dec_enc_attn_mask)</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">return</span> dec_outputs <span class="comment"># [batch_size, tgt_len, d_model]</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">decoder = Decoder()</span><br><span class="line"><span class="built_in">print</span>(decoder)</span><br></pre></td></tr></table></figure><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line">Decoder(</span><br><span class="line">  (tgt<span class="emphasis">_emb): Embedding(9, 512)</span></span><br><span class="line"><span class="emphasis">  (pos_</span>emb): PositionalEncoding(</span><br><span class="line"><span class="code">    (dropout): Dropout(p=0.1, inplace=False)</span></span><br><span class="line"><span class="code">  )</span></span><br><span class="line"><span class="code">  (layers): ModuleList(</span></span><br><span class="line"><span class="code">    (0-5): 6 x DecoderLayer(</span></span><br><span class="line"><span class="code">      (dec_self_attn): MultiHeadAttention(</span></span><br><span class="line"><span class="code">        (W_Q): Linear(in_features=512, out_features=512, bias=True)</span></span><br><span class="line"><span class="code">        (W_K): Linear(in_features=512, out_features=512, bias=True)</span></span><br><span class="line"><span class="code">        (W_V): Linear(in_features=512, out_features=512, bias=True)</span></span><br><span class="line"><span class="code">        (concat): Linear(in_features=512, out_features=512, bias=True)</span></span><br><span class="line"><span class="code">      )</span></span><br><span class="line"><span class="code">      (dec_enc_attn): MultiHeadAttention(</span></span><br><span class="line"><span class="code">        (W_Q): Linear(in_features=512, out_features=512, bias=True)</span></span><br><span class="line"><span class="code">        (W_K): Linear(in_features=512, out_features=512, bias=True)</span></span><br><span class="line"><span class="code">        (W_V): Linear(in_features=512, out_features=512, bias=True)</span></span><br><span class="line"><span class="code">        (concat): Linear(in_features=512, out_features=512, bias=True)</span></span><br><span class="line"><span class="code">      )</span></span><br><span class="line"><span class="code">      (pos_ffn): PositionwiseFeedForward(</span></span><br><span class="line"><span class="code">        (fc): Sequential(</span></span><br><span class="line"><span class="code">          (0): Linear(in_features=512, out_features=2048, bias=True)</span></span><br><span class="line"><span class="code">          (1): ReLU()</span></span><br><span class="line"><span class="code">          (2): Linear(in_features=2048, out_features=512, bias=True)</span></span><br><span class="line"><span class="code">        )</span></span><br><span class="line"><span class="code">      )</span></span><br><span class="line"><span class="code">    )</span></span><br><span class="line"><span class="code">  )</span></span><br><span class="line"><span class="code">)</span></span><br><span class="line"><span class="code"></span></span><br></pre></td></tr></table></figure><h1 id="4、将定义的编码器Enocder和解码器Decoder组合成Transformer"><a href="#4、将定义的编码器Enocder和解码器Decoder组合成Transformer" class="headerlink" title="4、将定义的编码器Enocder和解码器Decoder组合成Transformer"></a>4、将定义的编码器Enocder和解码器Decoder组合成Transformer</h1><ul><li>包含一个Encoder、一个Decoder、一个nn.Linear</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Transformer</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(Transformer, self).__init__()</span><br><span class="line">        self.encoder = Encoder().cuda()</span><br><span class="line">        self.decoder = Decoder().cuda()</span><br><span class="line">        self.projection = nn.Linear(d_model, tgt_vocab_size).cuda()</span><br><span class="line">        </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, enc_inputs, dec_inputs</span>):</span><br><span class="line">        <span class="comment"># enc_inputs:[batch_size, src_len]</span></span><br><span class="line">        <span class="comment"># dec_inputs:[batch_size, tgt_len]</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 编码器输入enc_inputs,输出enc_outputs</span></span><br><span class="line">        enc_outputs = self.encoder(enc_inputs)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 解码器输入dec_inputs-标注， enc_inputs-编码器的输入，为了计算mask，enc_outputs-编码器的输出为了计算多头注意力 </span></span><br><span class="line">        <span class="comment"># 输出是[batch_size, tgt_len, d_model]</span></span><br><span class="line">        dec_outputs = self.decoder(dec_inputs, enc_inputs, enc_outputs)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 将512维表示的token，转为换词表的长度，表示是每个词元的概率</span></span><br><span class="line">        dec_logits = self.projection(dec_outputs) <span class="comment"># dec_logits:[batch_size, tgt_len, tgt_vocab_size]</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 解散batch，一个batch中有batch_size个句子， 每个句子有tgt_len个词元</span></span><br><span class="line">        <span class="comment"># 将batch_size拉直，每个批次句子全部从上往下排列</span></span><br><span class="line">        <span class="comment"># 最后变形的原因是：nn.CrossEntropyLoss接收的输入的第二个维度必须是类别</span></span><br><span class="line">        <span class="keyword">return</span> dec_logits.view(-<span class="number">1</span>, dec_logits.size(-<span class="number">1</span>)) <span class="comment"># [batch_size*tgt_len, tgt_vocab_size]</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">summary(Transformer(), [(<span class="number">2</span>, <span class="number">5</span>), (<span class="number">2</span>, <span class="number">6</span>)], dtypes=[torch.long, torch.long])</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line">====================================================================================================</span><br><span class="line">Layer (<span class="built_in">type</span>:depth-idx)                             Output Shape              Param <span class="comment">#</span></span><br><span class="line">====================================================================================================</span><br><span class="line">Transformer                                        [<span class="number">12</span>, <span class="number">9</span>]                   --</span><br><span class="line">├─Encoder: <span class="number">1</span>-<span class="number">1</span>                                     [<span class="number">2</span>, <span class="number">5</span>, <span class="number">512</span>]               --</span><br><span class="line">│    └─Embedding: <span class="number">2</span>-<span class="number">1</span>                              [<span class="number">2</span>, <span class="number">5</span>, <span class="number">512</span>]               <span class="number">3</span>,072</span><br><span class="line">│    └─PositionalEncoding: <span class="number">2</span>-<span class="number">2</span>                     [<span class="number">2</span>, <span class="number">5</span>, <span class="number">512</span>]               --</span><br><span class="line">│    │    └─Dropout: <span class="number">3</span>-<span class="number">1</span>                           [<span class="number">2</span>, <span class="number">5</span>, <span class="number">512</span>]               --</span><br><span class="line">│    └─ModuleList: <span class="number">2</span>-<span class="number">3</span>                             --                        --</span><br><span class="line">│    │    └─EncoderLayer: <span class="number">3</span>-<span class="number">2</span>                      [<span class="number">2</span>, <span class="number">5</span>, <span class="number">512</span>]               <span class="number">3</span>,<span class="number">150</span>,<span class="number">336</span></span><br><span class="line">│    │    └─EncoderLayer: <span class="number">3</span>-<span class="number">3</span>                      [<span class="number">2</span>, <span class="number">5</span>, <span class="number">512</span>]               <span class="number">3</span>,<span class="number">150</span>,<span class="number">336</span></span><br><span class="line">│    │    └─EncoderLayer: <span class="number">3</span>-<span class="number">4</span>                      [<span class="number">2</span>, <span class="number">5</span>, <span class="number">512</span>]               <span class="number">3</span>,<span class="number">150</span>,<span class="number">336</span></span><br><span class="line">│    │    └─EncoderLayer: <span class="number">3</span>-<span class="number">5</span>                      [<span class="number">2</span>, <span class="number">5</span>, <span class="number">512</span>]               <span class="number">3</span>,<span class="number">150</span>,<span class="number">336</span></span><br><span class="line">│    │    └─EncoderLayer: <span class="number">3</span>-<span class="number">6</span>                      [<span class="number">2</span>, <span class="number">5</span>, <span class="number">512</span>]               <span class="number">3</span>,<span class="number">150</span>,<span class="number">336</span></span><br><span class="line">│    │    └─EncoderLayer: <span class="number">3</span>-<span class="number">7</span>                      [<span class="number">2</span>, <span class="number">5</span>, <span class="number">512</span>]               <span class="number">3</span>,<span class="number">150</span>,<span class="number">336</span></span><br><span class="line">├─Decoder: <span class="number">1</span>-<span class="number">2</span>                                     [<span class="number">2</span>, <span class="number">6</span>, <span class="number">512</span>]               --</span><br><span class="line">│    └─Embedding: <span class="number">2</span>-<span class="number">4</span>                              [<span class="number">2</span>, <span class="number">6</span>, <span class="number">512</span>]               <span class="number">4</span>,<span class="number">608</span></span><br><span class="line">│    └─PositionalEncoding: <span class="number">2</span>-<span class="number">5</span>                     [<span class="number">2</span>, <span class="number">6</span>, <span class="number">512</span>]               --</span><br><span class="line">│    │    └─Dropout: <span class="number">3</span>-<span class="number">8</span>                           [<span class="number">2</span>, <span class="number">6</span>, <span class="number">512</span>]               --</span><br><span class="line">│    └─ModuleList: <span class="number">2</span>-<span class="number">6</span>                             --                        --</span><br><span class="line">│    │    └─DecoderLayer: <span class="number">3</span>-<span class="number">9</span>                      [<span class="number">2</span>, <span class="number">6</span>, <span class="number">512</span>]               <span class="number">4</span>,<span class="number">200</span>,<span class="number">960</span></span><br><span class="line">│    │    └─DecoderLayer: <span class="number">3</span>-<span class="number">10</span>                     [<span class="number">2</span>, <span class="number">6</span>, <span class="number">512</span>]               <span class="number">4</span>,<span class="number">200</span>,<span class="number">960</span></span><br><span class="line">│    │    └─DecoderLayer: <span class="number">3</span>-<span class="number">11</span>                     [<span class="number">2</span>, <span class="number">6</span>, <span class="number">512</span>]               <span class="number">4</span>,<span class="number">200</span>,<span class="number">960</span></span><br><span class="line">│    │    └─DecoderLayer: <span class="number">3</span>-<span class="number">12</span>                     [<span class="number">2</span>, <span class="number">6</span>, <span class="number">512</span>]               <span class="number">4</span>,<span class="number">200</span>,<span class="number">960</span></span><br><span class="line">│    │    └─DecoderLayer: <span class="number">3</span>-<span class="number">13</span>                     [<span class="number">2</span>, <span class="number">6</span>, <span class="number">512</span>]               <span class="number">4</span>,<span class="number">200</span>,<span class="number">960</span></span><br><span class="line">│    │    └─DecoderLayer: <span class="number">3</span>-<span class="number">14</span>                     [<span class="number">2</span>, <span class="number">6</span>, <span class="number">512</span>]               <span class="number">4</span>,<span class="number">200</span>,<span class="number">960</span></span><br><span class="line">├─Linear: <span class="number">1</span>-<span class="number">3</span>                                      [<span class="number">2</span>, <span class="number">6</span>, <span class="number">9</span>]                 <span class="number">4</span>,<span class="number">617</span></span><br><span class="line">====================================================================================================</span><br><span class="line">Total params: <span class="number">44</span>,<span class="number">120</span>,073</span><br><span class="line">Trainable params: <span class="number">44</span>,<span class="number">120</span>,073</span><br><span class="line">Non-trainable params: <span class="number">0</span></span><br><span class="line">Total mult-adds (M): <span class="number">88.24</span></span><br><span class="line">====================================================================================================</span><br><span class="line">Input size (MB): <span class="number">0.00</span></span><br><span class="line">Forward/backward <span class="keyword">pass</span> size (MB): <span class="number">6.04</span></span><br><span class="line">Params size (MB): <span class="number">176.48</span></span><br><span class="line">Estimated Total Size (MB): <span class="number">182.52</span></span><br><span class="line">====================================================================================================</span><br></pre></td></tr></table></figure><h1 id="5、使用一个简单的翻译词典来测试模型"><a href="#5、使用一个简单的翻译词典来测试模型" class="headerlink" title="5、使用一个简单的翻译词典来测试模型"></a>5、使用一个简单的翻译词典来测试模型</h1><h2 id="5-1-数据预处理"><a href="#5-1-数据预处理" class="headerlink" title="5.1 数据预处理"></a>5.1 数据预处理</h2><ul><li>德语翻译成英文，自行构建词典<img src="/pic/transformers/7.png" width = "80%" height = "80%" /></li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># S:起始标记，E：结束标记，P:padding，将当前序列补齐之最长序列长度的占位符</span></span><br><span class="line">sentence = [</span><br><span class="line">    <span class="comment">#   enc_input编码器输入        dec_input             dec_output</span></span><br><span class="line">    [<span class="string">&#x27;ich mochte ein bier P&#x27;</span>, <span class="string">&#x27;S i want a beer .&#x27;</span>, <span class="string">&#x27;i want a beer . E&#x27;</span>], </span><br><span class="line">    [<span class="string">&#x27;ich mochte ein cola P&#x27;</span>, <span class="string">&#x27;S i want a coke .&#x27;</span>, <span class="string">&#x27;i want a coke . E&#x27;</span>],</span><br><span class="line">]    <span class="comment"># 德语：我想喝啤酒作为编码器输入，    </span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 构建词典， padding用0表表示</span></span><br><span class="line"><span class="comment"># 源词典</span></span><br><span class="line">src_vocab = &#123;<span class="string">&#x27;P&#x27;</span>:<span class="number">0</span>, <span class="string">&#x27;ich&#x27;</span>:<span class="number">1</span>, <span class="string">&#x27;mochte&#x27;</span>:<span class="number">2</span>, <span class="string">&#x27;ein&#x27;</span>:<span class="number">3</span>, <span class="string">&#x27;bier&#x27;</span>:<span class="number">4</span>, <span class="string">&#x27;cola&#x27;</span>:<span class="number">5</span>&#125;</span><br><span class="line">src_vocab_size = <span class="built_in">len</span>(src_vocab)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 目标词典（包含特殊字符）</span></span><br><span class="line">tgt_vocab = &#123;<span class="string">&#x27;P&#x27;</span>:<span class="number">0</span>, <span class="string">&#x27;i&#x27;</span>:<span class="number">1</span>, <span class="string">&#x27;want&#x27;</span>:<span class="number">2</span>, <span class="string">&#x27;a&#x27;</span>:<span class="number">3</span>, <span class="string">&#x27;beer&#x27;</span>:<span class="number">4</span>, <span class="string">&#x27;coke&#x27;</span>:<span class="number">5</span>, <span class="string">&#x27;S&#x27;</span>:<span class="number">6</span>, <span class="string">&#x27;E&#x27;</span>:<span class="number">7</span>, <span class="string">&#x27;.&#x27;</span>:<span class="number">8</span>&#125;</span><br><span class="line"><span class="comment"># 词典中编号和词元的转换</span></span><br><span class="line">idx2word = &#123;v:k <span class="keyword">for</span> k, v <span class="keyword">in</span> tgt_vocab.items()&#125;</span><br><span class="line">tgt_vocab_size = <span class="built_in">len</span>(tgt_vocab)</span><br><span class="line"></span><br><span class="line">src_len = <span class="number">5</span>  <span class="comment"># 设定输入序列enc_input的最长序列长度</span></span><br><span class="line">tgt_len = <span class="number">6</span>  <span class="comment"># 设定输出序列dec_input/dec_ouput的最长序列长度 </span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 将原始输入序列转换为token也就是序列号表示</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">make_data</span>(<span class="params">sentence</span>):</span><br><span class="line">    enc_inputs, dec_inputs, dec_outputs = [], [], []</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(sentence)):</span><br><span class="line">        enc_input = [src_vocab[word] <span class="keyword">for</span> word <span class="keyword">in</span> sentence[i][<span class="number">0</span>].split()]</span><br><span class="line">        dec_input = [tgt_vocab[word] <span class="keyword">for</span> word <span class="keyword">in</span> sentence[i][<span class="number">1</span>].split()]</span><br><span class="line">        dec_output = [tgt_vocab[word] <span class="keyword">for</span> word <span class="keyword">in</span> sentence[i][<span class="number">2</span>].split()]</span><br><span class="line">        </span><br><span class="line">        enc_inputs.append(enc_input)</span><br><span class="line">        dec_inputs.append(dec_input)</span><br><span class="line">        dec_outputs.append(dec_output)</span><br><span class="line">    <span class="comment"># torch.LongTensor专门用于储存整数型，tensor则可以表示浮点，整数等</span></span><br><span class="line">    <span class="keyword">return</span> torch.LongTensor(enc_inputs),torch.LongTensor(dec_inputs),torch.LongTensor(dec_outputs)</span><br><span class="line"></span><br><span class="line">enc_inputs, dec_inputs, dec_outputs = make_data(sentence)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27; enc_inputs: \n&#x27;</span>, enc_inputs)  <span class="comment"># enc_inputs: [2,5]</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27; dec_inputs: \n&#x27;</span>, dec_inputs)  <span class="comment"># dec_inputs: [2,6]</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27; dec_outputs: \n&#x27;</span>, dec_outputs) <span class="comment"># dec_outputs: [2,6]</span></span><br></pre></td></tr></table></figure><h2 id="5-2-加载数据定义超参数"><a href="#5-2-加载数据定义超参数" class="headerlink" title="5.2 加载数据定义超参数"></a>5.2 加载数据定义超参数</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 使用Dataset加载数据</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">MyDataSet</span>(Data.Dataset):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, enc_inputs, dec_inputs, dec_outputs</span>):</span><br><span class="line">        <span class="built_in">super</span>(MyDataSet, self).__init__()</span><br><span class="line">        self.enc_inputs = enc_inputs</span><br><span class="line">        self.dec_inputs = dec_inputs</span><br><span class="line">        self.dec_outputs = dec_outputs</span><br><span class="line">        </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__len__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="comment"># enc_inputs.shape = [2, 5], 所以返回2</span></span><br><span class="line">        <span class="keyword">return</span> self.enc_inputs.shape[<span class="number">0</span>]</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 根据idx返回的是一组enc_input, dec_input, dec_output</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__getitem__</span>(<span class="params">self, idx</span>):</span><br><span class="line">        <span class="keyword">return</span> self.enc_inputs[idx], self.dec_inputs[idx], self.dec_outputs[idx]</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 打包训练用的数据集，构建DataLoader</span></span><br><span class="line">loader = Data.DataLoader(dataset=MyDataSet(enc_inputs, dec_inputs, dec_outputs), batch_size=<span class="number">2</span>, shuffle=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 用来表示一个词的向量长度</span></span><br><span class="line">d_model = <span class="number">512</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># FFN的隐藏层的神经元个数</span></span><br><span class="line">d_ff = <span class="number">2048</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 分头后的q, k, v词向量对的长度，依照原文设为64</span></span><br><span class="line">d_k = d_v = <span class="number">64</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Encoder 和 Decoder的个数</span></span><br><span class="line">n_layers = <span class="number">6</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 多头注意力中head的个数</span></span><br><span class="line">n_heads = <span class="number">8</span></span><br></pre></td></tr></table></figure><h2 id="5-3-训练"><a href="#5-3-训练" class="headerlink" title="5.3 训练"></a>5.3 训练</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">&quot;&quot;&quot;训练&quot;&quot;&quot;</span></span><br><span class="line"><span class="comment">#实例化模型</span></span><br><span class="line">model = Transformer().cuda()</span><br><span class="line"><span class="comment"># 进入训练模式</span></span><br><span class="line">model.train()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义损失函数和优化器</span></span><br><span class="line">criterion = nn.CrossEntropyLoss(ignore_index=<span class="number">0</span>) <span class="comment"># 忽略为0的类别，因为这是padding，没有意义</span></span><br><span class="line">optimizer = optim.SGD(model.parameters(), lr=<span class="number">1e-3</span>, momentum=<span class="number">0.99</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 开始训练</span></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">10</span>):</span><br><span class="line">    <span class="keyword">for</span> enc_inputs, dec_inputs, dec_outputs <span class="keyword">in</span> loader:</span><br><span class="line">        <span class="comment"># 打包的数据中</span></span><br><span class="line">        <span class="comment"># enc_inputs:[batch_size, src_len][2,5] 是需要翻译的德文，作为编码器的输入</span></span><br><span class="line">        <span class="comment"># dec_inputs:[batch_size, tgt_len][2,6] 是带起始符翻译过后的英文，作为解码器的输入</span></span><br><span class="line">        <span class="comment"># dec_outputs:[batch_size, tgt_len][2,6]是带有结束符的翻译后的英文，作为标注用于计算损失</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 将数据转移到cuda中</span></span><br><span class="line">        enc_inputs, dec_inputs, dec_outputs = enc_inputs.cuda(), dec_inputs.cuda(), dec_outputs.cuda()</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 计算模型输出</span></span><br><span class="line">        outputs = model(enc_inputs, dec_inputs) <span class="comment"># outputs:[batch_size * tgt_len, tgt_vocab_size]</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 计算损失</span></span><br><span class="line">        loss = criterion(outputs, dec_outputs.view(-<span class="number">1</span>)) <span class="comment"># 将dec_outputs展平成一维张量</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 更新权重</span></span><br><span class="line">        optimizer.zero_grad()</span><br><span class="line">        loss.backward()</span><br><span class="line">        optimizer.step()</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 打印训练进度</span></span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&#x27;Epoch [<span class="subst">&#123;epoch + <span class="number">1</span>&#125;</span>/10],Loss:<span class="subst">&#123;loss.item()&#125;</span>&#x27;</span>)</span><br></pre></td></tr></table></figure><h2 id="5-4-测试"><a href="#5-4-测试" class="headerlink" title="5.4 测试"></a>5.4 测试</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 原文使用的是大小为4的束搜索，为简单起见这里使用贪心搜索</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">greedy_decoder</span>(<span class="params">model, enc_input, start_symbol</span>):</span><br><span class="line">    <span class="comment"># enc_input:[1, seq_len] 对应一句话</span></span><br><span class="line">    enc_outputs = model.encoder(enc_input) <span class="comment"># enc_outputs:[1, seq_len, 512]</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 生成一个1行0列的，和enc_inputs.data类型形同的空张量，待后续填充</span></span><br><span class="line">    dec_input = torch.zeros(<span class="number">1</span>, <span class="number">0</span>).type_as(enc_input.data) <span class="comment"># .data是为了避免影响梯度信息</span></span><br><span class="line">    </span><br><span class="line">    next_symbol = start_symbol</span><br><span class="line">    flag = <span class="literal">True</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">while</span> flag:</span><br><span class="line">        <span class="comment"># dec_input.detach() 创建dec_input的一个分离副本</span></span><br><span class="line">        <span class="comment"># 生成了一个只含有next_symbol的（1， 1）的张量</span></span><br><span class="line">        <span class="comment"># -1，表示在最火一个维度上进行拼接cat</span></span><br><span class="line">        <span class="comment"># 这行代码的作用是将next_symbol拼接到dec_input中，作为新一轮decoder的输入</span></span><br><span class="line">        dec_input = torch.cat([dec_input.detach(), torch.tensor([[next_symbol]], dtype=enc_input.dtype).cuda()], -<span class="number">1</span>)</span><br><span class="line">        </span><br><span class="line">        dec_outputs = model.decoder(dec_input,enc_input, enc_outputs) <span class="comment"># dec_outputs:[1, tgt_len, d_model]</span></span><br><span class="line">        </span><br><span class="line">        projected = model.projection(dec_outputs) <span class="comment"># projected:[1, 当前生成的tgt_len, tgt_vocab_size]</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># max返回的是一个元组（最大值，最大值对应的索引），使用[1]得到索引，也就是预测的下一个词元的索引</span></span><br><span class="line">        <span class="comment"># keepdim=False会导致减少一维</span></span><br><span class="line">        <span class="comment"># prob是一个一维列表，包含目前为止依次生成词的索引，最后一个是新生成的</span></span><br><span class="line">        prob = projected.squeeze(<span class="number">0</span>).<span class="built_in">max</span>(dim=-<span class="number">1</span>, keepdim=<span class="literal">False</span>)[<span class="number">1</span>]</span><br><span class="line">        </span><br><span class="line">        next_symbol = prob.data[-<span class="number">1</span>]</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">if</span> next_symbol == tgt_vocab[<span class="string">&#x27;.&#x27;</span>]:</span><br><span class="line">            flag = <span class="literal">False</span></span><br><span class="line">        <span class="built_in">print</span>(next_symbol)</span><br><span class="line">    <span class="keyword">return</span> dec_input <span class="comment"># [1, tgt_len]</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 测试</span></span><br><span class="line">model = torch.load(<span class="string">&#x27;MyTransformer.pth&#x27;</span>) <span class="comment"># 这种加载模型的方式不太好</span></span><br><span class="line">model.<span class="built_in">eval</span>()</span><br><span class="line"><span class="keyword">with</span> torch.no_grad():</span><br><span class="line">    <span class="comment"># 手动从loader中去一个batch的数据</span></span><br><span class="line">    enc_inputs, _, _, = <span class="built_in">next</span>(<span class="built_in">iter</span>(loader))</span><br><span class="line">    enc_inputs = enc_inputs.cuda()</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(enc_inputs)):</span><br><span class="line">        greedy_dec_input = greedy_decoder(model, enc_inputs[i].view(<span class="number">1</span>, -<span class="number">1</span>), start_symbol=tgt_vocab[<span class="string">&#x27;S&#x27;</span>])</span><br><span class="line">        predict = model(enc_inputs[i].view(<span class="number">1</span>, -<span class="number">1</span>), greedy_dec_input)</span><br><span class="line">        predict = predict.data.<span class="built_in">max</span>(dim=-<span class="number">1</span>, keepdim=<span class="literal">False</span>)[<span class="number">1</span>]</span><br><span class="line">        </span><br><span class="line">        <span class="built_in">print</span>(enc_inputs[i], <span class="string">&#x27;---&gt;&#x27;</span>, [idx2word[n.item()] <span class="keyword">for</span> n <span class="keyword">in</span> predict])</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">tensor(<span class="number">1</span>, device=<span class="string">&#x27;cuda:0&#x27;</span>)</span><br><span class="line">tensor(<span class="number">2</span>, device=<span class="string">&#x27;cuda:0&#x27;</span>)</span><br><span class="line">tensor(<span class="number">3</span>, device=<span class="string">&#x27;cuda:0&#x27;</span>)</span><br><span class="line">tensor(<span class="number">4</span>, device=<span class="string">&#x27;cuda:0&#x27;</span>)</span><br><span class="line">tensor(<span class="number">8</span>, device=<span class="string">&#x27;cuda:0&#x27;</span>)</span><br><span class="line">tensor([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">0</span>], device=<span class="string">&#x27;cuda:0&#x27;</span>) ---&gt; [<span class="string">&#x27;i&#x27;</span>, <span class="string">&#x27;want&#x27;</span>, <span class="string">&#x27;a&#x27;</span>, <span class="string">&#x27;beer&#x27;</span>, <span class="string">&#x27;.&#x27;</span>]</span><br><span class="line">tensor(<span class="number">1</span>, device=<span class="string">&#x27;cuda:0&#x27;</span>)</span><br><span class="line">tensor(<span class="number">2</span>, device=<span class="string">&#x27;cuda:0&#x27;</span>)</span><br><span class="line">tensor(<span class="number">3</span>, device=<span class="string">&#x27;cuda:0&#x27;</span>)</span><br><span class="line">tensor(<span class="number">5</span>, device=<span class="string">&#x27;cuda:0&#x27;</span>)</span><br><span class="line">tensor(<span class="number">8</span>, device=<span class="string">&#x27;cuda:0&#x27;</span>)</span><br><span class="line">tensor([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">5</span>, <span class="number">0</span>], device=<span class="string">&#x27;cuda:0&#x27;</span>) ---&gt; [<span class="string">&#x27;i&#x27;</span>, <span class="string">&#x27;want&#x27;</span>, <span class="string">&#x27;a&#x27;</span>, <span class="string">&#x27;coke&#x27;</span>, <span class="string">&#x27;.&#x27;</span>]</span><br><span class="line"></span><br></pre></td></tr></table></figure>]]></content>
    
    
    <summary type="html">&lt;p&gt;经典的transformer架构如下图所示，分为编码器和解码器。构建模型的过程中，先构建通用的模块，如位置编码、多头自注意力、前馈神经网络层等，然后组建编码器和解码器，最后统一到一个模型中。&lt;br&gt;&lt;img src=&quot;/pic/transformers/1.png&quot; width = &quot;60%&quot; height = &quot;60%&quot; /&gt;&lt;/p&gt;</summary>
    
    
    
    
  </entry>
  
  <entry>
    <title>AGI市场发展</title>
    <link href="http://example.com/2024/04/12/AGI%E5%B8%82%E5%9C%BA%E5%8F%91%E5%B1%95/"/>
    <id>http://example.com/2024/04/12/AGI%E5%B8%82%E5%9C%BA%E5%8F%91%E5%B1%95/</id>
    <published>2024-04-12T13:00:56.000Z</published>
    <updated>2024-04-12T13:20:22.752Z</updated>
    
    <content type="html"><![CDATA[<p>&emsp;&emsp;尽管面临泡沫质疑，资金仍在涌向大模型。科技巨头正在为通往通用人工智能的基础设施添砖加瓦。英伟达创造算力硬件，微软、亚马逊与谷歌分发这些算力，四家公司的总市值从ChatGPT刚推出时的4万亿美元，迈向10万亿美元，成为推动美国股市上涨的主导力量。在风险投资市场，几乎只有人工智能相关初创企业才能享受到估值的提升。</p><p>&emsp;&emsp;在估值不断上升中，人们开始担心，这一切是否能够持续。红杉资本年度演讲递出了“刀子”：去年企业投入500亿美元买的GPU，最终应用收回来30亿美元。</p><p>&emsp;&emsp;暂时无法盈利的创新，甚至最终无法盈利的创新，算是一次泡沫吗？在创新最终盈利之前，市场必然需要迎来一次泡沫吗？这一切取决于我们是否把生成式AI的突破所代表的通用人工智能（AGI），看成是一次真正的技术革命。</p><span id="more"></span><p><strong>通用人工智能是技术革命</strong><br>&emsp;&emsp;当下，沿着扩展定律发展的GPT模式，冒出了通用人工智能的火花。人们越来越相信作为新一代“通用技术”的AGI正在到来，将广泛影响各行各业。金融科技公司Klarna用大模型技术处理了2&#x2F;3的客户服务；<a href="http://mp.weixin.qq.com/s?__biz=MzkzMzI2ODAyNQ==&mid=2247488373&idx=1&sn=606df2754f98a4391b31c4369400da63&chksm=c24e4c23f539c5351a3ca1db4434e87f6cb050ccc34a5c6aa2ffb038bdb51621777c62172c3f&scene=21#wechat_redirect"><strong>首个AI软件工程师Devin</strong></a>可以用智能体独立完成相当数量的编码任务，危及码农饭碗；Sora将希望从理解文本信息扩展到理解物理世界。红杉资本估计，在探索通用人工智能的短短一年里，最终用户已经创造了30亿美元的总收入，这还不算云巨头以及Claude这样的大模型服务。要知道，SaaS市场用了近10年才达到这个规模。如此迅猛的速度与规模，让红杉资本确信，AI应用将有万亿级的潜力。如果将采购GPU的500亿美元按5年分摊，每年应该是100亿美元。<br>&emsp;&emsp;我们处于佩蕾丝（Carlota Perez）所谓的“技术革命”的早期。在她的定义里，技术革命是一组相互关联的技术，各自超越了所发端的行业，扩散到更广泛的领域，形成新的技术-经济范式。很难在完整的“技术革命”呈现前，描绘并列举那些缺失的拼图，比如超导或者核聚变等，曾在100天内惊动世界，也许将是低成本智能算力密集型经济的重要一环。<br>&emsp;&emsp;ChatGPT似乎就是率先到来的“大爆炸”时刻，相当于50年前英特尔的微处理器问世，开启了信息与互联网革命。它脱胎于信息与通讯技术（ICT）革命，但具备了数字时代升化出来的智能，让机器具备学习、预测、推理能力。AI与以往不同之处，在于它能帮助人类更好地应用现有的存量技术，加速存量技术的组合创新，并且与人类一起创造新的技术。目前可以看到的技术-经济范式中，有AI通过智能体技术与自然语言交互体验，引发一场白领革命，也有通过具身智能与人形机器人技术，弥补蓝领岗位缺口。</p><p><strong>脱离业务酿就泡沫</strong><br>&emsp;&emsp;在这一轮AI技术革命中，科技巨头是产业资本的代表，并且在自己的生态中，实现了产业资本与金融资本的融合。云巨头是web与移动技术革命的赢家，积累了大量的“闲置”财富，账面上拥有成百上千亿美元的现金。这些<a href="http://mp.weixin.qq.com/s?__biz=MzkzMzI2ODAyNQ==&mid=2247487907&idx=1&sn=a64fca0850bd18d6c8563a1f55381340&chksm=c24e4ef5f539c7e32a0f58271bbc28da87e11945bb1557b1a175fff69ed6d72983e43785d6bb&scene=21#wechat_redirect"><strong>科技巨头之间的竞争</strong></a>，已经从各自所擅长的数字经济领域，汇集到AI这技术革命趋势上，并非偶然，而是数字经济经过几十年发展而出现的一个不可避免的结果(historic imperative)。对于每一家科技巨头来说，AI是创新方向，它们做好了试验的准备，希望新的技术革命，增强和扩展其大部分现存业务。投资和收购初创AI企业，是其最重要的手段。去年，它们对AI初创企业的投资次数，比前一年多了57%，投资金额两倍于传统风投。今年，已经控制了OpenAI技术使用的微软，与独角兽开源AI企业<a href="http://mp.weixin.qq.com/s?__biz=MzkzMzI2ODAyNQ==&mid=2247488267&idx=1&sn=56391a5dcb78796622db458f390b4720&chksm=c24e4c5df539c54b712508b39b4a7845a3ebdf53aa1b92b1bf1f39dfc37f24bb35587edf7c6b&scene=21#wechat_redirect"><strong>Mistral达成深度合作</strong></a>；亚马逊兑现了对Anthropic总额40亿美元的第二笔投资款。巨头们采用了一种所谓“云洗钱”的方式，将投资消化在自己的业务体系内。它们从英伟达购买算力硬件GPU芯片，去年全年接近500亿美元，构建以AI加速为中心的智能数据中心，以“云信用”入股上述初创企业。后者在此之上训练越来越强大的大模型，吸引越来越多用户使用它们的API服务与AI推理。这些都表现为云巨头的云服务业务的增长，<a href="http://mp.weixin.qq.com/s?__biz=MzkzMzI2ODAyNQ==&mid=2247488077&idx=1&sn=067e767c6588d847f937cf0d92d9ea1e&chksm=c24e4d1bf539c40d0b1bb7b9005df2b620c0d01336ed9d08fb7af06666bc93b23a27a0ea687d&scene=21#wechat_redirect"><strong>体现在每一个季度的财报上</strong></a>，拉动了AI 计算业务的增长，也推升了公司股价的增长。<br><img src="/pic/AGI市场发展/1.png" width = "100%" height = "100%" /></p><h6 id="来源：CB-Insights"><a href="#来源：CB-Insights" class="headerlink" title="来源：CB Insights"></a>来源：CB Insights</h6><p>&emsp;&emsp;这意味着即使科技巨头在大模型企业的股权，最终无法退出兑现，就早已通过隐秘的“云洗钱”，完成了投入、收入与资本收益的正向循环。公司市值的提升，为再融资、人才激励、换股并购创造了更强的杠杆，有了一点金融游戏的意味。因为股价持续下跌，特斯拉已经挡不住AI人才跳槽去其他更豪爽的公司；马斯克只好让他们转岗到旗下xAI。如果巨头们在初创公司的股权兑现，那么就能再赢一次，毕竟微软投资的OpenAI，估值已经达到了860亿美元。<br>&emsp;&emsp;按照佩蕾丝的理论，科技巨头的这些投资，基本上属于“生产资本”的范畴。它最明显的特征，就是围绕业务展开，通过不断增加对创新和扩张的投资，积累越来越大的盈利能力。正如微软逐步将其Copilot融入所有业务，谷歌在推出更先进的大模型Gemini后，开始加速商业化，正在考虑对人工智能驱动的搜索收费。搜索广告及其他，是谷歌广告收入也是公司收入的核心来源，去年收入1750亿美元，占谷歌广告收入的 74%，占 2023 年总收入的 57%。更不用说Meta所有的广告业务，都建立在AI技术平台之上。<br>&emsp;&emsp;而金融资本本身不介入生产，致力于以货币的形式拥有财富，并使其增殖（multiplication）。金融资本天然追求财富，冒险寻找赔率最高、流动性最大的机会，即以风险资本的形式，下注到创新之中。这使得创新者能够尽情将自己的创新转变为商业现实，在不成熟的技术、不同的方向和未显露的市场上不断试错。其间，也混入了不少投机分子。创新始终伴随着失败与投机，进而产生了泡沫。<br>&emsp;&emsp;随着非理性的全面“繁荣”的到来，金融资本创造财富越来越不需要生产资本，实现了自我增值，将垃圾资产包装一下卖给下一个金融资本，泡沫到来了并且通向崩溃的结局。历史上发生的主要技术革命，都伴随着资本市场上的泡沫。<br><strong>放大镜中的泡沫</strong><br>&emsp;&emsp;如今，二级市场的金融资本对AI的亢奋程度被社交媒体和短视频放大了。金融数据提供商PitchBook回顾称，如果遵循历史趋势，去年应该有21 家AI相关公司IPO上市，但实际只有8家。市场热衷于英伟达与微软等确定性较高的巨头，并不全然相信高风险、亏损公司讲述的人工智能的故事。这与互联网泡沫前夕的市场表现并不一致，当时，只需要一个好听的公司名，股价就能暴涨。<br>&emsp;&emsp;一级市场也没有达到狂热的程度。有限合伙人（LP）们仍然谨慎。今年一季度，全球风险投资公司只募集了304 亿美元，与去年相比明显放缓。去年已经是自2016年以来的最低谷。这也将影响投资节奏。早期风险投资机构500 Global就诟病科技巨头以“云信用”入股，附带复杂交易条款的模式。<br>&emsp;&emsp;自动编写代码的初创企业Devin，在Demo其产品后不到三周，股值就增长了数倍，达到20亿美元。在AI创投领域，资本会更加集中于少数明星产品和团队，它们最终可能会独立成为超级平台，也可能与某个科技巨头融为一体。而更大量的AI应用初创公司的技术，可能很快被科技巨头的技术或者开源技术所取代。<a href="http://mp.weixin.qq.com/s?__biz=MzkzMzI2ODAyNQ==&mid=2247488419&idx=1&sn=9d10769abf76061b3b02ae75be61bcb9&chksm=c24e4cf5f539c5e3f9bb515ce0b6c444275403ad39c2930afd1d6720fec0de9fdeb4db184eff&scene=21#wechat_redirect"><strong>微软掏空Inflection的“雇佣收购”</strong></a>，反映出AI创投领域出现新特征：以最快的速度成为独角兽野兽，再以最快的速度被并购，或者宣布失败；而那些失败的初创公司，其团队和人才仍然被大型科技公司所器重。<br>&emsp;&emsp;充满不确定性的初创公司，想要融资并不容易。奥特曼支持的初创企业Ghost Autonomy，主要利用大型语言模型研发端到端的自动驾驶技术，因为长期盈利前景不明朗宣布关闭。内乱不断的Stability AI接近倒闭，核心人员几乎全部离职。生成式AI驱动的搜索引擎Perplexity，已经开始计划销售广告。OpenAI也希望2024年是“企业年”，正式宣布了辅助微调，帮助企业根据特定需求自定义大模型。<br>&emsp;&emsp;中国的传统风险投资机构比美国同行更为审慎。今年一季度，以金沙江创投的朱啸虎为代表，讲述了一个中国发展大模型的现实主义的故事。这位投出了滴滴和饿了么的投资人，现在信仰马上能商业化的AI 应用。<br><strong>泡沫的Scaling Law</strong><br>&emsp;&emsp;互联网泡沫时代铺设了大量闲置的光缆，降低了互联网服务的成本。而算力基础设施尚处于供不应求的阶段，更强大的大模型以及更大规模的应用场景，需要更低成本的智能算力。对于AI基础设施的投资还将继续增加，它的发展在一段时间内超前于应用的发展。如果这些被称为泡沫的话，那么通往通用人工智能将无法避免泡沫。扩展定律是最有可能通往通用人工智能的道路，它需要越来越强大的算力，也就需要生产资本与金融资本的持续投入。EpochAI最新的论文认为，算力和训练数据的增长，贡献了大模型性能提升的60-95%。<br>&emsp;&emsp;大模型正在变成智能算力密集的行业。在2020年，只有2个模型超过了10^23FLOP，到2024年，这样的模型数量增长到了81个；最强大模型的预训练算力需求已经来到了10^26 FLOP量级，并继续按训练算力需求每3.4个月翻倍的规律往前发展。<br><img src="/pic/AGI市场发展/2.png" width = "100%" height = "100%" /><br>&emsp;&emsp;随着推理需求占比提升，算力供给矛盾将进一步凸显。Sora一旦推出，据估算<a href="http://mp.weixin.qq.com/s?__biz=MzkzMzI2ODAyNQ==&mid=2247488441&idx=1&sn=ed021fa497ef63eaf16040c13c6f0bf7&chksm=c24e4ceff539c5f919548e60df2e5a77c10002c65395f41e3313bfdf3efd97bbfb530854c7e8&scene=21#wechat_redirect"><strong>峰值算力得备足72万张H100</strong></a>，仅GPU的投资就需要274亿美元。这意味着基础设施不足，而并非互联网泡沫时代的闲置。微软将向<a href="http://mp.weixin.qq.com/s?__biz=MzkzMzI2ODAyNQ==&mid=2247488495&idx=1&sn=8554f844da6500594069dd5c9f4bfda8&chksm=c24e4cb9f539c5af3a1d0bd40d27cf01484c1fbb6f59b3cf16358c37d763ebb704f32ecabe0c&scene=21#wechat_redirect"><strong>星门 （Stargate）计划投资1150亿美元</strong></a>，为OpenAI打造算力数量级几倍提升的数据中心，以支持其持续扩展AI系统规模。谷歌也有类似计划。算力密集意味着能源密集，在碳中和的目标下，就需要更为可持续的能源基础设施；规模化的分布式的推理需求，同时要求调度更为智能的能源基础设施。<br>&emsp;&emsp;数据基础设施及其工具的建设，也需要资金投入。目前，根据买家与类型不同，一张图片的价格在几美分至1美元不等，短视频每部2至4美元，长视频每小时100-300美元。如果将所有基础设施建设都纳入整个技术革命的范畴，而不是仅仅投资GPU，那么，也许据说<a href="http://mp.weixin.qq.com/s?__biz=MzkzMzI2ODAyNQ==&mid=2247488118&idx=1&sn=66641e67de0ac9ae4021b4eddc5e7770&chksm=c24e4d20f539c436509f4a095a247d3ad4be872e102fee7aec81e53858f72c93f4e9a04ba869&scene=21#wechat_redirect"><strong>奥特曼所说的7万亿美元</strong></a>是个接近答案的数字。<br>&emsp;&emsp;更多通向AGI的路径需要探索。即使是那些批评大模型的如加里·马库斯（Gary Marcus），其实是认为通往通用人工智能的道路不止GPT模式一条；DeepMind联合创始人哈萨比斯(Demis Hassabis)也认为，对科学研究的“炒作”还很不够。<br>&emsp;&emsp;目前的产业资本与金融资本，在各大科技巨头内部实现了集成。AI带来的资本效应，更加集中于M7 （美国七大科技巨头）。对于中国来说，政府在产业资本、金融资本、资本市场方面的扮演了日益重要的角色。中国提出“人工智能+”，正在抓紧建设算力、能源与数据的基础设施。去年底，工信部等6部门印发《算力基础设施高质量发展行动计划》，提及中国大陆的算力规模要从2023年的220 EFLOPS，增长到2025年超过300 EFLOPS，其中智能算力占比从25%提升至35%。<br>&emsp;&emsp;中国的产业资本与金融资本也需要“有序扩展”。今年一季度，阿里巴巴连续下注，投资了两家MiniMax与Moonshot AI（月之暗面）。至此，它已经覆盖了包括百川智能、智谱AI与零一万物等五家本土大模型独角兽企业。腾讯投资了其中三家。这两家云巨头的速度与幅度仍然比不上美国同行，市值之和停留在5000亿美元，仅为微软与亚马逊市值之和的1&#x2F;10。<br>&emsp;&emsp;中国要从钢筋水泥的“基建狂魔”摇身变为智能算力的“基建狂魔”，提供足够成熟与性价比的基础设施，包括通用人工智能推向各行各业，这一过程中，必然伴随着大量的试错与失败、亢奋与投机，来自科技巨头与金融资本从未来看到现在的信仰，甚至整个新兴领域的泡沫，恐怕是无法逾越阶段。</p><p>所有的技术革命，总是押着同样的韵脚展开。</p><p>转载自<a href="https://mp.weixin.qq.com/s/m-iktx3qQzfhWm1f7dPorA">未尽研究</a></p>]]></content>
    
    
    <summary type="html">&lt;p&gt;&amp;emsp;&amp;emsp;尽管面临泡沫质疑，资金仍在涌向大模型。科技巨头正在为通往通用人工智能的基础设施添砖加瓦。英伟达创造算力硬件，微软、亚马逊与谷歌分发这些算力，四家公司的总市值从ChatGPT刚推出时的4万亿美元，迈向10万亿美元，成为推动美国股市上涨的主导力量。在风险投资市场，几乎只有人工智能相关初创企业才能享受到估值的提升。&lt;/p&gt;
&lt;p&gt;&amp;emsp;&amp;emsp;在估值不断上升中，人们开始担心，这一切是否能够持续。红杉资本年度演讲递出了“刀子”：去年企业投入500亿美元买的GPU，最终应用收回来30亿美元。&lt;/p&gt;
&lt;p&gt;&amp;emsp;&amp;emsp;暂时无法盈利的创新，甚至最终无法盈利的创新，算是一次泡沫吗？在创新最终盈利之前，市场必然需要迎来一次泡沫吗？这一切取决于我们是否把生成式AI的突破所代表的通用人工智能（AGI），看成是一次真正的技术革命。&lt;/p&gt;</summary>
    
    
    
    
  </entry>
  
  <entry>
    <title>梯度消失和梯度爆炸</title>
    <link href="http://example.com/2024/04/10/%E6%A2%AF%E5%BA%A6%E6%B6%88%E5%A4%B1%E5%92%8C%E6%A2%AF%E5%BA%A6%E7%88%86%E7%82%B8/"/>
    <id>http://example.com/2024/04/10/%E6%A2%AF%E5%BA%A6%E6%B6%88%E5%A4%B1%E5%92%8C%E6%A2%AF%E5%BA%A6%E7%88%86%E7%82%B8/</id>
    <published>2024-04-10T10:37:26.000Z</published>
    <updated>2024-04-10T11:29:54.633Z</updated>
    
    <content type="html"><![CDATA[<h2 id="1、什么是梯度消失和梯度爆炸？"><a href="#1、什么是梯度消失和梯度爆炸？" class="headerlink" title="1、什么是梯度消失和梯度爆炸？"></a>1、什么是梯度消失和梯度爆炸？</h2><ul><li>梯度消失：在每次更新时，参数更新过小几乎不会移动，导致模型无法学习。</li><li>梯度爆炸：参数更新过大，破坏了模型的稳定收敛。<span id="more"></span></li></ul><h2 id="2、为什么会出现梯度消失和梯度爆炸？"><a href="#2、为什么会出现梯度消失和梯度爆炸？" class="headerlink" title="2、为什么会出现梯度消失和梯度爆炸？"></a>2、为什么会出现梯度消失和梯度爆炸？</h2><p>&emsp;&emsp;根本原因是深层次的神经网络模型的权重在反向传播过程中梯度的传播特性。层层网络下，参数矩阵在反向传播中反复累积的相乘，容易受到数值下溢问题的影响，当将太多的概率乘在一起时，它们的乘积可能非常大，也可能非常小。<br><img src="/pic/gradient/1.png" width = "50%" height = "50%" /></p><ul><li>不合适的初始化参数会导致梯度的消失和爆炸。</li><li>长期依赖问题，RNN中序列过长，梯度可能会逐渐衰减，远距离依赖的信息无法传播从而导致梯度消失。</li><li>激活函数选择不合适，例如sigmoid是以前网络中导致梯度消失的一个常见原因。</li><li>网络复杂度过高，参数量过大，可能会导致梯度值非常大，需要特殊的技巧来处理深层多参数模型。</li></ul><h2 id="3、如何缓解梯度消失或梯度爆炸问题？"><a href="#3、如何缓解梯度消失或梯度爆炸问题？" class="headerlink" title="3、如何缓解梯度消失或梯度爆炸问题？"></a>3、如何缓解梯度消失或梯度爆炸问题？</h2><p>针对梯度消失：</p><ul><li>使用合适的参数初始化方法，如Xavier或者He初始化方法。</li><li>使用合适的激活函数，如目前常用的ReLU系列等。</li><li>使用批量归一化。</li></ul><p>针对梯度爆炸：</p><ul><li>梯度裁剪，限制梯度的大小。</li><li>加入权重正则化，在损失函数中加入L1&#x2F;L2正则项，限制权重大小</li><li>合适的参数初始化方法，同上面。</li><li>使用更小的学习率。</li></ul><h2 id="4、题外：正则化"><a href="#4、题外：正则化" class="headerlink" title="4、题外：正则化"></a>4、题外：正则化</h2><p>正则化是为了防止模型过拟合，提高其泛化能力的方法。<br>有以下一些正则化方法：</p><ul><li>在损失函数中加入权重的L2正则惩罚项。</li><li>数据增强。</li><li>Dropout。</li><li>批量归一化。</li><li>权重衰减。</li><li>早停技术。</li></ul>]]></content>
    
    
    <summary type="html">&lt;h2 id=&quot;1、什么是梯度消失和梯度爆炸？&quot;&gt;&lt;a href=&quot;#1、什么是梯度消失和梯度爆炸？&quot; class=&quot;headerlink&quot; title=&quot;1、什么是梯度消失和梯度爆炸？&quot;&gt;&lt;/a&gt;1、什么是梯度消失和梯度爆炸？&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;梯度消失：在每次更新时，参数更新过小几乎不会移动，导致模型无法学习。&lt;/li&gt;
&lt;li&gt;梯度爆炸：参数更新过大，破坏了模型的稳定收敛。</summary>
    
    
    
    
  </entry>
  
  <entry>
    <title>BatchNorm批量归一化的简单介绍</title>
    <link href="http://example.com/2024/03/31/BatchNorm%E6%89%B9%E9%87%8F%E5%BD%92%E4%B8%80%E5%8C%96%E7%9A%84%E7%AE%80%E5%8D%95%E4%BB%8B%E7%BB%8D/"/>
    <id>http://example.com/2024/03/31/BatchNorm%E6%89%B9%E9%87%8F%E5%BD%92%E4%B8%80%E5%8C%96%E7%9A%84%E7%AE%80%E5%8D%95%E4%BB%8B%E7%BB%8D/</id>
    <published>2024-03-31T08:10:44.000Z</published>
    <updated>2024-04-01T11:06:00.202Z</updated>
    
    <content type="html"><![CDATA[<p>&emsp;&emsp;批量归一化(BatchNorm: BN)在搭建深度网络模型中是必不可少的一层，刚开始学习的时候还对其有所了解，后面渐渐的只会使用，已经忘记BN的真正作用和起作用的原理了。现在查找资料进行一个简单的梳理。</p><span id="more"></span><h3 id="1、Batch-Norm再模型中起什么作用？"><a href="#1、Batch-Norm再模型中起什么作用？" class="headerlink" title="1、Batch Norm再模型中起什么作用？"></a>1、Batch Norm再模型中起什么作用？</h3><ul><li>减少模型在训练过程中梯度消失或梯度爆炸等现象的发生，让训练过程更加稳定。</li><li>加快模型的收敛速度。</li></ul><h3 id="2、Batch-Norm在模型中是怎样做的计算？"><a href="#2、Batch-Norm在模型中是怎样做的计算？" class="headerlink" title="2、Batch Norm在模型中是怎样做的计算？"></a>2、Batch Norm在模型中是怎样做的计算？</h3><p>&emsp;&emsp;在卷积网络中，BN一般用在卷积层和非线性激活层之间。假设输入经过卷积计算后输出为(M, C, H, W)，其中M为batch大小，C为每个样本的通道维度，H* W张量的长宽。<br><img src="/pic/BN/1.png" width = "100%" height = "100%" /><br>BN要做的就是在每个通道上例如C1(灰色层)上将所有批量中的样本H* W个值拿出来，计算均值和方差：</p><ul><li>计算均值：<img src="/pic/BN/2.png" width = "50%"  /></li><li>计算方差：<img src="/pic/BN/3.png" width = "50%"  /></li></ul><p>从形式上来说，小批量中的一个样本x输入到批量规范化BN后，根据以下表达式对x进行转换：<br><img src="/pic/BN/4.png" width = "50%"  /><br> 拉伸参数和_偏移参数，它们的形状与x相同，是需要与其他模型参数一起学习的参数。</p><h3 id="3、批量归一化为什么会起作用？"><a href="#3、批量归一化为什么会起作用？" class="headerlink" title="3、批量归一化为什么会起作用？"></a>3、批量归一化为什么会起作用？</h3><p>&emsp;&emsp;为什么这项技术如此有效？批量规范化论文的作者还解释了其原理：通过减少_内部协变量偏移_（internal covariate shift）。不论是沿着从输入到输出的层，跨同一层中的单元，或是随着时间的推移，模型参数的随着训练更新变幻莫测，这些变量分布中的这种偏移可能会阻碍网络的收敛。但也有很多对这种解释的质疑，现在不进行讨论了：</p><ul><li>经过网络中前置层的变换，当前层输入的分布改变，影响了训练；Batch Norm方法所以有效，是由于在非线性层之前，通过控制各层输入分布的均值和方差，来稳定各层输入的分布，损失和梯度都减少了抖动，优化损失的解空间更平滑，从而促进了训练效果。</li><li>更深层的网络很复杂，容易过拟合，正则化对于模型更加重要，BN层起到了部分正则化的作用。</li></ul><h3 id="4、批量归一化为什么会起作用？"><a href="#4、批量归一化为什么会起作用？" class="headerlink" title="4、批量归一化为什么会起作用？"></a>4、批量归一化为什么会起作用？</h3><ul><li>只有使用足够大的batch，批量归一化中方法才是有效且稳定的，通常来说BN适合50到100范围的中等批量大小应用；</li><li>批量规范化层在”train“和“val”中的功能不同。 在训练过程中，我们无法得知使用整个数据集来估计平均值和方差，所以只能根据每个小批次的平均值和方差不断训练模型。 而在预测模式下，可以根据整个数据集精确计算批量规范化所需的平均值和方差。</li></ul>]]></content>
    
    
    <summary type="html">&lt;p&gt;&amp;emsp;&amp;emsp;批量归一化(BatchNorm: BN)在搭建深度网络模型中是必不可少的一层，刚开始学习的时候还对其有所了解，后面渐渐的只会使用，已经忘记BN的真正作用和起作用的原理了。现在查找资料进行一个简单的梳理。&lt;/p&gt;</summary>
    
    
    
    
  </entry>
  
  <entry>
    <title>AI的工作流</title>
    <link href="http://example.com/2024/03/27/AI%E7%9A%84%E5%B7%A5%E4%BD%9C%E6%B5%81/"/>
    <id>http://example.com/2024/03/27/AI%E7%9A%84%E5%B7%A5%E4%BD%9C%E6%B5%81/</id>
    <published>2024-03-27T11:15:14.000Z</published>
    <updated>2024-03-29T05:15:16.706Z</updated>
    
    <content type="html"><![CDATA[<h1 id="工作流是什么？"><a href="#工作流是什么？" class="headerlink" title="工作流是什么？"></a><strong>工作流是什么？</strong></h1><p>&emsp;&emsp;每个不同的工作，不同的岗位工作模式都是不一样的，我理解的工作流可能就是对应项目的开发流程，在互联网企业的工作流中，包含产品规划与开发、运营与市场推广、技术支持与维护、商务合作与客户关系等。在传统工业领域，其工作流差异也不是很大。我先介绍下传统工业研发领域的工作流是什么样子的，然后再讨论吴恩达教授最近说到的AI智能体工作流是一种怎么样的情况。</p><span id="more"></span><h2 id="传统行业的工作流"><a href="#传统行业的工作流" class="headerlink" title="传统行业的工作流"></a><strong>传统行业的工作流</strong></h2><p>&emsp;&emsp;大部分企业的研发一般是应用型研发，以实用性为主要目的，做出市场需求的产品。只有少数大型的企业如巴斯夫、陶氏、索尔维、汉高、万华等等跨国型大企业有能力和资金以及时间投入做开创性的研究，也就是从0-1的过程，并能创造出市场的需求。传统化学品研发的工作流也就是项目的开发流程大致是下图这种情况，但隔行如隔山，不知道其它工业如机器，汽车等其它行业是否也是如此。</p><p><img src="/pic/AIwork/1.png" alt="img"></p><h3 id="1、项目需求导入"><a href="#1、项目需求导入" class="headerlink" title="1、项目需求导入"></a><strong>1、项目需求导入</strong></h3><p>主要来源有三个：</p><ul><li>第一个是市场部经过前期调研觉得某类产品有市场，而我们目前还没有相关产品或者产品还满足不了市场要求，需要进行开发。</li><li>第二个是销售部门再开拓市场的时候了解到客户的需求，但我们目前的产品无法满足客户需求，需要开发或改进的。</li><li>第三个是研发人员根据经验或者行业信息等了解到公司所欠缺的一些产品，有必要进行开发，完善产品体系。</li></ul><h3 id="2、立项评审"><a href="#2、立项评审" class="headerlink" title="2、立项评审"></a><strong>2、立项评审</strong></h3><p>项目需求导入后需要经过评审，综合考虑市场行情，开发周期、投入产出比等情况来决定项目是否要做。<br>一般针对上面三个不同的需求情况，评审时考虑的情况也不同。</p><ul><li>针对第一个情况：一般市场部门已经经过了综合的考虑，经过简单共识，没有太多问题的话就可以正式立项。</li><li>针对第二个情况：这个需要考虑的就比较多了，需要仔细判断销售带来的客户需求是真需求还是为伪需求，也就是是否是其它工艺因素引起的，而非对产品性能的需求。还需要考虑，这个需求是不是相关行业的普遍需求？具不具备复制性？投入产出比怎么样？判断这些情况需要技术和研发等人员配合销售到客户现场处，了解更多情况才能确定。</li><li>针对第三个情况：研发人员向公司市场部门提起，然后经过市场部门协助进一步了解待开发产品的详细情况后，重复1的评审。</li></ul><h3 id="3、资料收集"><a href="#3、资料收集" class="headerlink" title="3、资料收集"></a><strong>3、资料收集</strong></h3><p>经过评审立项的项目进入资料收集阶段，这一阶段收集的信息是为后续的产品开发做准备，主要收集的信息有下面几个方面：</p><ul><li>市面上相关竞品的信息：包括产品的TDS、MSDS和公司专利等等，其中相关产品一定要拿到手，用于在试验室内进行评测对比。</li><li>文献信息：包括国内外的文献、综述、专利、报告、技术研讨会资料等等，其中最具参考性的是相关国外的专利信息和文献。在传统领域，信息更新迭代时相比计算机行业慢很多，很多文献可能是几十年前的，但不要因此就认为没有可参考性，因为其原理不变，虽然没有很多先进的手段表征，但前人对试验的态度更加严谨，考虑更加周全，采用的很多试验方法与实际也有很好的对应性，值得仔细看一看。</li><li>相关标准与检测方法：这是比较重要的信息，没有合理的标准和检测方法，就没法判断所作的试验配方有没有改进，选错试验方法的话，加速试验和实际使用情况间没有很好的对应性，会造成实验内各项指标很好，但实际使用是却很差的情况。像是深度学习中模型性的泛化性能不好一样，配方过拟合试验方法，泛化性下降。</li></ul><h3 id="4、配方设计与优化"><a href="#4、配方设计与优化" class="headerlink" title="4、配方设计与优化"></a><strong>4、配方设计与优化</strong></h3><p>在对相关行业有了解后，那么对开发产品配方中需要哪些东西就已经有了一个大致的了解。</p><ul><li>先确定配方的大致框架：例如在防锈油配方体系大致中分为四块：溶剂体系、磺酸盐主防锈剂体系、成膜剂、其它补充防锈剂和功能添加剂。</li><li>确定体系后开始进行原料的筛选，先是进行大范围的搜索，配方与配方间有很大的差异性，目的是先确定一个大致有效的组合，然后使用控制变量法对配方进行微调。这有点像是在对Transformer类模型进行学习率调节的步骤，先线性增长学习率，让模型预热起来，然后再逐步缓慢使用余弦衰减的学习率进行精细调整。</li><li>控制变量法：控制变量进行配方优化是关键，不然可能无法找到规律性，无法快速排除问题。但碰到多个配方原料间的协同和阻抗效应会比较麻烦，这时候就需要经验了。</li><li>当一个配方无论怎么优化都达不到要求时，就要赶紧从头对配方进行大的调整了，因为此时这个配方已经陷入了局部优化阶段，它的上限就在这里，需要赶紧跳出去，以免耽误项目进度。</li><li>最好针对同一个项目同时进行多个不同体系的配方开发，以防止精心选定的配方被后面突然冒出来的某个指标卡住，导致前功尽弃。这里有点像是集成学习，备用方案大部分情况下还是很必要的。</li></ul><h3 id="5、小试生产阶段"><a href="#5、小试生产阶段" class="headerlink" title="5、小试生产阶段"></a><strong>5、小试生产阶段</strong></h3><p>配方的优化与选定是在实验室内完成的，需要进行小试生产来进一步考察产品配方在进行生产的下面几种情况，不同的行业对小试的定义不同，一般小试在百公斤级左右：</p><ul><li>验证配方产品生产的可行性与稳定性。</li><li>初步的确定生产工艺。</li><li>生产的产品提供给部分客户试用，获取反馈来对配方再次进行优化。这一步很关键，客户现场真正的验证比试验室指标更具说服力，销售有产品的应用案例可以参考，在推广产品时也更加高效。</li></ul><h3 id="6、中试生产"><a href="#6、中试生产" class="headerlink" title="6、中试生产"></a><strong>6、中试生产</strong></h3><p>中试生产时使用大型工艺和设备，需要的评估的有下面几点：</p><ul><li>产品在大规模生产下，工艺和设备的适用性。特别是一些需要传热的操作过程例如升温和降温操作，需要充分考虑大型设备的控制精度和效率，防止安全事故的发生。</li><li>产品在生产时产生的残次品和三废的处理。</li><li>进一步确定产品的生产操作工艺。</li></ul><h3 id="7、输出技术文件"><a href="#7、输出技术文件" class="headerlink" title="7、输出技术文件"></a><strong>7、输出技术文件</strong></h3><p>进行到这里项目基本结束，需要把项目过程中涉及的各项文件输出给各个需要的部门。</p><ul><li>针对生产部门需要输出：原材料入厂检验指标、产品生产工艺、过程检验指标等。</li><li>针对检测部门需要输出：产品的型式检验和出场检验，以及产品的分级指标。</li><li>针对销售部门需要输出：产品的说明书TDS&#x2F;MSDS和应用案例等，还有一些例如Rose等法规文件。</li><li>针对公司技术部门需要输出：产品的开发过程，技术路线，便于知识留存。</li></ul><h2 id="AI的工作流是什么？"><a href="#AI的工作流是什么？" class="headerlink" title="AI的工作流是什么？"></a><strong>AI的工作流是什么？</strong></h2><p>&emsp;&emsp;在Chatgpt、GPT-4、Sora的陆续推出后，人工智能继续下一步的发展是什么呢？人工智能著名学者、斯坦福大学教授吴恩达指出：<strong>AI 智能体工作流将在今年推动人工智能取得巨大进步，甚至可能超过下一代基础模型。</strong> 他呼吁所有从事人工智能工作的人都关注 AI 智能体工作流，并撰写一篇博客简单阐述了原因。<br><img src="/pic/AIwork/2.png" alt="img"></p><p>原文的意思是：<br>    当前，我们主要在零样本模式下使用 LLM，提供 prompt，逐个 token 地生成最终输出，没有进行调整。这类似于要求某人从头到尾写一篇文章，直接打字，不允许退格，并期望得到高质量的结果。尽管有困难，LLM 在这项任务上仍然表现得非常好！<br>&emsp;&emsp;非常直观的是，对作家来说，不断的修改是产生好文章的关键。开发配方也是，需要不断的迭代。那扩展到人工智能，就像我们给出提示词让模型一步一步慢慢来会产生更好的输出结果一样，让AI自己在不断修改和迭代的工作流模式下，输出的内容也能够产生更好的结果！<br>如下图所示，吴恩达的团队发现：GPT-3.5（零样本）的正确率为 48.1%，GPT-4（零样本）的表现更好，为 67.0%。然而，相比于迭代智能体工作流，从 GPT-3.5 到 GPT-4 的改进不大。事实上，在智能体循环（agent loop）中，GPT-3.5 的正确率高达 95.1%。<br><img src="/pic/AIwork/3.png" alt="img"><br>&emsp;&emsp;吴恩达分享了一个对构建智能体的设计模式进行分类的框架：<br>简单来说，这个框架包括：</p><ul><li>反思：LLM 检查自己的工作，以提出改进方法。</li><li>工具使用：LLM 拥有网络搜索、代码执行或任何其他功能来帮助其收集信息、采取行动或处理数据。</li><li>规划：LLM 提出并执行一个多步骤计划来实现目标（例如，撰写论文大纲，然后进行在线研究，然后撰写草稿……）。</li><li>多智能体协作：多个 AI 智能体一起工作，分配任务并讨论和辩论想法，以提出比单个智能体更好的解决方案。</li></ul><p>有了工作流之后，AI会越来越往自动发展的方向进行。很好奇，这种会自我完善工具是什么样子的。</p>]]></content>
    
    
    <summary type="html">&lt;h1 id=&quot;工作流是什么？&quot;&gt;&lt;a href=&quot;#工作流是什么？&quot; class=&quot;headerlink&quot; title=&quot;工作流是什么？&quot;&gt;&lt;/a&gt;&lt;strong&gt;工作流是什么？&lt;/strong&gt;&lt;/h1&gt;&lt;p&gt;&amp;emsp;&amp;emsp;每个不同的工作，不同的岗位工作模式都是不一样的，我理解的工作流可能就是对应项目的开发流程，在互联网企业的工作流中，包含产品规划与开发、运营与市场推广、技术支持与维护、商务合作与客户关系等。在传统工业领域，其工作流差异也不是很大。我先介绍下传统工业研发领域的工作流是什么样子的，然后再讨论吴恩达教授最近说到的AI智能体工作流是一种怎么样的情况。&lt;/p&gt;</summary>
    
    
    
    
  </entry>
  
  <entry>
    <title>AI十年浮沉，与改变命运的大模型</title>
    <link href="http://example.com/2023/11/10/AI%E5%8D%81%E5%B9%B4%E6%B5%AE%E6%B2%89%EF%BC%8C%E4%B8%8E%E6%94%B9%E5%8F%98%E5%91%BD%E8%BF%90%E7%9A%84%E5%A4%A7%E6%A8%A1%E5%9E%8B/"/>
    <id>http://example.com/2023/11/10/AI%E5%8D%81%E5%B9%B4%E6%B5%AE%E6%B2%89%EF%BC%8C%E4%B8%8E%E6%94%B9%E5%8F%98%E5%91%BD%E8%BF%90%E7%9A%84%E5%A4%A7%E6%A8%A1%E5%9E%8B/</id>
    <published>2023-11-10T11:15:14.000Z</published>
    <updated>2024-03-29T05:13:50.029Z</updated>
    
    <content type="html"><![CDATA[<p> <a href="https://mp.weixin.qq.com/s/KyUiHgtcQRU2Rvjbyyn-Tw">原文：远川研究所</a></p><h2 id="AI十年浮沉，与改变命运的大模型"><a href="#AI十年浮沉，与改变命运的大模型" class="headerlink" title="AI十年浮沉，与改变命运的大模型"></a>AI十年浮沉，与改变命运的大模型</h2><p>　　&emsp;&emsp;2019年，前谷歌CEO埃里克·施密特向白宫递交了一份颇为“诡谲”的报告。这份长达750页的文件，核心观点其实只有一句话：若美国再不加大投资，中国将彻底主导AI领域[1]。<br>　　施密特此举其实有“骗经费”之嫌：一年前，谷歌曾迫于舆论压力退出了一个政府AI项目，他一直对此耿耿于怀。但报告本身却并非胡诌。同年，美国数据创新中心也发布了一份报告，声称中国AI实力全球第二，且在数据等层面比美国更具优势[2]。<br>　　<span id="more"></span><br>　　&emsp;&emsp;在全民追赶GPT的今天，这则旧闻读起来颇有几分“魔幻感”。报告甚至上升到了“国家安全”的高度然而，施密特口中的AI，与如今人们谈论的GPT并非一个东西。以GPT为代表的AI，指的其实是大模型。它拥有生成图像、音频、视频等内容的能力，像是个文艺青年。但彼时让美国人深感威胁的，多指识别型AI（小模型）。它擅长各类数据分析工作，如同一个木得感情的运算机器。大模型走红前，识别型AI曾被寄予了太多改变世界的厚望，在中国催生了一段群雄并起、热钱涌动的黄金岁月。巅峰时期，中国AI初创企业的融资金额甚至超过了美国——李开复将其形容为“有三个AI专家就能估值7亿、靠AI概念就能忽悠投资人”，也不怪美国人感到焦虑。只是好景不长：后来VC陆陆续续退出，曾经风头无两的独角兽也褪去了身上的光环，残暴的欢愉最终以残暴终结。对此，百度集团执行副总裁、百度智能云事业群总裁沈抖曾打过这么一个比方：大模型出现前的AI像是氧气——本身很有价值，但自己不会燃烧，必须找到可燃物才能把价值给发挥出来。这里的“可燃物”，指的是落地场景。AI起起落落这十年，成也场景，败也场景。旧范式的困境2016年，谷歌AlphaGO不仅彻底颠覆了围棋，也改变了当时的AI创业。一时间，VC、科学家、大学教授，乃到各路乡镇企业家，无一不在谈论着AI商业化的可能性。短短一年时间，国内诞生了528家AI企业，催生371起AI投融资，同比涨幅达到了38.9%；同一时间，中国AI企业申请了9000多项AI专利，几乎是美国新增专利数的两倍有余[3]。不过，整个行业闭眼狂奔的同时，鲜有人会注意到初冬的号角已经悄悄吹响。2019年，繁荣戛然而止。首先是融资遇冷：这一年的AI融资金额仅有186亿元，相比2018年直接腰斩了一个0。受此影响，AI初创企业的数量也大幅缩水，仅有鼎盛时期的1&#x2F;20。至于那些从竞争中幸存、成功“上岸”的AI企业，其财务状况仍旧惨不忍睹。据不完全统计，近九成AI企业都处于严重亏损的状态[4]。</p><h2 id="大起大落背后，是AI长期以来的产业化困境。"><a href="#大起大落背后，是AI长期以来的产业化困境。" class="headerlink" title="大起大落背后，是AI长期以来的产业化困境。"></a>大起大落背后，是AI长期以来的产业化困境。</h2><p>　　&emsp;&emsp;2021年之前，业界三大主流AI技术分别是计算机视觉、语音识别、自然语言识别，本质都属于识别型AI。单从技术层面来看，上述技术都具备着充分的下游应用空间，想象力充足。例如在“AI+安防”领域，2020年时已有453亿元的市场，且增速可观，预计到2025年时市场规模将再翻一番[6]。可下游需求不断扩大，并无法拯救亏钱的AI企业。识别型AI的技术特点，决定了它是一门技术、投入与产出不成正比的生意。识别型AI采用的是小模型——这是一种专为特定任务而生的技术。在实际训练小模型时，研究员只需给AI灌入标注过的特定数据，便能让AI获得对应的能力。如果想要一个能抓“闯红灯”的AI，那么无需教它语文数学，只要让它从小学习各种闯红灯的视频即可。小模型的优点在于简单、高效，专用向的AI能够很好地完成对应任务。但其缺点同样明显：由于没学过其他知识，一个AI只能解决一个问题。譬如一个抓闯红灯的AI，显然不会具备识别超速、违规变道的能力。<br>　　由于小模型不具备通用性，导致识别型AI只能成为一门类手工作坊的定制生意。<br>        且具体到实际应用中，一个需求有时还需要定制不止一个AI。例如在工业领域，在制造冶金钢卷时有缺陷检测这一步骤。如果将这项工作交由AI，定制起来其实相当麻烦。因为钢卷分为冷轧、热轧，所以AI企业需要同时用“冷轧-合格”“冷轧-缺陷”“热轧-合格”“热轧-缺陷”四组数据训练四遍[7]。其繁琐程度，与“五彩斑斓的黑”有的一拼。<br>        定制需求繁琐的同时，对人力要求还不低——这活儿可不是月薪三千的大学生能干的。出于业务需要，头部AI企业都聘请了大批科学家、博士生与教授作为研究员，而AI研究员又是出了名的“高薪岗位”。2015年，谷歌为了不让知名研究员伊利亚（Ilya Sutskever）跳槽，曾开出过200万美元&#x2F;年的高薪——后来桑达尔·皮查伊继任谷歌CEO时，其基本工资是同一个数字。财新曾对国内AI企业招股书做过一笔测算，发现：它们每挣1块钱，就要花掉0.75元的人力成本。再算上定制AI的算力、数据开销、以及其他成本，几乎做一单亏一单。人们这才惊奇地发现，AI这门生意似乎远不如想象中那么性感。陷入死局后，AI企业们只能寄希望于一场“推翻重来”式变革。幸运的是，没过几年，暴风雨真的来了。</p><h2 id="通用性的价值"><a href="#通用性的价值" class="headerlink" title="通用性的价值"></a>通用性的价值</h2><p>　　&emsp;&emsp;红杉资本率先嗅到了风雨欲来的气息。2022年9月，红杉发表了文章《Generative AI: ACreative New World》，预言一场全新的科技竞赛即将来临。投资人同行很快闻风而动，一度冷清的AI圈再度人声鼎沸。这篇文章发布仅半年，有头有脸的科技公司们几乎全部一头扎进了AI浪潮之中，要用人智能把每个行业都重新做一遍。例如当下火热的直播行业，大模型应用的空间就相当广阔。对于那些养不起专业直播团队的商家，如今只需输入商品信息，百度的文心一言能够自动生成话术、配音以及数字人主播，直接包揽了整个流程。<br>　　企业无需再花钱雇主播、想话术、搞培训，能轻松实现7*　24小时直播，对中小企业而言无疑是个重大利好。在沈抖看来，大模型创业带来的众多机会，将带动数字化经济更进一步，有机会在全球范围内掀起一股产业再造的浪潮。他认为，随着大模型深入数字经济，更多产业会出现新的改变。智能化不仅让整个生产流程大幅提效，也改变了很多原有的生产关系，包括人和人、人和设备、人和系统的关系。以前没有智能时，很多工作实际上是靠人来操作；有了智能以后，机器、设备、系统都可以按照人的思想去学习。未来的工作模式，很有可能是一个聪明的人去指导一堆机器人。这些机器人执行人类的决定，重塑整个生产线。沈抖说道，“生成式AI已经形成全球性的‘AI再造业务’趋势，企业迎来‘智能化跃迁’的历史机遇。”<br>　　&emsp;&emsp;AI产业能够二度迎来春天，背后其实是大模型技术迈向成熟。这场变革的起点发生在2017年：彼时，谷歌几位研究员公开了深度学习模型Transformer。以此为基础，OpenAI等机构开始尝试研究大模型，一种不同于识别型AI的全新技术。其研究成果，正是如今备受关注的GPT，即Generative Pre-trained Transformer（生成式预训练Transformer）。众所周知，一个AI由模型、数据、算力三要素构成。相比于传统的识别型AI（小模型），以GPT为代表大模型在数据、模型等方面均有不同程度的革新，赋予了大模型更强的通用性。这恰好改善了小模型时代的产业化痛点，AI实现即插即用。AI企业终于能摆脱手工作坊般的生产模式，有机会变成一门好生意。大模型的这一价值，几乎吸引了全球所有的目光。连早已退休、专心搞慈善的比尔盖茨，都为此兴奋不已。他在一篇文章中写道，自己有幸亲历了人类可能最重要的两场革命的开端：第一次发生在Windows萌芽、PC市场刚刚兴起的80年代，而第二次正是去年——大模型刚刚开始涌现的时候。而沈抖认为，作为“通才”的大模型还催生一种全新的产业化路径：MaaS（Model as aService），模型即服务。“根据我们提供的文心一言的大模型服务，企业能够以此为基础，结合他们所在行业去微调出一个行业大模型，再用这个行业大模型去服务整个行业。”未来，大模型可能会成为一个类似于安卓的“超级底座”， 每个行业都迎来AI再造的机会，并给应用端带来大量的机会。<br>　　&emsp;&emsp;而在美国，已经诞生了依托于大模型开展业务的独角兽。在德克萨斯州，一家初创企业凭借微调后的AI应用，在短短18个月时间做到了15亿美元估值，年收入已有3000万美元，比直接做大模型的企业还赚钱[10]。不过，产业化潜力仅仅是大模型价值的一个方面。不同于过去两年流行的元宇宙、Web3等概念，大模型带来了实打实的生产力提升。例如长安汽车在参与百度文心一言的邀测时，就体验了一回大模型的生成PPT功能。过去，制作一份PPT通常要花费半天甚至一天时间；如今，只需三分钟即可做出一份内容齐全、格式精美的PPT。未来，机械的重复性工作将完全可以交给AI，员工可以集中于创造更大价值的工作，企业竞争力增加同时增加社会财富。</p><h2 id="“百模大战”来了，一如当年的“百团大战”。"><a href="#“百模大战”来了，一如当年的“百团大战”。" class="headerlink" title="“百模大战”来了，一如当年的“百团大战”。"></a>“百模大战”来了，一如当年的“百团大战”。</h2><p>　　&emsp;&emsp;国内大公司纷纷扎堆推出大模型，百度文心一言最先，360、腾讯、阿里、商汤、科大讯飞、字节等纷纷紧追。大模型产品大多同时兼具对话问答、文章创作、代码写作等多项技能。市场上供给多了，但这些产品的“智力”水平却参差不齐，如何挑选成为了一个难题。<br>　　目前主流观点认为，可以有以下两个标准：<br>　　&emsp;&emsp;第一，从芯片到框架、模型再到应用的全链路环节都具备极为优秀的性能。芯片决定了算力，这是AI训练的基础。OpenAI曾做过一笔测算：2012年开始，全球AI训练所用的计算量平均每3.43个月便会翻一倍，远超摩尔定律。在肉眼可见的未来，“算力不足”都将会是制约AI发展的最大因素。因此，芯片要做到性能过硬。只有制造出算力极强的高端芯片，才能满足智算时代的计算需求，让云好用，这是基础条件。而在框架和模型层面，软件适配程度要更高，算法积累足够，大模型能力要更强。深度学习框架需要做到全栈自主可控，推动大模型不断迭代升级。在此基础上，推出多样云服务使得大模型适应各产业的数字、智能化需求，做好模型即服务（MaaS）。<br>　　&emsp;&emsp;第二，能为大家提供全链路的服务保障。服务商不仅要具备多元的能力，还需要将它们有机地结合起来：如此一来不仅能带来更高的可靠性、帮助企业降本增效，还降低AI使用门槛，既易用，又好用。好的智算基础设施，就是要高效解决算力、算法、数据处理等多维度的问题，而且这些维度之间不是独立存在的，而是互相依赖，相互优化、紧密耦合，从而提升整体基础设施效能。比如芯片解决算力问题，框架解决算法开发问题，大模型解决模型泛化问题，他们三者就犹如三人四足比赛中的团队合伙人，只有彼此心有灵犀、协调互补，才能打出完美的团队配合，赢得比赛胜利。百度的解决方案，是将文心一言大模型与百度智能云服务结合起来，提供一种更便捷的一站式服务，即“云智一体”。该方案同时提供了算力与大模型服务，企业只需提供自身行业的数据做微调，即可快速生产出符合市场需求的AI产品。不过，百度智能云能做的还不止微调一项，它还额外集成了推理与托管服务。<br>　　&emsp;&emsp;推理是大模型所具备的一项能力，指利用训练好的AI去进行结论推导的过程。如果说微调是将已有的房子重新装修一遍，那么推理则相当于给你空地和材料，搭建一个全新的房子。虽然推理的成本高于微调，但能让大模型新增更多原本不具备的能力，满足企业更深层次的开发需求。至于托管，则能帮助较为早期的大模型创业者解决搭建团队的难题。高薪的AI研究员一直是市场上最抢手的资源之一，初创企业不仅资金有限，且很难在人才竞争中获得太多优势。相比之下，托管服务则会提供一支高度专业的团队，帮助第三方企业管理他们的AI解决方案。由此可见，微调、推理、托管三大服务的功能各不相同，能够满足不同阶段企业的需求。<br>　　&emsp;&emsp;如今，上述“云智一体”服务已成为一种行业共识。百度智能云作为这一概念的提出者，具备一些独有的优势。2022年，百度副总裁沈抖升级了原有的架构，正式对外官宣了“云智一体3.0”。该架构的优势在于，形成了“芯片-框架-大模型-行业应用”的智能化闭环路径，百度是全球唯一在每一层都有自研产品的公司。比如芯片层有昆仑芯，框架有中国市场份额第一的飞桨。沈抖认为，四层架构的高效协同，能实现更高的运转效率，“我们能在同等算力投入下，把某个应用的性能提高100%，相当于让算力需求降低了50%。”这是百度智能云非常独特的优势，也使得如今，“云智一体”不仅仅只是一项服务，更有可能成为数字时代的一种新基建。经济学博士任泽平曾在2023年跨年演讲中预测，智能云将成为数字经济的重要技术支撑，并在未来20年成为支撑中国经济繁荣发展的新型基础设施。对当今社会而言，这些变化既是将来时，也是进行时——AI正在以肉眼可见的速度重塑这个社会。对此，舆论上充斥着许多不安的声音。不过，当时代的浪潮扑面而来，企业、个体最好的选择，只有拥抱它。</p><h2 id="尾声"><a href="#尾声" class="headerlink" title="尾声"></a>尾声</h2><p>  &emsp;&emsp;不论是红杉资本还是黄仁勋，都喜欢将当下称为“AI的IPhone时刻”。初代IPhone发布于2007年。彼时，摩托罗拉、诺基亚才是手机市场的霸主，张小龙还在忙着做QQ邮箱，百度搜索的网页访问量每天都在增长，没人知道世界上第一款智能手机到底会带来些什么。然而，当乔布斯讲出那句“今天，苹果将重新发明手机”时，世界依旧为之沸腾，变革的伏笔已经埋下。相比于市面上的其他产品，初代IPhone的革新之处其实只有两个方面：一是取消了键盘，改用触屏设计；二是借鉴了电脑的设计，尝试在手机中加入浏览器、股票、天气等应用。10年之后，初代IPhone的这两大设计已然成为了智能手机的设计标准，以此构建的移动互联网改写了数十亿人的生活。沈抖曾在采访时提到，初代IPhone的出现，将手机分成了两类：智能手机与非智能手机，而非智能手机最终被淘汰了。大模型出来后，同样会把企业分成两类：一类是智能企业，一类是非智能企业——类似的故事有可能会再次上演。<br>  关于大模型的未来，如今看来仍有些模糊，但唯有一点是确信的——新时代的序幕正缓缓拉开，一场重新定义时代的比赛已经开始。<br>参考资料<br>[1] China will dominate AI unless U.S. invests more, commission warns，Axios<br>[2] 2019年AI实力对决：美国领跑，中国追赶，欧盟弱势，智东西<br>[3] 乌镇指数：全球人工智能发展报告（2017）<br>[4] 2021年人工智能行业发展白皮书<br>[5] 中国人工智能软件及应用市场研究报告-2020，IDC<br>[6] 2021年中国AI+安防行业发展研究报告，艾瑞咨询<br>[7] 正在消失的机器视觉公司，脑极体<br>[8] Generative AI: A Creative New World，sequoia<br>[9] 揭秘ChatGPT身后的AIGC技术和它的中国同行们，海通国际<br>[10] The Best Little Unicorn in Texas: Jasper Was Winning the AI Race—Then ChatGPT Blew Up the Whole<br>Game，The Information<br>[11] AI Developers Stymied by Server Shortage at AWS, Microsoft, Google，The Information</p>]]></content>
    
    
    <summary type="html">&lt;p&gt; &lt;a href=&quot;https://mp.weixin.qq.com/s/KyUiHgtcQRU2Rvjbyyn-Tw&quot;&gt;原文：远川研究所&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&quot;AI十年浮沉，与改变命运的大模型&quot;&gt;&lt;a href=&quot;#AI十年浮沉，与改变命运的大模型&quot; class=&quot;headerlink&quot; title=&quot;AI十年浮沉，与改变命运的大模型&quot;&gt;&lt;/a&gt;AI十年浮沉，与改变命运的大模型&lt;/h2&gt;&lt;p&gt;　　&amp;emsp;&amp;emsp;2019年，前谷歌CEO埃里克·施密特向白宫递交了一份颇为“诡谲”的报告。这份长达750页的文件，核心观点其实只有一句话：若美国再不加大投资，中国将彻底主导AI领域[1]。&lt;br&gt;　　施密特此举其实有“骗经费”之嫌：一年前，谷歌曾迫于舆论压力退出了一个政府AI项目，他一直对此耿耿于怀。但报告本身却并非胡诌。同年，美国数据创新中心也发布了一份报告，声称中国AI实力全球第二，且在数据等层面比美国更具优势[2]。&lt;br&gt;</summary>
    
    
    
    
  </entry>
  
  <entry>
    <title>记一次失败的立项评审会</title>
    <link href="http://example.com/2023/04/12/%E8%AE%B0%E4%B8%80%E6%AC%A1%E5%A4%B1%E8%B4%A5%E7%9A%84%E7%AB%8B%E9%A1%B9%E8%AF%84%E5%AE%A1%E4%BC%9A/"/>
    <id>http://example.com/2023/04/12/%E8%AE%B0%E4%B8%80%E6%AC%A1%E5%A4%B1%E8%B4%A5%E7%9A%84%E7%AB%8B%E9%A1%B9%E8%AF%84%E5%AE%A1%E4%BC%9A/</id>
    <published>2023-04-12T12:33:12.000Z</published>
    <updated>2024-04-01T11:08:59.148Z</updated>
    
    <content type="html"><![CDATA[<p>&emsp;&emsp;每到年底，公司内部都会组织一个项目评审会议用来决定来年将要进行立项研发的产品项目。今年共有5个项目参与评审，我有其中一个。去年我也有一个项目参加评审，但由于我提前过年回家了，由领导代为评了，所以今年也是我第一次参加公司的评审会议。</p><span id="more"></span><p>&emsp;&emsp;总的感觉对自己的这次评审很不满意，没说该说的，反而说了不该说的，很不应当。分析一下欠缺考虑的地方，有以下几点：<br>1、首先是语速过快，过快的语速把自己的紧张感显露无疑，让参加会议的人员液听不清项目内容。<br>2、没有把握住项目的要点，从项目的来源到项目开发背景，详细的市场调查都没能很清楚的搞明白。<br>3、没有项目投入与产出，没有量化的指标来衡量投入产出比。<br>4、说出了不该说的内容，虽然有外部平替资源在替代测试，但不应该在此时提出来，提出来会让人觉得，那你再进行此项目的意义何在。这也是这次评审会我自觉最失败的地方。<br>&emsp;&emsp;但本项目的来源就不是很明确，没有做到详尽的市场调查，甚至竞品很久都拿不到。<br>&emsp;&emsp;项目虽然最后通过评审，但也有很多经验要吸取。对项目的市场调查要很详尽，这是项目决定做不做的最重要因素，要避免闭门造车，这一点是每个公司的禁忌；列出投入产出比，项目计划节点，项目目标；讲清楚做的意图难点。暂时只想到这么多，希望下次得到改进。</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;&amp;emsp;&amp;emsp;每到年底，公司内部都会组织一个项目评审会议用来决定来年将要进行立项研发的产品项目。今年共有5个项目参与评审，我有其中一个。去年我也有一个项目参加评审，但由于我提前过年回家了，由领导代为评了，所以今年也是我第一次参加公司的评审会议。&lt;/p&gt;</summary>
    
    
    
    
  </entry>
  
  <entry>
    <title>ppt写作注意事项</title>
    <link href="http://example.com/2023/03/12/PPT%E5%86%99%E4%BD%9C%E6%B3%A8%E6%84%8F%E4%BA%8B%E9%A1%B9/"/>
    <id>http://example.com/2023/03/12/PPT%E5%86%99%E4%BD%9C%E6%B3%A8%E6%84%8F%E4%BA%8B%E9%A1%B9/</id>
    <published>2023-03-12T12:33:38.000Z</published>
    <updated>2024-03-29T05:09:07.692Z</updated>
    
    <content type="html"><![CDATA[<p>PPT的写作技能，毋庸置疑是工作上很有用的技能。工作再努力，也要让人看到才可以。正所谓高调做事，低调做人。</p><p>闲话少说，我觉得以下几点PPT写作技巧以后可能会用到：</p><ol><li>站在你汇报对象的角度来考虑怎么写这个主题，找出听众真正在意的地方，突出描述。</li><li>格式一定要整洁，例如 ：字体大小和格式要统一，单位之间要有空格 ， 表格大小和位置要统一，字体间距设置好。</li><li>ppt中图片可以多，但字不能太多，要尽可能说出来，而不是写出来，</li><li>每页最好有一个小总结。</li><li>配色尽量和谐，不要有太突兀的配色。</li></ol>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;PPT的写作技能，毋庸置疑是工作上很有用的技能。工作再努力，也要让人看到才可以。正所谓高调做事，低调做人。&lt;/p&gt;
&lt;p&gt;闲话少说，我觉得以下几点PPT写作技巧以后可能会用到：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;站在你汇报对象的角度来考虑怎么写这个主题，找出听众真正在意的地方，突出</summary>
      
    
    
    
    
  </entry>
  
  <entry>
    <title>红楼梦-选句</title>
    <link href="http://example.com/2023/03/12/%E7%BA%A2%E6%A5%BC%E6%A2%A6-%E9%80%89%E5%8F%A5/"/>
    <id>http://example.com/2023/03/12/%E7%BA%A2%E6%A5%BC%E6%A2%A6-%E9%80%89%E5%8F%A5/</id>
    <published>2023-03-12T12:29:38.000Z</published>
    <updated>2023-03-12T12:51:16.883Z</updated>
    
    <content type="html"><![CDATA[<ul><li>你证我证，心证意证  </li><li>是无有证，斯可云证  </li><li>无可云证，是立足境</li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;ul&gt;
&lt;li&gt;你证我证，心证意证  &lt;/li&gt;
&lt;li&gt;是无有证，斯可云证  &lt;/li&gt;
&lt;li&gt;无可云证，是立足境&lt;/li&gt;
&lt;/ul&gt;
</summary>
      
    
    
    
    
  </entry>
  
  <entry>
    <title>2022钓鱼笔记</title>
    <link href="http://example.com/2023/03/12/2022%E9%92%93%E9%B1%BC%E7%AC%94%E8%AE%B0/"/>
    <id>http://example.com/2023/03/12/2022%E9%92%93%E9%B1%BC%E7%AC%94%E8%AE%B0/</id>
    <published>2023-03-12T12:21:05.000Z</published>
    <updated>2024-03-29T05:30:22.698Z</updated>
    
    <content type="html"><![CDATA[<p>&emsp;&emsp;今年初在家隔离期间，实在无聊，天天在窗边发呆。居住的地方外面就是一条河，经常能够看到一些钓友偷摸跑出去在河边路亚，在小区团购群里晒渔货。当时别说是鱼了，就连蔬菜都吃不上，能来一条大鲈鱼加餐，很是让人羡慕。</p><span id="more"></span><p>&emsp;&emsp;解封后，立即买了一套纺车轮路亚竿准备也去玩一玩。拿到杆子后缠好主线，绑上假饵，来到河边，学着其它钓友的样子准备抛投，但抛的时候，假饵不是一头扎在水里就是飞的老高却落在面前，还时不时出现炸线的情况，完全没有钓友们那种潇洒一挥，抛出老远的感觉，很是郁闷。差点就抛竿-炸线-剪线-回家-新手一条龙了，但好在碰到一位很有耐心的钓友悉心指导了一番，才基本熟悉了抛投技巧，抛投主要靠手腕和手臂发力，身子不要摆动，照着练习一会后虽然抛不是很远至少是能顺利的把假饵抛出去了。钓友看了下我的装备，说杆子和线组合不适合这河里的鱼情，2.5的pe线太粗杆子（mh）也偏硬了些。根据钓友意见再次选购了一套装备，这次买的是水滴轮。水滴轮虽相比纺车轮更易炸线，多练习几次后逐渐找到感觉上手也没有特别难。相比纺车轮，水滴轮抛投时感觉更加灵活些。<br>&emsp;&emsp;本来我就是喜欢钓鱼的，之前一直手杆垂钓，但门口的河是运河道，往来过船不断，手杆垂钓很是不方便。对比下来路亚在这里更合适，无需打窝聚鱼，相应的准备时间也很少。<br>&emsp;&emsp;入手路亚杆后，下班休息时就往河边去抛几杆，刚开始十多天一直没有钓到过鱼。在河边碰到的钓友建议在早窗口期出来试一试，这时候鱼比较活跃，鱼口比较好。次日早上5点就起床，准备在河边抛几杆到7点再去上班。来到河边钓友常钓的一个标点，是个结构区下面修路时扔的乱石块很多。使用铅头钩，抛下去后沿着岸边慢慢跳底（钓友教的），跳到一半就一个重重的鱼口，直接中鱼了！起来这么早，我本来还是有点朦朦胧胧的，这下瞬间来精神了！鱼不是很大，所以一路收线直接就飞上来了。是条小海鲈鱼，很漂亮！至此第一次体会到了路亚上鱼的快乐，和手杆真是别样感觉。  <br><img src="/pic/fishing/1.png" alt="img"><br>&emsp;&emsp;体验过上鱼的感觉后，更加喜欢路亚了。几乎每天下班后都会拿着杆子在河边抛个几杆才过瘾，周末更不用说。但好像过新手保护期一般，除了几条小翘嘴基本没上什么正经鱼过。虽然没钓到什么鱼，但钓友倒是碰到很多，一起彼此交流经验分享标点，有时相约一起出钓，纵使打龟也比一个人开心些。<br>&emsp;&emsp;一次在周末时间和钓友约了一早出发去一个闸口附近路亚，听说那最近出鱼，到了闸口，周围已经有很多钓友了，手杆、路亚都有。来回一块地方，河边的地也都被踩平了，此处估计反复已经被搜了不知多少遍，不管了反正也是试下运气，使用铅头钩开始跳底，快一个小时没什么反应。钓友已经换地方了，我也准备抛几杆走呢，突然一个重口，中鱼了！直接炸水，看样子不小，但我收起来的时候却感觉力很小，以为脱勾了呢，收到河边才发现鱼还在，可能刚好向着我这岸边游来所以没感觉到发力。拎上来准备取钩拍照，结果手被鱼扎了好几下。。。控鱼器还是需要的。<br><img src="/pic/fishing/2.png" alt="img"><img src="/pic/fishing/3.png" alt="img"><br>&emsp;&emsp;整个夏天，休息时间不是在路亚就是在路亚的路上，晚上玩手机的时候还都是刷些钓鱼视频，中毒不浅。期间只有这个翘嘴还算值得一晒。<br><img src="/pic/fishing/4.png" alt="img"><br>&emsp;&emsp;一直到十一回家时，浑身晒的黢黑，我妈问我是不是在外面挖煤来着 。<br>骑着电驴，拿着杆子，走到一处钓一处。虽然空军居多，但沿途遇到的风景都还很不错，抛着杆，收着线，即使没鱼这感觉也很惬意。<br><img src="/pic/fishing/5.png" alt="img"></p>]]></content>
    
    
    <summary type="html">&lt;p&gt;&amp;emsp;&amp;emsp;今年初在家隔离期间，实在无聊，天天在窗边发呆。居住的地方外面就是一条河，经常能够看到一些钓友偷摸跑出去在河边路亚，在小区团购群里晒渔货。当时别说是鱼了，就连蔬菜都吃不上，能来一条大鲈鱼加餐，很是让人羡慕。&lt;/p&gt;</summary>
    
    
    
    
  </entry>
  
  <entry>
    <title>深度学习是什么？能干什么？怎么工作的？--应用费曼学习方法</title>
    <link href="http://example.com/2022/09/02/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%98%AF%E4%BB%80%E4%B9%88%EF%BC%9F-%E5%BA%94%E7%94%A8%E8%B4%B9%E6%9B%BC%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95/"/>
    <id>http://example.com/2022/09/02/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%98%AF%E4%BB%80%E4%B9%88%EF%BC%9F-%E5%BA%94%E7%94%A8%E8%B4%B9%E6%9B%BC%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95/</id>
    <published>2022-09-02T02:32:59.000Z</published>
    <updated>2024-03-29T05:31:14.020Z</updated>
    
    <content type="html"><![CDATA[<p> <strong>1、深度学习是什么？</strong></p><ul><li><p>深度学习使用计算机计算形成一套公式，公式内含有大量参数，对参数训练然后解决后面相似问题。</p></li><li><p>深度学习是模型，用来将经验累积起来，累计的具体过程很难解释，就像我们知道有尾巴，有四只脚，身体柔软，会喵喵叫吃老鼠就是猫这个动物一样，深度学习如何知道一个动物是不是猫的过程就不是很清楚，很可能记住了其它特征参数，例如腿的夹角、头与身体的比例等等可能的特征情况，毕竟它能记住非常多的特征。</p><span id="more"></span></li><li><p>深度学习是一个专家系统，混合经验知识。</p></li></ul><p> <strong>2、深度学习能干什么？</strong></p><p>   能够对图片和视频进行是被分类，但是不知道的它就分辨不了，比如识别猫狗的模型中出现了一个凹凸曼就不认识了或者出现一个人能也就不认识了。  **</p><p>识别分类：  </p><ul><li>用再火灾检测里面，可以配合其它传感器指标，比如粉尘浓度，可燃物数量，空气质量等传感器识别可能的火灾场景。</li><li>在商场店铺里，识别入店人员年龄段，然后更新产品。</li><li>在医用领域解读各种检测的图片。</li><li>在分类分拣中使用，例如果蔬筛分、其它物品分拣等。</li><li>识别文字，商品同类搜索等。</li><li>面部识别。</li></ul><p> 预测：</p><ul><li>在药物开发中预测潜在药物分子结构。</li><li>预测更准确的天气。</li><li>店铺购物推荐。视频推荐。</li></ul><p> 回答类：  </p><ul><li>像是ChatGPT一样回答问题。</li><li>生成故事，文本。</li><li>学习作者风格，续写故事。</li><li>游戏文案故事生成。</li></ul><p> <strong>3、深度学习怎么工作的？</strong></p><p> 先准备标注好的目标数据集，然后选择模型进行训练，调参等达到最高准确度，然后预测使用。有一大堆参数，训练迭代这些参数然后输入数据计算结果。</p><p>附一份<a href="https://aws.amazon.com/cn/what-is/deep-learning/">AWS的一份关于深度学习的介绍</a></p>]]></content>
    
    
    <summary type="html">&lt;p&gt; &lt;strong&gt;1、深度学习是什么？&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;深度学习使用计算机计算形成一套公式，公式内含有大量参数，对参数训练然后解决后面相似问题。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;深度学习是模型，用来将经验累积起来，累计的具体过程很难解释，就像我们知道有尾巴，有四只脚，身体柔软，会喵喵叫吃老鼠就是猫这个动物一样，深度学习如何知道一个动物是不是猫的过程就不是很清楚，很可能记住了其它特征参数，例如腿的夹角、头与身体的比例等等可能的特征情况，毕竟它能记住非常多的特征。&lt;/p&gt;</summary>
    
    
    
    
  </entry>
  
  <entry>
    <title>部署博客中遇到的问题</title>
    <link href="http://example.com/2022/08/12/%E9%83%A8%E7%BD%B2%E5%8D%9A%E5%AE%A2%E4%B8%AD%E9%81%87%E5%88%B0%E7%9A%84%E9%97%AE%E9%A2%98/"/>
    <id>http://example.com/2022/08/12/%E9%83%A8%E7%BD%B2%E5%8D%9A%E5%AE%A2%E4%B8%AD%E9%81%87%E5%88%B0%E7%9A%84%E9%97%AE%E9%A2%98/</id>
    <published>2022-08-12T12:08:48.000Z</published>
    <updated>2024-03-29T05:31:38.035Z</updated>
    
    <content type="html"><![CDATA[<p>在部署博客时参考了教程<a href="https://blog.csdn.net/yaorongke/article/details/119089190">GitHub Pages + Hexo搭建个人博客网站，史上最全教程</a>，教程很全面，但我在试剂操作时仍然遇到了不少问题（我菜。。），记录一下，避免下次再碰到同样的问题。</p><span id="more"></span><h3 id="1、安装nodejs出现的问题"><a href="#1、安装nodejs出现的问题" class="headerlink" title="1、安装nodejs出现的问题"></a>1、安装nodejs出现的问题</h3><p>在按照<a href="https://hexo.io/zh-cn/docs/">官方文档</a>安装nodejs后，使用npm -v 出现下图提示，搜索了一下，解决方案液挺简单，修改nodejs两个文件就行。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">npm WARN config global `--global`, `--local` are deprecated. Use `--location=global` instead.</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>修改措施如下：<br>将nodejs根目录里面的这两个文件剪切到桌面（在原文件夹里无法编辑），将文件中的<code>prefix -g</code> 改为 <code>prefix --location=global</code>保存后剪切回原来文件夹就行了。<br>![[1.png]]<br>修改后不在报错。<br> <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">(base) C:\boke&gt;npm -v</span><br><span class="line">8.16.0</span><br></pre></td></tr></table></figure></p><h3 id="2、安装next主题出现的问题"><a href="#2、安装next主题出现的问题" class="headerlink" title="2、安装next主题出现的问题"></a>2、安装next主题出现的问题</h3><p>按照博客中操作安装next主题后并更换后，主题无法使用，博客出现乱码。<br>原来是<a href="https://github.com/iissnan/hexo-theme-next">老版本的next</a>不在提供支持，所以会报错。<br>![[Pasted image 20220805161109.png]]<br>更换到最新你的版本</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cd</span> hexo</span><br><span class="line">git <span class="built_in">clone</span> https://github.com/theme-next/hexo-theme-next themes/next</span><br></pre></td></tr></table></figure><p>并在此基础上后续安装hexo-deployer-git库时一直安装失败出现</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;在部署博客时参考了教程&lt;a href=&quot;https://blog.csdn.net/yaorongke/article/details/119089190&quot;&gt;GitHub Pages + Hexo搭建个人博客网站，史上最全教程&lt;/a&gt;，教程很全面，但我在试剂操作时仍然遇到了不少问题（我菜。。），记录一下，避免下次再碰到同样的问题。&lt;/p&gt;</summary>
    
    
    
    
  </entry>
  
</feed>
