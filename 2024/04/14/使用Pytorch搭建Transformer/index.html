<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width">
<meta name="theme-color" content="#222"><meta name="generator" content="Hexo 6.3.0">

  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">



<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/all.min.css" integrity="sha256-CTSx/A06dm1B063156EVh15m6Y67pAjZZaQc89LLSrU=" crossorigin="anonymous">

<script class="next-config" data-name="main" type="application/json">{"hostname":"example.com","root":"/","images":"/images","scheme":"Pisces","darkmode":false,"version":"8.18.2","exturl":false,"sidebar":{"position":"left","display":"always","padding":18,"offset":12},"copycode":{"enable":false,"style":null},"fold":{"enable":false,"height":500},"bookmark":{"enable":false,"color":"#222","save":"auto"},"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"stickytabs":false,"motion":{"enable":false,"async":false,"transition":{"menu_item":"fadeInDown","post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"prism":false,"i18n":{"placeholder":"搜索...","empty":"没有找到任何搜索结果：${query}","hits_time":"找到 ${hits} 个搜索结果（用时 ${time} 毫秒）","hits":"找到 ${hits} 个搜索结果"}}</script><script src="/js/config.js"></script>

    <meta name="description" content="经典的transformer架构如下图所示，分为编码器和解码器。构建模型的过程中，先构建通用的模块，如位置编码、多头自注意力、前馈神经网络层等，然后组建编码器和解码器，最后统一到一个模型中。">
<meta property="og:type" content="article">
<meta property="og:title" content="使用Pytorch搭建Transformer">
<meta property="og:url" content="http://example.com/2024/04/14/%E4%BD%BF%E7%94%A8Pytorch%E6%90%AD%E5%BB%BATransformer/index.html">
<meta property="og:site_name" content="凹凸岛">
<meta property="og:description" content="经典的transformer架构如下图所示，分为编码器和解码器。构建模型的过程中，先构建通用的模块，如位置编码、多头自注意力、前馈神经网络层等，然后组建编码器和解码器，最后统一到一个模型中。">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="http://example.com/pic/transformers/1.png">
<meta property="og:image" content="http://example.com/pic/transformers/2.png">
<meta property="og:image" content="http://example.com/pic/transformers/3.png">
<meta property="og:image" content="http://example.com/pic/transformers/4.png">
<meta property="og:image" content="http://example.com/pic/transformers/5.png">
<meta property="og:image" content="http://example.com/pic/transformers/6.png">
<meta property="og:image" content="http://example.com/pic/transformers/7.png">
<meta property="article:published_time" content="2024-04-14T13:48:25.000Z">
<meta property="article:modified_time" content="2024-04-16T14:07:08.607Z">
<meta property="article:author" content="凹凸">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://example.com/pic/transformers/1.png">


<link rel="canonical" href="http://example.com/2024/04/14/%E4%BD%BF%E7%94%A8Pytorch%E6%90%AD%E5%BB%BATransformer/">



<script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":false,"isPost":true,"lang":"zh-CN","comments":true,"permalink":"http://example.com/2024/04/14/%E4%BD%BF%E7%94%A8Pytorch%E6%90%AD%E5%BB%BATransformer/","path":"2024/04/14/使用Pytorch搭建Transformer/","title":"使用Pytorch搭建Transformer"}</script>

<script class="next-config" data-name="calendar" type="application/json">""</script>
<title>使用Pytorch搭建Transformer | 凹凸岛</title>
  








  <noscript>
    <link rel="stylesheet" href="/css/noscript.css">
  </noscript>
<link rel="alternate" href="/atom.xml" title="凹凸岛" type="application/atom+xml">
</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="headband"></div>

  <main class="main">
    <div class="column">
      <header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏" role="button">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <p class="site-title">凹凸岛</p>
      <i class="logo-line"></i>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger" aria-label="搜索" role="button">
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a></li>
  </ul>
</nav>




</header>
        
  
  <aside class="sidebar">

    <div class="sidebar-inner sidebar-nav-active sidebar-toc-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
            <div class="post-toc animated"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#1%E3%80%81%E5%AE%9A%E4%B9%89%E9%80%9A%E7%94%A8%E7%9A%84%E5%B1%82"><span class="nav-number">1.</span> <span class="nav-text">1、定义通用的层</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#1-1-%E4%BD%8D%E7%BD%AE%E7%BC%96%E7%A0%81-Positional-Encoding"><span class="nav-number">1.1.</span> <span class="nav-text">1.1 位置编码 Positional Encoding</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#1-2-ScaledDotProductAttention"><span class="nav-number">1.2.</span> <span class="nav-text">1.2 ScaledDotProductAttention</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#1-3-%E5%A4%9A%E5%A4%B4%E8%87%AA%E6%B3%A8%E6%84%8F%E5%8A%9B-MultiHeadAttention"><span class="nav-number">1.3.</span> <span class="nav-text">1.3 多头自注意力 MultiHeadAttention</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#1-4-%E5%89%8D%E9%A6%88%E7%BD%91%E7%BB%9C-MLP-%E5%92%8C%E5%B1%82%E5%BD%92%E4%B8%80%E5%8C%96FeedForward-Networks"><span class="nav-number">1.4.</span> <span class="nav-text">1.4 前馈网络(MLP)和层归一化FeedForward Networks</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#1-5-Pad-Mask"><span class="nav-number">1.5.</span> <span class="nav-text">1.5 Pad Mask</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#2%E3%80%81%E5%AE%9A%E4%B9%89Encoder"><span class="nav-number">2.</span> <span class="nav-text">2、定义Encoder</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#3%E3%80%81%E5%AE%9A%E4%B9%89Decoder"><span class="nav-number">3.</span> <span class="nav-text">3、定义Decoder</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#3-1-%E5%AE%9A%E4%B9%89%E6%8E%A9%E8%94%BD%E8%87%AA%E6%B3%A8%E6%84%8F%E5%8A%9BSubsequence-Mask"><span class="nav-number">3.1.</span> <span class="nav-text">3.1 定义掩蔽自注意力Subsequence Mask</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#3-2-DecoderLayer"><span class="nav-number">3.2.</span> <span class="nav-text">3.2 DecoderLayer</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#3-3-%E7%BB%84%E5%90%88%E6%88%90Decoder"><span class="nav-number">3.3.</span> <span class="nav-text">3.3 组合成Decoder</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#4%E3%80%81%E5%B0%86%E5%AE%9A%E4%B9%89%E7%9A%84%E7%BC%96%E7%A0%81%E5%99%A8Enocder%E5%92%8C%E8%A7%A3%E7%A0%81%E5%99%A8Decoder%E7%BB%84%E5%90%88%E6%88%90Transformer"><span class="nav-number">4.</span> <span class="nav-text">4、将定义的编码器Enocder和解码器Decoder组合成Transformer</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#5%E3%80%81%E4%BD%BF%E7%94%A8%E4%B8%80%E4%B8%AA%E7%AE%80%E5%8D%95%E7%9A%84%E7%BF%BB%E8%AF%91%E8%AF%8D%E5%85%B8%E6%9D%A5%E6%B5%8B%E8%AF%95%E6%A8%A1%E5%9E%8B"><span class="nav-number">5.</span> <span class="nav-text">5、使用一个简单的翻译词典来测试模型</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#5-1-%E6%95%B0%E6%8D%AE%E9%A2%84%E5%A4%84%E7%90%86"><span class="nav-number">5.1.</span> <span class="nav-text">5.1 数据预处理</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#5-2-%E5%8A%A0%E8%BD%BD%E6%95%B0%E6%8D%AE%E5%AE%9A%E4%B9%89%E8%B6%85%E5%8F%82%E6%95%B0"><span class="nav-number">5.2.</span> <span class="nav-text">5.2 加载数据定义超参数</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#5-3-%E8%AE%AD%E7%BB%83"><span class="nav-number">5.3.</span> <span class="nav-text">5.3 训练</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#5-4-%E6%B5%8B%E8%AF%95"><span class="nav-number">5.4.</span> <span class="nav-text">5.4 测试</span></a></li></ol></li></ol></div>
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">凹凸</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
        <a href="/archives/">
          <span class="site-state-item-count">13</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
  </nav>
</div>

        </div>
      </div>
    </div>

    
  </aside>


    </div>

    <div class="main-inner post posts-expand">


  


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://example.com/2024/04/14/%E4%BD%BF%E7%94%A8Pytorch%E6%90%AD%E5%BB%BATransformer/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="凹凸">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="凹凸岛">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="使用Pytorch搭建Transformer | 凹凸岛">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          使用Pytorch搭建Transformer
        </h1>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2024-04-14 21:48:25" itemprop="dateCreated datePublished" datetime="2024-04-14T21:48:25+08:00">2024-04-14</time>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody"><p>经典的transformer架构如下图所示，分为编码器和解码器。构建模型的过程中，先构建通用的模块，如位置编码、多头自注意力、前馈神经网络层等，然后组建编码器和解码器，最后统一到一个模型中。<br><img src="/pic/transformers/1.png" width = "60%" height = "60%" /></p>
<span id="more"></span>
<h1 id="1、定义通用的层"><a href="#1、定义通用的层" class="headerlink" title="1、定义通用的层"></a>1、定义通用的层</h1><h2 id="1-1-位置编码-Positional-Encoding"><a href="#1-1-位置编码-Positional-Encoding" class="headerlink" title="1.1 位置编码 Positional Encoding"></a>1.1 位置编码 Positional Encoding</h2><ul>
<li>用于为输入的词向量添加位置编码</li>
<li>思路：<ul>
<li>先构建一个包含5000长度的向量表</li>
<li>在构建关于位置信息的矩阵，使用余弦衰减函数作为位置信息编码</li>
<li>因为我们在构建注意力机制时希望当前位置和其他位置之间的相似度可以随着距离的增大而逐渐减小。余弦衰减函数正好满足这样的需求，因为它越靠近原点的地方越陡峭，在远离原点的地方则越来越平缓。这样就可以让相近的位置有更多的交互机会，而较远的位置则较少参与交互。</li>
</ul>
</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">PositionalEncoding</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, d_model, dropout=<span class="number">0.1</span>, max_len=<span class="number">5000</span></span>): <span class="comment"># dropout原文是0.1</span></span><br><span class="line">        <span class="comment"># max_len是假设一个句子最多包含5000个token</span></span><br><span class="line">        <span class="built_in">super</span>(PositionalEncoding, self).__init__()</span><br><span class="line">        self.dropout = nn.Dropout(p=dropout)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 开始位置编码部分，先生成一个max_len * d_model的矩阵，即5000*512</span></span><br><span class="line">        <span class="comment"># 5000是一个句子中最多的token数， 512是一个token用多长的向量来表示，5000*512这个矩阵用于表示一个句子的信息</span></span><br><span class="line">        pe = torch.zeros(max_len, d_model)</span><br><span class="line">        <span class="comment"># pos用于表示每个位置的索引，从0到5000</span></span><br><span class="line">        pos = torch.arange(<span class="number">0</span>, max_len, dtype=torch.<span class="built_in">float</span>).unsqueeze(<span class="number">1</span>) <span class="comment"># pos.shape=([5000, 1])</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 先把括号内的分时求出来，pos是[5000,1]，分母是256，通过广播机制相乘后是[5000, 256]</span></span><br><span class="line">        <span class="comment"># 计算一个余弦衰减函数， 将输入的位置信息转化为向量模式</span></span><br><span class="line">        div_term = pos / <span class="built_in">pow</span>(<span class="number">10000.0</span>, torch.arange(<span class="number">0</span>, d_model, <span class="number">2</span>).<span class="built_in">float</span>() / d_model) </span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 在取正余弦，周期型的取正弦和余弦有助于捕获更多位置信息</span></span><br><span class="line">        pe[:, <span class="number">0</span>::<span class="number">2</span>] = torch.sin(div_term)</span><br><span class="line">        pe[:, <span class="number">1</span>::<span class="number">2</span>] = torch.cos(div_term)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 一个句子要做一次配pe， 一个batch中会有多个句子，所以增加以为用来和输入的一个batch的数据相加时做广播</span></span><br><span class="line">        pe = pe.unsqueeze(<span class="number">0</span>) <span class="comment"># [5000,512]到[15000,512]</span></span><br><span class="line">        <span class="comment"># 将pe作为固定参数保存到缓冲区，不会被更新</span></span><br><span class="line">        self.register_buffer(<span class="string">&#x27;pe&#x27;</span>, pe)</span><br><span class="line">        </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="comment"># x时输入[batch_size, seq_len, d_model]</span></span><br><span class="line">        <span class="comment"># 5000时预定义的最大seq_len, 也就是把至多这么多位置的位置函数都计算好了，使用的时候直接相加就行了</span></span><br><span class="line">        x = x + self.pe[:, :x.size(<span class="number">1</span>), :]</span><br><span class="line">        <span class="keyword">return</span> self.dropout(x) <span class="comment"># 加入降噪，防止过拟合， 返回[batch_size, seq_len, d_model]</span></span><br></pre></td></tr></table></figure>

<h2 id="1-2-ScaledDotProductAttention"><a href="#1-2-ScaledDotProductAttention" class="headerlink" title="1.2 ScaledDotProductAttention"></a>1.2 ScaledDotProductAttention</h2><ul>
<li>用于计算缩放点积注意力， 在MultiHeadAttention中被调用。</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">ScaledDotProductionAttention</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(ScaledDotProductionAttention, self).__init__()</span><br><span class="line">        </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, Q, K, V, attn_mask</span>):</span><br><span class="line">        <span class="comment"># Q: [batch_size, n_heads, len_q, d_k]</span></span><br><span class="line">        <span class="comment"># K: [batch_size, n_heads, len_k, d_k]</span></span><br><span class="line">        <span class="comment"># V: [batch_size, n_heads, len_v(=len_k), d_v]</span></span><br><span class="line">        <span class="comment"># 总共使用两处自注意力， 一处时自注意力，一处时掩蔽自注意力</span></span><br><span class="line">        <span class="comment"># attn_mask: [batch_size, n_heads, seq_len, seq_len]</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 1、计算注意力分数QK^T/sqrt(d_k),</span></span><br><span class="line">        <span class="comment"># np.sqrt(d_k) 是一个平方根函数，它可以将一个非负实数的平方根提取出来。除以 sqrt(d_k) 来降低结果的数值大小，使其落入一个小范围。</span></span><br><span class="line">        <span class="comment"># 为什么要除一个根号d_k？这是因为点积的数量级增长很大，因此将 softmax 函数推向了梯度极小的区域</span></span><br><span class="line">        <span class="comment"># 这样做的目的是使softmax函数能够更加稳定地运行，从而提高模型的性能和鲁棒性。</span></span><br><span class="line">        scores = torch.matmul(Q, K.transpose(-<span class="number">1</span>, -<span class="number">2</span>)) / np.sqrt(d_k) </span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 2、进行mask和softmax</span></span><br><span class="line">        <span class="comment"># mask为True的地方会被设置为-1e9，相当于不考虑</span></span><br><span class="line">        scores.masked_fill_(attn_mask, -<span class="number">1e9</span>)</span><br><span class="line">        attn = nn.Softmax(dim=-<span class="number">1</span>)(scores) <span class="comment"># attn:[batch_szie, n_heads, len_q, len_k]</span></span><br><span class="line">        <span class="comment"># 3、乘V得到最终的加权和</span></span><br><span class="line">        context = torch.matmul(attn, V) <span class="comment"># context:[batch_size, n_heads, len_q, d_v]</span></span><br><span class="line">        <span class="comment"># 返回的context: [batch_size, n_heads, len_q, d_v]本质上还是batch_size个句子，</span></span><br><span class="line">        <span class="comment"># 只不过每个句子中词向量维度512被分成了8个部分，分别由8个头各自看一部分，每个头算的是整个句子(一列)的512/8=64个维度，最后按列拼接起来</span></span><br><span class="line">        </span><br><span class="line">        <span class="keyword">return</span> context <span class="comment"># [batch_size, n_heads, len_q, d_v]</span></span><br></pre></td></tr></table></figure>

<h2 id="1-3-多头自注意力-MultiHeadAttention"><a href="#1-3-多头自注意力-MultiHeadAttention" class="headerlink" title="1.3 多头自注意力 MultiHeadAttention"></a>1.3 多头自注意力 MultiHeadAttention</h2><ul>
<li>多头注意力的实现， Transformer的核心。</li>
<li>原来一个头对应512个特征，现在分为8个头，每个头处理64个特征<img src="/pic/transformers/2.png" width = "100%" height = "100%" /></li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">MultiHeadAttention</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(MultiHeadAttention, self).__init__()</span><br><span class="line">        self.W_Q = nn.Linear(d_model, d_model)</span><br><span class="line">        self.W_K = nn.Linear(d_model, d_model)</span><br><span class="line">        self.W_V = nn.Linear(d_model, d_model)</span><br><span class="line">        self.concat = nn.Linear(d_model, d_model)</span><br><span class="line">        </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, input_Q, input_K, input_V, attn_mask</span>):</span><br><span class="line">        <span class="comment"># input_Q:[batch_size, len_q, d_model] len_q是作为query的句子的长度，比如enc_inputs(2, 5, 512), 句子长度5就是len_q</span></span><br><span class="line">        <span class="comment"># input_K:[batch_size, len_k, d_model]</span></span><br><span class="line">        <span class="comment"># input_V:[batch_size, len_v(=len_k), d_model]</span></span><br><span class="line">        </span><br><span class="line">        residual, batch_size = input_Q, input_Q.size(<span class="number">0</span>)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 1、线性投影：通过原始输入，映射出QKV，然后调整形状，准备多个头，每个头对应64个特征</span></span><br><span class="line">        Q = self.W_Q(input_Q).view(batch_size, -<span class="number">1</span>, n_heads, d_k).transpose(<span class="number">1</span>, <span class="number">2</span>) <span class="comment"># Q:[batch_size, n_heads, len_q, d_k]</span></span><br><span class="line">        K = self.W_K(input_K).view(batch_size, -<span class="number">1</span>, n_heads, d_k).transpose(<span class="number">1</span>, <span class="number">2</span>) <span class="comment"># K:[batch_size, n_heads, len_k, d_k]</span></span><br><span class="line">        V = self.W_V(input_V).view(batch_size, -<span class="number">1</span>, n_heads, d_v).transpose(<span class="number">1</span>, <span class="number">2</span>) <span class="comment"># V:[batch_size, n_heads, len_v, d_k]</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 2、计算注意力</span></span><br><span class="line">        <span class="comment"># 先自我复制n_heads次，为每个头准备一份mask</span></span><br><span class="line">        attn_mask = attn_mask.unsqueeze(<span class="number">1</span>).repeat(<span class="number">1</span>, n_heads, <span class="number">1</span>, <span class="number">1</span>) <span class="comment"># attn_mask:[batch_size, n_heads, seq_len, seq_len]</span></span><br><span class="line">        context = ScaledDotProductionAttention()(Q, K, V, attn_mask) <span class="comment"># context:[batch_size, n_heads, len_q, d_v]</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 3、concat部分</span></span><br><span class="line">        context = torch.cat([context[:,i,:,:] <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(context.size(<span class="number">1</span>))], dim=-<span class="number">1</span>)</span><br><span class="line">        output = self.concat(context) <span class="comment"># [batch_size, len_q, d_model]</span></span><br><span class="line">        <span class="keyword">return</span> nn.LayerNorm(d_model).cuda()(output + residual) <span class="comment"># output:[batch_size, len_q, d_mdoel] 这一步加了残差</span></span><br></pre></td></tr></table></figure>

<h2 id="1-4-前馈网络-MLP-和层归一化FeedForward-Networks"><a href="#1-4-前馈网络-MLP-和层归一化FeedForward-Networks" class="headerlink" title="1.4 前馈网络(MLP)和层归一化FeedForward Networks"></a>1.4 前馈网络(MLP)和层归一化FeedForward Networks</h2><ul>
<li>对应Feed Forward和 Add &amp; Norm层归一化</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">PositionwiseFeedForward</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(PositionwiseFeedForward, self).__init__()</span><br><span class="line">        <span class="comment"># 前馈神经网络也就是MLP多层感知机</span></span><br><span class="line">        self.fc = nn.Sequential(</span><br><span class="line">           nn.Linear(d_model, d_ff),</span><br><span class="line">            nn.ReLU(),</span><br><span class="line">            nn.Linear(d_ff, d_model)</span><br><span class="line">        )</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, inputs</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;inputs:[batch_size, seq_len, d_model]&quot;&quot;&quot;</span></span><br><span class="line">        residual = inputs</span><br><span class="line">        output = self.fc(inputs)</span><br><span class="line">        <span class="keyword">return</span> nn.LayerNorm(d_model).cuda()(output + residual) <span class="comment"># 先残差连接，然后层归一化从右往左计算</span></span><br></pre></td></tr></table></figure>

<h2 id="1-5-Pad-Mask"><a href="#1-5-Pad-Mask" class="headerlink" title="1.5 Pad Mask"></a>1.5 Pad Mask</h2><ul>
<li>首先我们要清楚，这是一个计算mask的函数，它的返回是一个布尔矩阵，为True的位置是需要被mask掉的，False的位置是不需要动的</li>
<li>其次这个函数是理解Transformer代码中非常重要的一环，因为我们输入模型的句子有长有短，我们用占位符P统一补足成了最长的那个句子的长度，而这些占位符是没有意义的，不能让他们吸收到query的注意力，因此我们要把这些位置设为True</li>
<li>这个计算出的mask在何时被使用呢？<ul>
<li>在query和key的转置相乘得出（len_q,len_k）这个注意力分数矩阵以后，将使用本函数得到的mask来掩盖相乘结果矩阵</li>
<li>原来的相乘结果矩阵（len_q,len_k）中第 i 行第 j 列的意义是“作为q的序列中第i个词对作为k的序列中第j个词的注意力分数”，而第 i 整行就是q中这个词对k中所有词的注意力，第 j 整列就是q中所有词对k中第j个词的注意力分数，作为padding，q中的所有词都不应该注意它，因此对应列均需设为True</li>
</ul>
</li>
<li>为什么只有k的padding位被mask了，q的padding位为什么没被mask？（即此函数的返回矩阵为什么只有最后几列是True，最后几行不应该也是True么）<ul>
<li>按理来说是这样的，作为padding不该被别人注意，同时它也不该注意别人，计算出的padding对其他词的注意力也是无意义的，我们这里其实是偷了个懒，但这是因为：q中的padding对k中的词的注意力我们是不会用到的，因为我们不会用一个padding字符去预测下一个词，并且它的向量表示不管怎么更新都不会影响到别的q中别的词的计算，所以我们就放任自流了。但k中的padding不一样，如果不管它将无意义的吸收掉大量q中词汇的注意力，使得模型的学习出现偏差。</li>
</ul>
</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 为enc_input和 dec_input做一个mask，把占位符P的token（也就是0）mask掉</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">get_attn_pad_mask</span>(<span class="params">seq_q, seq_k</span>):</span><br><span class="line">    batch_size, len_q = seq_q.size()</span><br><span class="line">    batch_size, len_k = seq_k.size()</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># seq_k.data.eq(0)返回一个相同大小的布尔张量，seq_k元素等于0的位置为True， 否则为False</span></span><br><span class="line">    <span class="comment"># .unsqueeze(1)扩充维度</span></span><br><span class="line">    pad_attn_mask = seq_k.data.eq(<span class="number">0</span>).unsqueeze(<span class="number">1</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 要为每一个q提供一份k， 所以把第二维度扩展了q次</span></span><br><span class="line">    <span class="comment"># expand并非真正加倍内存，知识重复了引用，对任意引用的修改都会修改原始值</span></span><br><span class="line">    <span class="comment"># 不修改这个mask，用来节省内存</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> pad_attn_mask.expand(batch_size, len_q, len_k) <span class="comment"># return :[batch_size, len_q, len_k]</span></span><br></pre></td></tr></table></figure>

<h1 id="2、定义Encoder"><a href="#2、定义Encoder" class="headerlink" title="2、定义Encoder"></a>2、定义Encoder</h1><ul>
<li>先定义一个Encoder Layer层：包含一个MultiHeadAttention和一个FFN。</li>
<li>叠加几次然后加上一个源序列词向量嵌入nn.Embedding、一个位置编码和6个Encoder Layer位置编码组成完整的Encoder。<img src="/pic/transformers/3.png" width = "40%" height = "40%" /></li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">EncoderLayer</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(EncoderLayer, self).__init__()</span><br><span class="line">        self.enc_self_attn = MultiHeadAttention()</span><br><span class="line">        self.pos_ffn = PositionwiseFeedForward()</span><br><span class="line">        </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, enc_inputs, enc_self_attn_mask</span>):</span><br><span class="line">        <span class="comment"># enc_inputs:[batch_size, scr_len, d_model]</span></span><br><span class="line">        <span class="comment"># enc_self_attn_mask:[batch_size, scr_len, src_len]</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Q,K,V均为enc_inputs</span></span><br><span class="line">        enc_ouputs = self.enc_self_attn(enc_inputs, enc_inputs, enc_inputs, enc_self_attn_mask) <span class="comment"># [batch_size, src_len, d_model]</span></span><br><span class="line">        enc_ouputs = self.pos_ffn(enc_ouputs) <span class="comment"># [batch_size, scr_len, d_model]</span></span><br><span class="line">        </span><br><span class="line">        <span class="keyword">return</span> enc_ouputs <span class="comment"># [batch_size,  src_len, d_mdoel]</span></span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Encoder</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(Encoder, self).__init__()</span><br><span class="line">        <span class="comment"># 使用torch自带的词嵌入，输入是词元数和每个词元使用多少维特征表示</span></span><br><span class="line">        self.src_emb = nn.Embedding(src_vocab_size, d_model)</span><br><span class="line">        </span><br><span class="line">        self.pos_emb = PositionalEncoding(d_model)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 创建6个编码器，作为list</span></span><br><span class="line">        self.layers = nn.ModuleList([EncoderLayer() <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(n_layers)])</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, enc_inputs</span>):</span><br><span class="line">        <span class="comment"># enc_inputs:[batch_size, src_len]</span></span><br><span class="line">        <span class="comment"># 第一步词嵌入会将输入张量形状改变</span></span><br><span class="line">        enc_outputs = self.src_emb(enc_inputs) <span class="comment"># [batch_size, src_len]--&gt;[batch_size, src_len, d_model]</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 加入位置编码，张量形状不变</span></span><br><span class="line">        enc_outputs = self.pos_emb(enc_outputs) <span class="comment"># --&gt;[batch_size, src_len, d_model]</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 准备mask</span></span><br><span class="line">        enc_self_attn_mask = get_attn_pad_mask(enc_inputs, enc_inputs) <span class="comment"># [batch_size, src_len, src_len] 和输入形状一样</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 使用6个编码器</span></span><br><span class="line">        <span class="keyword">for</span> layer <span class="keyword">in</span> self.layers:</span><br><span class="line">            enc_outputs = layer(enc_outputs, enc_self_attn_mask)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> enc_outputs <span class="comment"># [batch_size, src_len, d_model]</span></span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">encoder = Encoder()</span><br><span class="line"><span class="built_in">print</span>(encoder)</span><br></pre></td></tr></table></figure>

<figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">Encoder(</span><br><span class="line">  (src<span class="emphasis">_emb): Embedding(6, 512)</span></span><br><span class="line"><span class="emphasis">  (pos_</span>emb): PositionalEncoding(</span><br><span class="line"><span class="code">    (dropout): Dropout(p=0.1, inplace=False)</span></span><br><span class="line"><span class="code">  )</span></span><br><span class="line"><span class="code">  (layers): ModuleList(</span></span><br><span class="line"><span class="code">    (0-5): 6 x EncoderLayer(</span></span><br><span class="line"><span class="code">      (enc_self_attn): MultiHeadAttention(</span></span><br><span class="line"><span class="code">        (W_Q): Linear(in_features=512, out_features=512, bias=True)</span></span><br><span class="line"><span class="code">        (W_K): Linear(in_features=512, out_features=512, bias=True)</span></span><br><span class="line"><span class="code">        (W_V): Linear(in_features=512, out_features=512, bias=True)</span></span><br><span class="line"><span class="code">        (concat): Linear(in_features=512, out_features=512, bias=True)</span></span><br><span class="line"><span class="code">      )</span></span><br><span class="line"><span class="code">      (pos_ffn): PositionwiseFeedForward(</span></span><br><span class="line"><span class="code">        (fc): Sequential(</span></span><br><span class="line"><span class="code">          (0): Linear(in_features=512, out_features=2048, bias=True)</span></span><br><span class="line"><span class="code">          (1): ReLU()</span></span><br><span class="line"><span class="code">          (2): Linear(in_features=2048, out_features=512, bias=True)</span></span><br><span class="line"><span class="code">        )</span></span><br><span class="line"><span class="code">      )</span></span><br><span class="line"><span class="code">    )</span></span><br><span class="line"><span class="code">  )</span></span><br><span class="line"><span class="code">)</span></span><br></pre></td></tr></table></figure>

<h1 id="3、定义Decoder"><a href="#3、定义Decoder" class="headerlink" title="3、定义Decoder"></a>3、定义Decoder</h1><h2 id="3-1-定义掩蔽自注意力Subsequence-Mask"><a href="#3-1-定义掩蔽自注意力Subsequence-Mask" class="headerlink" title="3.1 定义掩蔽自注意力Subsequence Mask"></a>3.1 定义掩蔽自注意力Subsequence Mask</h2><ul>
<li>对应Transformer模型架构中Decoder的第一个掩蔽注意力，防止模型看到未来时刻的输入</li>
<li>在推理的时候，是看不到还未推理的单词的所以在训练时也需要将未来时刻的词元掩蔽掉<img src="/pic/transformers/4.png" width = "60%" height = "60%" /></li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 用于获取对后续位置的掩码，防止在预测过程中看到未来时刻的输入</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">get_attn_subsequence_mask</span>(<span class="params">seq</span>):</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># seq:[batch_size, tgt_len]</span></span><br><span class="line">    <span class="comment"># batch_size个tgt_len * tgt_len的mask矩阵</span></span><br><span class="line">    attn_shape = [seq.size(<span class="number">0</span>), seq.size(<span class="number">1</span>), seq.size(<span class="number">1</span>)]</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># np.triu是生成一个upper traingular matrix上三角矩阵，K是相对主对角线的偏移量</span></span><br><span class="line">    <span class="comment"># k=1是之不包含住对角线</span></span><br><span class="line">    subsequence_mask = np.triu(np.ones(attn_shape), k=<span class="number">1</span>)</span><br><span class="line">    subsequence_mask = torch.from_numpy(subsequence_mask).byte() <span class="comment"># 因为只有0，1所以使用byte节省内存</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> subsequence_mask <span class="comment"># [batch_size, tgt_len, tgt_len]</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h2 id="3-2-DecoderLayer"><a href="#3-2-DecoderLayer" class="headerlink" title="3.2 DecoderLayer"></a>3.2 DecoderLayer</h2><ul>
<li>包含两个MultiHeadAttention和一个FFN<img src="/pic/transformers/5.png" width = "70%" height = "70%" /></li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">DecoderLayer</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(DecoderLayer, self).__init__()</span><br><span class="line">        self.dec_self_attn = MultiHeadAttention()</span><br><span class="line">        self.dec_enc_attn = MultiHeadAttention()</span><br><span class="line">        self.pos_ffn = PositionwiseFeedForward()</span><br><span class="line">        </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, dec_inputs, enc_outputs, dec_self_attn_mask, dec_enc_attn_mask</span>):</span><br><span class="line">        <span class="comment"># dec_inputs: [batch_size, tgt_len, d_model]</span></span><br><span class="line">        <span class="comment"># enc_outputs:[batch_size, src_len, d_model]</span></span><br><span class="line">        <span class="comment"># dec_self_attn_mask:[batch_size, tgt_len, d_model]</span></span><br><span class="line">        <span class="comment"># dec_enc_attn_mask:[batch_size, tgt_len, src_len] 前者是Q后者是K</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 第一个掩码自注意力，输入是上一个输出，所以QKV都是一样的</span></span><br><span class="line">        dec_outputs = self.dec_self_attn(dec_inputs, dec_inputs, dec_inputs, dec_self_attn_mask)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 第二个多头自注意力，并且使用的是Q，去查询编码器输出的K，V也是编码器输出的！</span></span><br><span class="line">        dec_outputs = self.dec_enc_attn(dec_outputs, enc_outputs, enc_outputs, dec_enc_attn_mask)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 对后一个是ffw层</span></span><br><span class="line">        dec_outputs = self.pos_ffn(dec_outputs)</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">return</span> dec_outputs <span class="comment"># [batch_size, tgt_len, d_model]</span></span><br></pre></td></tr></table></figure>

<h2 id="3-3-组合成Decoder"><a href="#3-3-组合成Decoder" class="headerlink" title="3.3 组合成Decoder"></a>3.3 组合成Decoder</h2><ul>
<li>包含一个目标序列词向量序列嵌入，一个位置编码和6个Decoder Layer。<img src="/pic/transformers/6.png" width = "50%" height = "50%" /></li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Decoder</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(Decoder, self).__init__()</span><br><span class="line">        self.tgt_emb = nn.Embedding(tgt_vocab_size, d_model)</span><br><span class="line">        self.pos_emb = PositionalEncoding(d_model)</span><br><span class="line">        self.layers = nn.ModuleList([DecoderLayer() <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(n_layers)])</span><br><span class="line">        </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, dec_inputs, enc_inputs, enc_outputs</span>):</span><br><span class="line">        <span class="comment"># dec_inputs:[batch_size, tgt_len] 对应Q</span></span><br><span class="line">        <span class="comment"># enc_inputs:[batch_size, src_len] 对应K V</span></span><br><span class="line">        <span class="comment"># enc_outputs:[batch_size, src_len, d_model] 用来计算padding mask</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 词嵌入</span></span><br><span class="line">        dec_outputs = self.tgt_emb(dec_inputs) </span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 添加位置编码，并转移到cuda</span></span><br><span class="line">        dec_outputs = self.pos_emb(dec_outputs).cuda() </span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 计算占位符需要的掩码</span></span><br><span class="line">        dec_self_attn_pad_mask = get_attn_pad_mask(dec_inputs, dec_inputs).cuda()  </span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 计算掩码自注意力需要的掩码</span></span><br><span class="line">        dec_self_attn_subsequence_mask = get_attn_subsequence_mask(dec_inputs).cuda()</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 将两个掩码合并在一起，大于0的位置是需要mask的, 这是第一个掩码自注意力需要的mask</span></span><br><span class="line">        dec_self_attn_mask = torch.gt((dec_self_attn_pad_mask + dec_self_attn_subsequence_mask), <span class="number">0</span>).cuda()</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 这是第二个多头自注意力需要的mask</span></span><br><span class="line">        dec_enc_attn_mask = get_attn_pad_mask(dec_inputs, enc_inputs)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 循环6个层</span></span><br><span class="line">        <span class="keyword">for</span> layer <span class="keyword">in</span> self.layers:</span><br><span class="line">            dec_outputs = layer(dec_outputs, enc_outputs, dec_self_attn_mask, dec_enc_attn_mask)</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">return</span> dec_outputs <span class="comment"># [batch_size, tgt_len, d_model]</span></span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">decoder = Decoder()</span><br><span class="line"><span class="built_in">print</span>(decoder)</span><br></pre></td></tr></table></figure>

<figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line">Decoder(</span><br><span class="line">  (tgt<span class="emphasis">_emb): Embedding(9, 512)</span></span><br><span class="line"><span class="emphasis">  (pos_</span>emb): PositionalEncoding(</span><br><span class="line"><span class="code">    (dropout): Dropout(p=0.1, inplace=False)</span></span><br><span class="line"><span class="code">  )</span></span><br><span class="line"><span class="code">  (layers): ModuleList(</span></span><br><span class="line"><span class="code">    (0-5): 6 x DecoderLayer(</span></span><br><span class="line"><span class="code">      (dec_self_attn): MultiHeadAttention(</span></span><br><span class="line"><span class="code">        (W_Q): Linear(in_features=512, out_features=512, bias=True)</span></span><br><span class="line"><span class="code">        (W_K): Linear(in_features=512, out_features=512, bias=True)</span></span><br><span class="line"><span class="code">        (W_V): Linear(in_features=512, out_features=512, bias=True)</span></span><br><span class="line"><span class="code">        (concat): Linear(in_features=512, out_features=512, bias=True)</span></span><br><span class="line"><span class="code">      )</span></span><br><span class="line"><span class="code">      (dec_enc_attn): MultiHeadAttention(</span></span><br><span class="line"><span class="code">        (W_Q): Linear(in_features=512, out_features=512, bias=True)</span></span><br><span class="line"><span class="code">        (W_K): Linear(in_features=512, out_features=512, bias=True)</span></span><br><span class="line"><span class="code">        (W_V): Linear(in_features=512, out_features=512, bias=True)</span></span><br><span class="line"><span class="code">        (concat): Linear(in_features=512, out_features=512, bias=True)</span></span><br><span class="line"><span class="code">      )</span></span><br><span class="line"><span class="code">      (pos_ffn): PositionwiseFeedForward(</span></span><br><span class="line"><span class="code">        (fc): Sequential(</span></span><br><span class="line"><span class="code">          (0): Linear(in_features=512, out_features=2048, bias=True)</span></span><br><span class="line"><span class="code">          (1): ReLU()</span></span><br><span class="line"><span class="code">          (2): Linear(in_features=2048, out_features=512, bias=True)</span></span><br><span class="line"><span class="code">        )</span></span><br><span class="line"><span class="code">      )</span></span><br><span class="line"><span class="code">    )</span></span><br><span class="line"><span class="code">  )</span></span><br><span class="line"><span class="code">)</span></span><br><span class="line"><span class="code"></span></span><br></pre></td></tr></table></figure>

<h1 id="4、将定义的编码器Enocder和解码器Decoder组合成Transformer"><a href="#4、将定义的编码器Enocder和解码器Decoder组合成Transformer" class="headerlink" title="4、将定义的编码器Enocder和解码器Decoder组合成Transformer"></a>4、将定义的编码器Enocder和解码器Decoder组合成Transformer</h1><ul>
<li>包含一个Encoder、一个Decoder、一个nn.Linear</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Transformer</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(Transformer, self).__init__()</span><br><span class="line">        self.encoder = Encoder().cuda()</span><br><span class="line">        self.decoder = Decoder().cuda()</span><br><span class="line">        self.projection = nn.Linear(d_model, tgt_vocab_size).cuda()</span><br><span class="line">        </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, enc_inputs, dec_inputs</span>):</span><br><span class="line">        <span class="comment"># enc_inputs:[batch_size, src_len]</span></span><br><span class="line">        <span class="comment"># dec_inputs:[batch_size, tgt_len]</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 编码器输入enc_inputs,输出enc_outputs</span></span><br><span class="line">        enc_outputs = self.encoder(enc_inputs)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 解码器输入dec_inputs-标注， enc_inputs-编码器的输入，为了计算mask，enc_outputs-编码器的输出为了计算多头注意力 </span></span><br><span class="line">        <span class="comment"># 输出是[batch_size, tgt_len, d_model]</span></span><br><span class="line">        dec_outputs = self.decoder(dec_inputs, enc_inputs, enc_outputs)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 将512维表示的token，转为换词表的长度，表示是每个词元的概率</span></span><br><span class="line">        dec_logits = self.projection(dec_outputs) <span class="comment"># dec_logits:[batch_size, tgt_len, tgt_vocab_size]</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 解散batch，一个batch中有batch_size个句子， 每个句子有tgt_len个词元</span></span><br><span class="line">        <span class="comment"># 将batch_size拉直，每个批次句子全部从上往下排列</span></span><br><span class="line">        <span class="comment"># 最后变形的原因是：nn.CrossEntropyLoss接收的输入的第二个维度必须是类别</span></span><br><span class="line">        <span class="keyword">return</span> dec_logits.view(-<span class="number">1</span>, dec_logits.size(-<span class="number">1</span>)) <span class="comment"># [batch_size*tgt_len, tgt_vocab_size]</span></span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">summary(Transformer(), [(<span class="number">2</span>, <span class="number">5</span>), (<span class="number">2</span>, <span class="number">6</span>)], dtypes=[torch.long, torch.long])</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line">====================================================================================================</span><br><span class="line">Layer (<span class="built_in">type</span>:depth-idx)                             Output Shape              Param <span class="comment">#</span></span><br><span class="line">====================================================================================================</span><br><span class="line">Transformer                                        [<span class="number">12</span>, <span class="number">9</span>]                   --</span><br><span class="line">├─Encoder: <span class="number">1</span>-<span class="number">1</span>                                     [<span class="number">2</span>, <span class="number">5</span>, <span class="number">512</span>]               --</span><br><span class="line">│    └─Embedding: <span class="number">2</span>-<span class="number">1</span>                              [<span class="number">2</span>, <span class="number">5</span>, <span class="number">512</span>]               <span class="number">3</span>,072</span><br><span class="line">│    └─PositionalEncoding: <span class="number">2</span>-<span class="number">2</span>                     [<span class="number">2</span>, <span class="number">5</span>, <span class="number">512</span>]               --</span><br><span class="line">│    │    └─Dropout: <span class="number">3</span>-<span class="number">1</span>                           [<span class="number">2</span>, <span class="number">5</span>, <span class="number">512</span>]               --</span><br><span class="line">│    └─ModuleList: <span class="number">2</span>-<span class="number">3</span>                             --                        --</span><br><span class="line">│    │    └─EncoderLayer: <span class="number">3</span>-<span class="number">2</span>                      [<span class="number">2</span>, <span class="number">5</span>, <span class="number">512</span>]               <span class="number">3</span>,<span class="number">150</span>,<span class="number">336</span></span><br><span class="line">│    │    └─EncoderLayer: <span class="number">3</span>-<span class="number">3</span>                      [<span class="number">2</span>, <span class="number">5</span>, <span class="number">512</span>]               <span class="number">3</span>,<span class="number">150</span>,<span class="number">336</span></span><br><span class="line">│    │    └─EncoderLayer: <span class="number">3</span>-<span class="number">4</span>                      [<span class="number">2</span>, <span class="number">5</span>, <span class="number">512</span>]               <span class="number">3</span>,<span class="number">150</span>,<span class="number">336</span></span><br><span class="line">│    │    └─EncoderLayer: <span class="number">3</span>-<span class="number">5</span>                      [<span class="number">2</span>, <span class="number">5</span>, <span class="number">512</span>]               <span class="number">3</span>,<span class="number">150</span>,<span class="number">336</span></span><br><span class="line">│    │    └─EncoderLayer: <span class="number">3</span>-<span class="number">6</span>                      [<span class="number">2</span>, <span class="number">5</span>, <span class="number">512</span>]               <span class="number">3</span>,<span class="number">150</span>,<span class="number">336</span></span><br><span class="line">│    │    └─EncoderLayer: <span class="number">3</span>-<span class="number">7</span>                      [<span class="number">2</span>, <span class="number">5</span>, <span class="number">512</span>]               <span class="number">3</span>,<span class="number">150</span>,<span class="number">336</span></span><br><span class="line">├─Decoder: <span class="number">1</span>-<span class="number">2</span>                                     [<span class="number">2</span>, <span class="number">6</span>, <span class="number">512</span>]               --</span><br><span class="line">│    └─Embedding: <span class="number">2</span>-<span class="number">4</span>                              [<span class="number">2</span>, <span class="number">6</span>, <span class="number">512</span>]               <span class="number">4</span>,<span class="number">608</span></span><br><span class="line">│    └─PositionalEncoding: <span class="number">2</span>-<span class="number">5</span>                     [<span class="number">2</span>, <span class="number">6</span>, <span class="number">512</span>]               --</span><br><span class="line">│    │    └─Dropout: <span class="number">3</span>-<span class="number">8</span>                           [<span class="number">2</span>, <span class="number">6</span>, <span class="number">512</span>]               --</span><br><span class="line">│    └─ModuleList: <span class="number">2</span>-<span class="number">6</span>                             --                        --</span><br><span class="line">│    │    └─DecoderLayer: <span class="number">3</span>-<span class="number">9</span>                      [<span class="number">2</span>, <span class="number">6</span>, <span class="number">512</span>]               <span class="number">4</span>,<span class="number">200</span>,<span class="number">960</span></span><br><span class="line">│    │    └─DecoderLayer: <span class="number">3</span>-<span class="number">10</span>                     [<span class="number">2</span>, <span class="number">6</span>, <span class="number">512</span>]               <span class="number">4</span>,<span class="number">200</span>,<span class="number">960</span></span><br><span class="line">│    │    └─DecoderLayer: <span class="number">3</span>-<span class="number">11</span>                     [<span class="number">2</span>, <span class="number">6</span>, <span class="number">512</span>]               <span class="number">4</span>,<span class="number">200</span>,<span class="number">960</span></span><br><span class="line">│    │    └─DecoderLayer: <span class="number">3</span>-<span class="number">12</span>                     [<span class="number">2</span>, <span class="number">6</span>, <span class="number">512</span>]               <span class="number">4</span>,<span class="number">200</span>,<span class="number">960</span></span><br><span class="line">│    │    └─DecoderLayer: <span class="number">3</span>-<span class="number">13</span>                     [<span class="number">2</span>, <span class="number">6</span>, <span class="number">512</span>]               <span class="number">4</span>,<span class="number">200</span>,<span class="number">960</span></span><br><span class="line">│    │    └─DecoderLayer: <span class="number">3</span>-<span class="number">14</span>                     [<span class="number">2</span>, <span class="number">6</span>, <span class="number">512</span>]               <span class="number">4</span>,<span class="number">200</span>,<span class="number">960</span></span><br><span class="line">├─Linear: <span class="number">1</span>-<span class="number">3</span>                                      [<span class="number">2</span>, <span class="number">6</span>, <span class="number">9</span>]                 <span class="number">4</span>,<span class="number">617</span></span><br><span class="line">====================================================================================================</span><br><span class="line">Total params: <span class="number">44</span>,<span class="number">120</span>,073</span><br><span class="line">Trainable params: <span class="number">44</span>,<span class="number">120</span>,073</span><br><span class="line">Non-trainable params: <span class="number">0</span></span><br><span class="line">Total mult-adds (M): <span class="number">88.24</span></span><br><span class="line">====================================================================================================</span><br><span class="line">Input size (MB): <span class="number">0.00</span></span><br><span class="line">Forward/backward <span class="keyword">pass</span> size (MB): <span class="number">6.04</span></span><br><span class="line">Params size (MB): <span class="number">176.48</span></span><br><span class="line">Estimated Total Size (MB): <span class="number">182.52</span></span><br><span class="line">====================================================================================================</span><br></pre></td></tr></table></figure>

<h1 id="5、使用一个简单的翻译词典来测试模型"><a href="#5、使用一个简单的翻译词典来测试模型" class="headerlink" title="5、使用一个简单的翻译词典来测试模型"></a>5、使用一个简单的翻译词典来测试模型</h1><h2 id="5-1-数据预处理"><a href="#5-1-数据预处理" class="headerlink" title="5.1 数据预处理"></a>5.1 数据预处理</h2><ul>
<li>德语翻译成英文，自行构建词典<img src="/pic/transformers/7.png" width = "80%" height = "80%" /></li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># S:起始标记，E：结束标记，P:padding，将当前序列补齐之最长序列长度的占位符</span></span><br><span class="line">sentence = [</span><br><span class="line">    <span class="comment">#   enc_input编码器输入        dec_input             dec_output</span></span><br><span class="line">    [<span class="string">&#x27;ich mochte ein bier P&#x27;</span>, <span class="string">&#x27;S i want a beer .&#x27;</span>, <span class="string">&#x27;i want a beer . E&#x27;</span>], </span><br><span class="line">    [<span class="string">&#x27;ich mochte ein cola P&#x27;</span>, <span class="string">&#x27;S i want a coke .&#x27;</span>, <span class="string">&#x27;i want a coke . E&#x27;</span>],</span><br><span class="line">]    <span class="comment"># 德语：我想喝啤酒作为编码器输入，    </span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 构建词典， padding用0表表示</span></span><br><span class="line"><span class="comment"># 源词典</span></span><br><span class="line">src_vocab = &#123;<span class="string">&#x27;P&#x27;</span>:<span class="number">0</span>, <span class="string">&#x27;ich&#x27;</span>:<span class="number">1</span>, <span class="string">&#x27;mochte&#x27;</span>:<span class="number">2</span>, <span class="string">&#x27;ein&#x27;</span>:<span class="number">3</span>, <span class="string">&#x27;bier&#x27;</span>:<span class="number">4</span>, <span class="string">&#x27;cola&#x27;</span>:<span class="number">5</span>&#125;</span><br><span class="line">src_vocab_size = <span class="built_in">len</span>(src_vocab)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 目标词典（包含特殊字符）</span></span><br><span class="line">tgt_vocab = &#123;<span class="string">&#x27;P&#x27;</span>:<span class="number">0</span>, <span class="string">&#x27;i&#x27;</span>:<span class="number">1</span>, <span class="string">&#x27;want&#x27;</span>:<span class="number">2</span>, <span class="string">&#x27;a&#x27;</span>:<span class="number">3</span>, <span class="string">&#x27;beer&#x27;</span>:<span class="number">4</span>, <span class="string">&#x27;coke&#x27;</span>:<span class="number">5</span>, <span class="string">&#x27;S&#x27;</span>:<span class="number">6</span>, <span class="string">&#x27;E&#x27;</span>:<span class="number">7</span>, <span class="string">&#x27;.&#x27;</span>:<span class="number">8</span>&#125;</span><br><span class="line"><span class="comment"># 词典中编号和词元的转换</span></span><br><span class="line">idx2word = &#123;v:k <span class="keyword">for</span> k, v <span class="keyword">in</span> tgt_vocab.items()&#125;</span><br><span class="line">tgt_vocab_size = <span class="built_in">len</span>(tgt_vocab)</span><br><span class="line"></span><br><span class="line">src_len = <span class="number">5</span>  <span class="comment"># 设定输入序列enc_input的最长序列长度</span></span><br><span class="line">tgt_len = <span class="number">6</span>  <span class="comment"># 设定输出序列dec_input/dec_ouput的最长序列长度 </span></span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 将原始输入序列转换为token也就是序列号表示</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">make_data</span>(<span class="params">sentence</span>):</span><br><span class="line">    enc_inputs, dec_inputs, dec_outputs = [], [], []</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(sentence)):</span><br><span class="line">        enc_input = [src_vocab[word] <span class="keyword">for</span> word <span class="keyword">in</span> sentence[i][<span class="number">0</span>].split()]</span><br><span class="line">        dec_input = [tgt_vocab[word] <span class="keyword">for</span> word <span class="keyword">in</span> sentence[i][<span class="number">1</span>].split()]</span><br><span class="line">        dec_output = [tgt_vocab[word] <span class="keyword">for</span> word <span class="keyword">in</span> sentence[i][<span class="number">2</span>].split()]</span><br><span class="line">        </span><br><span class="line">        enc_inputs.append(enc_input)</span><br><span class="line">        dec_inputs.append(dec_input)</span><br><span class="line">        dec_outputs.append(dec_output)</span><br><span class="line">    <span class="comment"># torch.LongTensor专门用于储存整数型，tensor则可以表示浮点，整数等</span></span><br><span class="line">    <span class="keyword">return</span> torch.LongTensor(enc_inputs),torch.LongTensor(dec_inputs),torch.LongTensor(dec_outputs)</span><br><span class="line"></span><br><span class="line">enc_inputs, dec_inputs, dec_outputs = make_data(sentence)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27; enc_inputs: \n&#x27;</span>, enc_inputs)  <span class="comment"># enc_inputs: [2,5]</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27; dec_inputs: \n&#x27;</span>, dec_inputs)  <span class="comment"># dec_inputs: [2,6]</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27; dec_outputs: \n&#x27;</span>, dec_outputs) <span class="comment"># dec_outputs: [2,6]</span></span><br></pre></td></tr></table></figure>

<h2 id="5-2-加载数据定义超参数"><a href="#5-2-加载数据定义超参数" class="headerlink" title="5.2 加载数据定义超参数"></a>5.2 加载数据定义超参数</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 使用Dataset加载数据</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">MyDataSet</span>(Data.Dataset):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, enc_inputs, dec_inputs, dec_outputs</span>):</span><br><span class="line">        <span class="built_in">super</span>(MyDataSet, self).__init__()</span><br><span class="line">        self.enc_inputs = enc_inputs</span><br><span class="line">        self.dec_inputs = dec_inputs</span><br><span class="line">        self.dec_outputs = dec_outputs</span><br><span class="line">        </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__len__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="comment"># enc_inputs.shape = [2, 5], 所以返回2</span></span><br><span class="line">        <span class="keyword">return</span> self.enc_inputs.shape[<span class="number">0</span>]</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 根据idx返回的是一组enc_input, dec_input, dec_output</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__getitem__</span>(<span class="params">self, idx</span>):</span><br><span class="line">        <span class="keyword">return</span> self.enc_inputs[idx], self.dec_inputs[idx], self.dec_outputs[idx]</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 打包训练用的数据集，构建DataLoader</span></span><br><span class="line">loader = Data.DataLoader(dataset=MyDataSet(enc_inputs, dec_inputs, dec_outputs), batch_size=<span class="number">2</span>, shuffle=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 用来表示一个词的向量长度</span></span><br><span class="line">d_model = <span class="number">512</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># FFN的隐藏层的神经元个数</span></span><br><span class="line">d_ff = <span class="number">2048</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 分头后的q, k, v词向量对的长度，依照原文设为64</span></span><br><span class="line">d_k = d_v = <span class="number">64</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Encoder 和 Decoder的个数</span></span><br><span class="line">n_layers = <span class="number">6</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 多头注意力中head的个数</span></span><br><span class="line">n_heads = <span class="number">8</span></span><br></pre></td></tr></table></figure>

<h2 id="5-3-训练"><a href="#5-3-训练" class="headerlink" title="5.3 训练"></a>5.3 训练</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">&quot;&quot;&quot;训练&quot;&quot;&quot;</span></span><br><span class="line"><span class="comment">#实例化模型</span></span><br><span class="line">model = Transformer().cuda()</span><br><span class="line"><span class="comment"># 进入训练模式</span></span><br><span class="line">model.train()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义损失函数和优化器</span></span><br><span class="line">criterion = nn.CrossEntropyLoss(ignore_index=<span class="number">0</span>) <span class="comment"># 忽略为0的类别，因为这是padding，没有意义</span></span><br><span class="line">optimizer = optim.SGD(model.parameters(), lr=<span class="number">1e-3</span>, momentum=<span class="number">0.99</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 开始训练</span></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">10</span>):</span><br><span class="line">    <span class="keyword">for</span> enc_inputs, dec_inputs, dec_outputs <span class="keyword">in</span> loader:</span><br><span class="line">        <span class="comment"># 打包的数据中</span></span><br><span class="line">        <span class="comment"># enc_inputs:[batch_size, src_len][2,5] 是需要翻译的德文，作为编码器的输入</span></span><br><span class="line">        <span class="comment"># dec_inputs:[batch_size, tgt_len][2,6] 是带起始符翻译过后的英文，作为解码器的输入</span></span><br><span class="line">        <span class="comment"># dec_outputs:[batch_size, tgt_len][2,6]是带有结束符的翻译后的英文，作为标注用于计算损失</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 将数据转移到cuda中</span></span><br><span class="line">        enc_inputs, dec_inputs, dec_outputs = enc_inputs.cuda(), dec_inputs.cuda(), dec_outputs.cuda()</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 计算模型输出</span></span><br><span class="line">        outputs = model(enc_inputs, dec_inputs) <span class="comment"># outputs:[batch_size * tgt_len, tgt_vocab_size]</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 计算损失</span></span><br><span class="line">        loss = criterion(outputs, dec_outputs.view(-<span class="number">1</span>)) <span class="comment"># 将dec_outputs展平成一维张量</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 更新权重</span></span><br><span class="line">        optimizer.zero_grad()</span><br><span class="line">        loss.backward()</span><br><span class="line">        optimizer.step()</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 打印训练进度</span></span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&#x27;Epoch [<span class="subst">&#123;epoch + <span class="number">1</span>&#125;</span>/10],Loss:<span class="subst">&#123;loss.item()&#125;</span>&#x27;</span>)</span><br></pre></td></tr></table></figure>

<h2 id="5-4-测试"><a href="#5-4-测试" class="headerlink" title="5.4 测试"></a>5.4 测试</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 原文使用的是大小为4的束搜索，为简单起见这里使用贪心搜索</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">greedy_decoder</span>(<span class="params">model, enc_input, start_symbol</span>):</span><br><span class="line">    <span class="comment"># enc_input:[1, seq_len] 对应一句话</span></span><br><span class="line">    enc_outputs = model.encoder(enc_input) <span class="comment"># enc_outputs:[1, seq_len, 512]</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 生成一个1行0列的，和enc_inputs.data类型形同的空张量，待后续填充</span></span><br><span class="line">    dec_input = torch.zeros(<span class="number">1</span>, <span class="number">0</span>).type_as(enc_input.data) <span class="comment"># .data是为了避免影响梯度信息</span></span><br><span class="line">    </span><br><span class="line">    next_symbol = start_symbol</span><br><span class="line">    flag = <span class="literal">True</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">while</span> flag:</span><br><span class="line">        <span class="comment"># dec_input.detach() 创建dec_input的一个分离副本</span></span><br><span class="line">        <span class="comment"># 生成了一个只含有next_symbol的（1， 1）的张量</span></span><br><span class="line">        <span class="comment"># -1，表示在最火一个维度上进行拼接cat</span></span><br><span class="line">        <span class="comment"># 这行代码的作用是将next_symbol拼接到dec_input中，作为新一轮decoder的输入</span></span><br><span class="line">        dec_input = torch.cat([dec_input.detach(), torch.tensor([[next_symbol]], dtype=enc_input.dtype).cuda()], -<span class="number">1</span>)</span><br><span class="line">        </span><br><span class="line">        dec_outputs = model.decoder(dec_input,enc_input, enc_outputs) <span class="comment"># dec_outputs:[1, tgt_len, d_model]</span></span><br><span class="line">        </span><br><span class="line">        projected = model.projection(dec_outputs) <span class="comment"># projected:[1, 当前生成的tgt_len, tgt_vocab_size]</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># max返回的是一个元组（最大值，最大值对应的索引），使用[1]得到索引，也就是预测的下一个词元的索引</span></span><br><span class="line">        <span class="comment"># keepdim=False会导致减少一维</span></span><br><span class="line">        <span class="comment"># prob是一个一维列表，包含目前为止依次生成词的索引，最后一个是新生成的</span></span><br><span class="line">        prob = projected.squeeze(<span class="number">0</span>).<span class="built_in">max</span>(dim=-<span class="number">1</span>, keepdim=<span class="literal">False</span>)[<span class="number">1</span>]</span><br><span class="line">        </span><br><span class="line">        next_symbol = prob.data[-<span class="number">1</span>]</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">if</span> next_symbol == tgt_vocab[<span class="string">&#x27;.&#x27;</span>]:</span><br><span class="line">            flag = <span class="literal">False</span></span><br><span class="line">        <span class="built_in">print</span>(next_symbol)</span><br><span class="line">    <span class="keyword">return</span> dec_input <span class="comment"># [1, tgt_len]</span></span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 测试</span></span><br><span class="line">model = torch.load(<span class="string">&#x27;MyTransformer.pth&#x27;</span>) <span class="comment"># 这种加载模型的方式不太好</span></span><br><span class="line">model.<span class="built_in">eval</span>()</span><br><span class="line"><span class="keyword">with</span> torch.no_grad():</span><br><span class="line">    <span class="comment"># 手动从loader中去一个batch的数据</span></span><br><span class="line">    enc_inputs, _, _, = <span class="built_in">next</span>(<span class="built_in">iter</span>(loader))</span><br><span class="line">    enc_inputs = enc_inputs.cuda()</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(enc_inputs)):</span><br><span class="line">        greedy_dec_input = greedy_decoder(model, enc_inputs[i].view(<span class="number">1</span>, -<span class="number">1</span>), start_symbol=tgt_vocab[<span class="string">&#x27;S&#x27;</span>])</span><br><span class="line">        predict = model(enc_inputs[i].view(<span class="number">1</span>, -<span class="number">1</span>), greedy_dec_input)</span><br><span class="line">        predict = predict.data.<span class="built_in">max</span>(dim=-<span class="number">1</span>, keepdim=<span class="literal">False</span>)[<span class="number">1</span>]</span><br><span class="line">        </span><br><span class="line">        <span class="built_in">print</span>(enc_inputs[i], <span class="string">&#x27;---&gt;&#x27;</span>, [idx2word[n.item()] <span class="keyword">for</span> n <span class="keyword">in</span> predict])</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">tensor(<span class="number">1</span>, device=<span class="string">&#x27;cuda:0&#x27;</span>)</span><br><span class="line">tensor(<span class="number">2</span>, device=<span class="string">&#x27;cuda:0&#x27;</span>)</span><br><span class="line">tensor(<span class="number">3</span>, device=<span class="string">&#x27;cuda:0&#x27;</span>)</span><br><span class="line">tensor(<span class="number">4</span>, device=<span class="string">&#x27;cuda:0&#x27;</span>)</span><br><span class="line">tensor(<span class="number">8</span>, device=<span class="string">&#x27;cuda:0&#x27;</span>)</span><br><span class="line">tensor([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">0</span>], device=<span class="string">&#x27;cuda:0&#x27;</span>) ---&gt; [<span class="string">&#x27;i&#x27;</span>, <span class="string">&#x27;want&#x27;</span>, <span class="string">&#x27;a&#x27;</span>, <span class="string">&#x27;beer&#x27;</span>, <span class="string">&#x27;.&#x27;</span>]</span><br><span class="line">tensor(<span class="number">1</span>, device=<span class="string">&#x27;cuda:0&#x27;</span>)</span><br><span class="line">tensor(<span class="number">2</span>, device=<span class="string">&#x27;cuda:0&#x27;</span>)</span><br><span class="line">tensor(<span class="number">3</span>, device=<span class="string">&#x27;cuda:0&#x27;</span>)</span><br><span class="line">tensor(<span class="number">5</span>, device=<span class="string">&#x27;cuda:0&#x27;</span>)</span><br><span class="line">tensor(<span class="number">8</span>, device=<span class="string">&#x27;cuda:0&#x27;</span>)</span><br><span class="line">tensor([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">5</span>, <span class="number">0</span>], device=<span class="string">&#x27;cuda:0&#x27;</span>) ---&gt; [<span class="string">&#x27;i&#x27;</span>, <span class="string">&#x27;want&#x27;</span>, <span class="string">&#x27;a&#x27;</span>, <span class="string">&#x27;coke&#x27;</span>, <span class="string">&#x27;.&#x27;</span>]</span><br><span class="line"></span><br></pre></td></tr></table></figure>


    </div>

    
    
    

    <footer class="post-footer">

        

          <div class="post-nav">
            <div class="post-nav-item">
                <a href="/2024/04/12/AGI%E5%B8%82%E5%9C%BA%E5%8F%91%E5%B1%95/" rel="prev" title="AGI市场发展">
                  <i class="fa fa-angle-left"></i> AGI市场发展
                </a>
            </div>
            <div class="post-nav-item">
                <a href="/2024/04/22/%E6%95%B0%E6%8D%AE%E5%8F%AF%E8%A7%86%E5%8C%96%EF%BC%88EDA%EF%BC%89/" rel="next" title="数据可视化（EDA）">
                  数据可视化（EDA） <i class="fa fa-angle-right"></i>
                </a>
            </div>
          </div>
    </footer>
  </article>
</div>






</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">

  <div class="copyright">
    &copy; 
    <span itemprop="copyrightYear">2024</span>
    <span class="with-love">
      <i class="fa fa-heart"></i>
    </span>
    <span class="author" itemprop="copyrightHolder">凹凸</span>
  </div>

    </div>
  </footer>

  
  <div class="back-to-top" role="button" aria-label="返回顶部">
    <i class="fa fa-arrow-up fa-lg"></i>
    <span>0%</span>
  </div>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


  
  <script src="https://cdnjs.cloudflare.com/ajax/libs/animejs/3.2.1/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous"></script>
<script src="/js/comments.js"></script><script src="/js/utils.js"></script><script src="/js/next-boot.js"></script>

  






  





</body>
</html>
